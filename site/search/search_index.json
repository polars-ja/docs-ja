{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"Blazingly Fast DataFrame Library  <p>Polars is a blazingly fast DataFrame library for manipulating structured data. The core is written in Rust, and available for Python, R and NodeJS.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Fast: Written from scratch in Rust, designed close to the machine and without external dependencies.</li> <li>I/O: First class support for all common data storage layers: local, cloud storage &amp; databases.</li> <li>Intuitive API: Write your queries the way they were intended. Polars, internally, will determine the most efficient way to execute using its query optimizer.</li> <li>Out of Core: The streaming API allows you to process your results without requiring all your data to be in memory at the same time</li> <li>Parallel: Utilises the power of your machine by dividing the workload among the available CPU cores without any additional configuration.</li> <li>Vectorized Query Engine: Using Apache Arrow, a columnar data format, to process your queries in a vectorized manner and SIMD to optimize CPU usage.</li> </ul> <p>Users new to DataFrames</p> <p>A DataFrame is a 2-dimensional data structure that is useful for data manipulation and analysis. With labeled axes for rows and columns, each column can contain different data types, making complex data operations such as merging and aggregation much easier. Due to their flexibility and intuitive way of storing and working with data, DataFrames have become increasingly popular in modern data analytics and engineering.</p>"},{"location":"#philosophy","title":"Philosophy","text":"<p>The goal of Polars is to provide a lightning fast DataFrame library that:</p> <ul> <li>Utilizes all available cores on your machine.</li> <li>Optimizes queries to reduce unneeded work/memory allocations.</li> <li>Handles datasets much larger than your available RAM.</li> <li>A consistent and predictable API.</li> <li>Adheres to a strict schema (data-types should be known before running the query).</li> </ul> <p>Polars is written in Rust which gives it C/C++ performance and allows it to fully control performance critical parts in a query engine.</p>"},{"location":"#example","title":"Example","text":"Python Rust <p> <code>scan_csv</code> \u00b7 <code>filter</code> \u00b7 <code>group_by</code> \u00b7 <code>collect</code> <pre><code>import polars as pl\n\nq = (\n    pl.scan_csv(\"docs/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .group_by(\"species\")\n    .agg(pl.all().sum())\n)\n\ndf = q.collect()\n</code></pre></p> <p> <code>LazyCsvReader</code> \u00b7 <code>filter</code> \u00b7 <code>group_by</code> \u00b7 <code>collect</code> \u00b7  Available on feature streaming \u00b7  Available on feature csv <pre><code>use polars::prelude::*;\n\nlet q = LazyCsvReader::new(\"docs/data/iris.csv\")\n    .has_header(true)\n    .finish()?\n    .filter(col(\"sepal_length\").gt(lit(5)))\n    .group_by(vec![col(\"species\")])\n    .agg([col(\"*\").sum()]);\n\nlet df = q.collect()?;\n</code></pre></p> <p>A more extensive introduction can be found in the next chapter.</p>"},{"location":"#community","title":"Community","text":"<p>Polars has a very active community with frequent releases (approximately weekly). Below are some of the top contributors to the project:</p> <p> </p>"},{"location":"#contributing","title":"Contributing","text":"<p>We appreciate all contributions, from reporting bugs to implementing new features. Read our contributing guide to learn more.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"},{"location":"people/","title":"People","text":""},{"location":"_build/snippets/under_construction/","title":"Under construction","text":"<p> Under Construction </p> <p>This section is still under development. Want to help out? Consider contributing and making a pull request to our repository. Please read our contributing guide on how to proceed.</p>"},{"location":"api/","title":"API reference","text":"<p>The API reference contains detailed descriptions of all public functions and objects. It's the best place to look if you need information on a specific function.</p>"},{"location":"api/#python","title":"Python","text":"<p>The Python API reference is built using Sphinx. It's available in our docs.</p>"},{"location":"api/#rust","title":"Rust","text":"<p>The Rust API reference is built using Cargo. It's available on docs.rs.</p>"},{"location":"development/versioning/","title":"Versioning","text":""},{"location":"development/versioning/#version-changes","title":"Version changes","text":"<p>Polars adheres to the semantic versioning specification.</p> <p>As Polars has not released its <code>1.0.0</code> version yet, breaking releases lead to a minor version increase (e.g. from <code>0.18.15</code> to <code>0.19.0</code>), while all other releases increment the patch version (e.g. from <code>0.18.15</code> to <code>0.18.16</code>)</p>"},{"location":"development/versioning/#policy-for-breaking-changes","title":"Policy for breaking changes","text":"<p>Polars takes backwards compatibility seriously, but we are not afraid to change things if it leads to a better product.</p>"},{"location":"development/versioning/#philosophy","title":"Philosophy","text":"<p>We don't always get it right on the first try. We learn as we go along and get feedback from our users. Sometimes, we're a little too eager to get out a new feature and didn't ponder all the possible implications.</p> <p>If this happens, we correct our mistakes and introduce a breaking change. Most of the time, this is no big deal. Users get a deprecation warning, they do a quick search-and-replace in their code base, and that's that.</p> <p>At times, we run into an issue requires more effort on our user's part to fix. A change in the query engine can seriously impact the assumptions in a data pipeline. We do not make such changes lightly, but we will make them if we believe it makes Polars better.</p> <p>Freeing ourselves of past indiscretions is important to keep Polars moving forward. We know it takes time and energy for our users to keep up with new releases but, in the end, it benefits everyone for Polars to be the best product possible.</p>"},{"location":"development/versioning/#what-qualifies-as-a-breaking-change","title":"What qualifies as a breaking change","text":"<p>A breaking change occurs when an existing component of the public API is changed or removed.</p> <p>A feature is part of the public API if it is documented in the API reference.</p> <p>Examples of breaking changes:</p> <ul> <li>A deprecated function or method is removed.</li> <li>The default value of a parameter is changed.</li> <li>The outcome of a query has changed due to changes to the query engine.</li> </ul> <p>Examples of changes that are not considered breaking:</p> <ul> <li>An undocumented function is removed.</li> <li>The module path of a public class is changed.</li> <li>An optional parameter is added to an existing method.</li> </ul> <p>Bug fixes are not considered a breaking change, even though it may impact some users' workflows.</p>"},{"location":"development/versioning/#unstable-functionality","title":"Unstable functionality","text":"<p>Some parts of the public API are marked as unstable. You can recognize this functionality from the warning in the API reference, or from the warning issued when the configuration option <code>warn_unstable</code> is active. There are a number of reasons functionality may be marked as unstable:</p> <ul> <li>We are unsure about the exact API. The name, function signature, or implementation are likely to change in the future.</li> <li>The functionality is not tested extensively yet. Bugs may pop up when used in real-world scenarios.</li> <li>The functionality does not integrate well with the full Polars API. You may find it works in one context but not in another.</li> </ul> <p>Releasing functionality as unstable allows us to gather important feedback from users that use Polars in real-world scenarios. This helps us fine-tune things before giving it the final stamp of approval. Users are only interested in solid, well-tested functionality can avoid this part of the API.</p> <p>Functionality marked as unstable may change at any point without it being considered a breaking change.</p>"},{"location":"development/versioning/#deprecation-warnings","title":"Deprecation warnings","text":"<p>If we decide to introduce a breaking change, the existing behavior is deprecated if possible. For example, if we choose to rename a function, the new function is added alongside the old function, and using the old function will result in a deprecation warning.</p> <p>Not all changes can be deprecated nicely. A change to the query engine may have effects across a large part of the API. Such changes will not be warned for, but will be included in the changelog and the migration guide.</p> <p>Warning</p> <p>Breaking changes to the Rust API are not deprecated first, but will be listed in the changelog. Supporting deprecated functionality would slow down development too much at this point in time.</p>"},{"location":"development/versioning/#deprecation-period","title":"Deprecation period","text":"<p>As a rule, deprecated functionality is removed two breaking releases after the deprecation happens. For example:</p> <ul> <li>Before the release of <code>1.0.0</code>: a function deprecated in version <code>0.18.3</code> will be removed in version <code>0.20.0</code></li> <li>After the release of <code>1.0.0</code>: a function deprecated in version <code>1.2.3</code> will be removed in version <code>3.0.0</code></li> </ul> <p>This means that if your program does not raise any deprecation warnings, it should be mostly safe to upgrade to the next breaking release. As breaking releases happen about once every three months, this allows three to six months to adjust to any pending breaking changes.</p> <p>In some cases, we may decide to adjust the deprecation period. If retaining the deprecated functionality blocks other improvements to Polars, we may shorten the deprecation period to a single breaking release. This will be mentioned in the warning message. If the deprecation affects many users, we may extend the deprecation period.</p>"},{"location":"development/contributing/","title":"Overview","text":"<p>Thanks for taking the time to contribute! We appreciate all contributions, from reporting bugs to implementing new features. If you're unclear on how to proceed after reading this guide, please contact us on Discord.</p>"},{"location":"development/contributing/#reporting-bugs","title":"Reporting bugs","text":"<p>We use GitHub issues to track bugs and suggested enhancements. You can report a bug by opening a new issue. Use the appropriate issue type for the language you are using (Rust / Python).</p> <p>Before creating a bug report, please check that your bug has not already been reported, and that your bug exists on the latest version of Polars. If you find a closed issue that seems to report the same bug you're experiencing, open a new issue and include a link to the original issue in your issue description.</p> <p>Please include as many details as possible in your bug report. The information helps the maintainers resolve the issue faster.</p>"},{"location":"development/contributing/#suggesting-enhancements","title":"Suggesting enhancements","text":"<p>We use GitHub issues to track bugs and suggested enhancements. You can suggest an enhancement by opening a new feature request. Before creating an enhancement suggestion, please check that a similar issue does not already exist.</p> <p>Please describe the behavior you want and why, and provide examples of how Polars would be used if your feature were added.</p>"},{"location":"development/contributing/#contributing-to-the-codebase","title":"Contributing to the codebase","text":""},{"location":"development/contributing/#picking-an-issue","title":"Picking an issue","text":"<p>Pick an issue by going through the issue tracker and finding an issue you would like to work on. Feel free to pick any issue with an accepted label that is not already assigned. We use the help wanted label to indicate issues that are high on our wishlist.</p> <p>If you are a first time contributor, you might want to look for issues labeled good first issue. The Polars code base is quite complex, so starting with a small issue will help you find your way around!</p> <p>If you would like to take on an issue, please comment on the issue to let others know. You may use the issue to discuss possible solutions.</p>"},{"location":"development/contributing/#setting-up-your-local-environment","title":"Setting up your local environment","text":"<p>Polars development flow relies on both Rust and Python, which means setting up your local development environment is not trivial. If you run into problems, please contact us on Discord.</p> <p>Note that if you are a Windows user, the steps below might not work as expected; try developing using WSL. Under native Windows, you may have to manually copy the contents of <code>toolchain.toml</code> to <code>py-polars/toolchain.toml</code>, as Git for Windows may not correctly handle symbolic links.</p> <p>Start by forking the Polars repository, then clone your forked repository using <code>git</code>:</p> <pre><code>git clone https://github.com/&lt;username&gt;/polars.git\ncd polars\n</code></pre> <p>In order to work on Polars effectively, you will need Rust, Python, and dprint.</p> <p>First, install Rust using rustup. After the initial installation, you will also need to install the nightly toolchain:</p> <pre><code>rustup toolchain install nightly --component miri\n</code></pre> <p>Next, install Python, for example using pyenv. We recommend using the latest Python version (<code>3.12</code>). Make sure you deactivate any active virtual environments or conda environments, as the steps below will create a new virtual environment for Polars. You will need Python even if you intend to work on the Rust code only, as we rely on the Python tests to verify all functionality.</p> <p>Finally, install dprint. This is not strictly required, but it is recommended as we use it to autoformat certain file types.</p> <p>You can now check that everything works correctly by going into the <code>py-polars</code> directory and running the test suite (warning: this may be slow the first time you run it):</p> <pre><code>cd py-polars\nmake test\n</code></pre> <p>This will do a number of things:</p> <ul> <li>Use Python to create a virtual environment in the <code>.venv</code> folder.</li> <li>Use pip to install all Python dependencies for development, linting, and building documentation.</li> <li>Use Rust to compile and install Polars in your virtual environment. At least 8GB of RAM is recommended for this step to run smoothly.</li> <li>Use pytest to run the Python unittests in your virtual environment</li> </ul> <p>Check if linting also works correctly by running:</p> <pre><code>make pre-commit\n</code></pre> <p>Note that we do not actually use the pre-commit tool. We use the Makefile to conveniently run the following formatting and linting tools:</p> <ul> <li>ruff</li> <li>mypy</li> <li>rustfmt</li> <li>clippy</li> <li>dprint</li> </ul> <p>If this all runs correctly, you're ready to start contributing to the Polars codebase!</p>"},{"location":"development/contributing/#working-on-your-issue","title":"Working on your issue","text":"<p>Create a new git branch from the <code>main</code> branch in your local repository, and start coding!</p> <p>The Rust code is located in the <code>crates</code> directory, while the Python codebase is located in the <code>py-polars</code> directory. Both directories contain a <code>Makefile</code> with helpful commands. Most notably:</p> <ul> <li><code>make test</code> to run the test suite (see the test suite docs for more info)</li> <li><code>make pre-commit</code> to run autoformatting and linting</li> </ul> <p>Note that your work cannot be merged if these checks fail! Run <code>make help</code> to get a list of other helpful commands.</p> <p>Two other things to keep in mind:</p> <ul> <li>If you add code that should be tested, add tests.</li> <li>If you change the public API, update the documentation.</li> </ul>"},{"location":"development/contributing/#pull-requests","title":"Pull requests","text":"<p>When you have resolved your issue, open a pull request in the Polars repository. Please adhere to the following guidelines:</p> <ul> <li>Start your pull request title with a conventional commit tag. This helps us add your contribution to the right section of the changelog. We use the Angular convention. Scope can be <code>rust</code> and/or <code>python</code>, depending on your contribution.</li> <li>Use a descriptive title starting with an uppercase letter. This text will end up in the changelog.</li> <li>In the pull request description, link to the issue you were working on.</li> <li>Add any relevant information to the description that you think may help the maintainers review your code.</li> <li>Make sure your branch is rebased against the latest version of the <code>main</code> branch.</li> <li>Make sure all GitHub Actions checks pass.</li> </ul> <p>After you have opened your pull request, a maintainer will review it and possibly leave some comments. Once all issues are resolved, the maintainer will merge your pull request, and your work will be part of the next Polars release!</p> <p>Keep in mind that your work does not have to be perfect right away! If you are stuck or unsure about your solution, feel free to open a draft pull request and ask for help.</p>"},{"location":"development/contributing/#contributing-to-documentation","title":"Contributing to documentation","text":"<p>The most important components of Polars documentation are the user guide, the API references, and the database of questions on StackOverflow.</p>"},{"location":"development/contributing/#user-guide","title":"User guide","text":"<p>The user guide is maintained in the <code>docs/user-guide</code> folder. Before creating a PR first raise an issue to discuss what you feel is missing or could be improved.</p>"},{"location":"development/contributing/#building-and-serving-the-user-guide","title":"Building and serving the user guide","text":"<p>The user guide is built using MkDocs. You install the dependencies for building the user guide by running <code>make build</code> in the root of the repo.</p> <p>Activate the virtual environment and run <code>mkdocs serve</code> to build and serve the user guide, so you can view it locally and see updates as you make changes.</p>"},{"location":"development/contributing/#creating-a-new-user-guide-page","title":"Creating a new user guide page","text":"<p>Each user guide page is based on a <code>.md</code> markdown file. This file must be listed in <code>mkdocs.yml</code>.</p>"},{"location":"development/contributing/#adding-a-shell-code-block","title":"Adding a shell code block","text":"<p>To add a code block with code to be run in a shell with tabs for Python and Rust, use the following format:</p> <pre><code>=== \":fontawesome-brands-python: Python\"\n\n    ```shell\n    $ pip install fsspec\n    ```\n\n=== \":fontawesome-brands-rust: Rust\"\n\n    ```shell\n    $ cargo add aws_sdk_s3\n    ```\n</code></pre>"},{"location":"development/contributing/#adding-a-code-block","title":"Adding a code block","text":"<p>The snippets for Python and Rust code blocks are in the <code>docs/src/python/</code> and <code>docs/src/rust/</code> directories, respectively. To add a code snippet with Python or Rust code to a <code>.md</code> page, use the following format:</p> <pre><code>{{code_block('user-guide/io/cloud-storage','read_parquet',['read_parquet','read_csv'])}}\n</code></pre> <ul> <li>The first argument is a path to either or both files called <code>docs/src/python/user-guide/io/cloud-storage.py</code> and <code>docs/src/rust/user-guide/io/cloud-storage.rs</code>.</li> <li>The second argument is the name given at the start and end of each snippet in the <code>.py</code> or <code>.rs</code> file</li> <li>The third argument is a list of links to functions in the API docs. For each element of the list there must be a corresponding entry in <code>docs/_build/API_REFERENCE_LINKS.yml</code></li> </ul> <p>If the corresponding <code>.py</code> and <code>.rs</code> snippet files both exist then each snippet named in the second argument to <code>code_block</code> above must exist or the build will fail. An empty snippet should be added to the <code>.py</code> or <code>.rs</code> file if the snippet is not needed.</p> <p>Each snippet is formatted as follows:</p> <pre><code># --8&lt;-- [start:read_parquet]\nimport polars as pl\n\ndf = pl.read_parquet(\"file.parquet\")\n# --8&lt;-- [end:read_parquet]\n</code></pre> <p>The snippet is delimited by <code>--8&lt;-- [start:&lt;snippet_name&gt;]</code> and <code>--8&lt;-- [end:&lt;snippet_name&gt;]</code>. The snippet name must match the name given in the second argument to <code>code_block</code> above.</p>"},{"location":"development/contributing/#linting","title":"Linting","text":"<p>Before committing, install <code>dprint</code> (see above) and run <code>dprint fmt</code> from the <code>docs</code> directory to lint the markdown files.</p>"},{"location":"development/contributing/#api-reference","title":"API reference","text":"<p>Polars has separate API references for Rust and Python. These are generated directly from the codebase, so in order to contribute, you will have to follow the steps outlined in this section above.</p>"},{"location":"development/contributing/#rust","title":"Rust","text":"<p>Rust Polars uses <code>cargo doc</code> to build its documentation. Contributions to improve or clarify the API reference are welcome.</p>"},{"location":"development/contributing/#python","title":"Python","text":"<p>For the Python API reference, we always welcome good docstring examples. There are still parts of the API that do not have any code examples. This is a great way to start contributing to Polars!</p> <p>Note that we follow the numpydoc convention. Docstring examples should also follow the Black codestyle. From the <code>py-polars</code> directory, run <code>make fmt</code> to make sure your additions pass the linter, and run <code>make doctest</code> to make sure your docstring examples are valid.</p> <p>Polars uses Sphinx to build the API reference. This means docstrings in general should follow the reST format. If you want to build the API reference locally, go to the <code>py-polars/docs</code> directory and run <code>make html</code>. The resulting HTML files will be in <code>py-polars/docs/build/html</code>.</p> <p>New additions to the API should be added manually to the API reference by adding an entry to the correct <code>.rst</code> file in the <code>py-polars/docs/source/reference</code> directory.</p>"},{"location":"development/contributing/#stackoverflow","title":"StackOverflow","text":"<p>We use StackOverflow to create a database of high quality questions and answers that is searchable and remains up-to-date. There is a separate tag for each language:</p> <ul> <li>Python Polars</li> <li>Rust Polars</li> </ul> <p>Contributions in the form of well-formulated questions or answers are always welcome! If you add a new question, please notify us by adding a matching issue to our GitHub issue tracker.</p>"},{"location":"development/contributing/#release-flow","title":"Release flow","text":"<p>This section is intended for Polars maintainers.</p> <p>Polars releases Rust crates to crates.io and Python packages to PyPI.</p> <p>New releases are marked by an official GitHub release and an associated git tag. We utilize Release Drafter to automatically draft GitHub releases with release notes.</p>"},{"location":"development/contributing/#steps","title":"Steps","text":"<p>The steps for releasing a new Rust or Python version are similar. The release process is mostly automated through GitHub Actions, but some manual steps are required. Follow the steps below to release a new version.</p> <p>Start by bumping the version number in the source code:</p> <ol> <li>Check the releases page on GitHub and find the appropriate draft release. Note the version number associated with this release.</li> <li>Make sure your fork is up-to-date with the latest version of the main Polars repository, and create a new branch.</li> <li> <p>Bump the version number.</p> </li> <li> <p>Rust: Update the version number in all <code>Cargo.toml</code> files in the <code>polars</code> directory and subdirectories. You'll probably want to use some search/replace strategy, as there are quite a few crates that need to be updated.</p> </li> <li> <p>Python: Update the version number in <code>py-polars/Cargo.toml</code> to match the version of the draft release.</p> </li> <li> <p>From the <code>py-polars</code> directory, run <code>make build</code> to generate a new <code>Cargo.lock</code> file.</p> </li> <li>Create a new commit with all files added. The name of the commit should follow the format <code>release(&lt;language&gt;): &lt;Language&gt; Polars &lt;version-number&gt;</code>. For example: <code>release(python): Python Polars 0.16.1</code></li> <li>Push your branch and open a new pull request to the <code>main</code> branch of the main Polars repository.</li> <li>Wait for the GitHub Actions checks to pass, then squash and merge your pull request.</li> </ol> <p>Directly after merging your pull request, release the new version:</p> <ol> <li>Go to the release workflow (Python/Rust), click Run workflow in the top right, and click the green button. This will trigger the workflow, which will build all release artifacts and publish them.</li> <li>Wait for the workflow to finish, then check crates.io/PyPI/GitHub to verify that the new Polars release is now available.</li> </ol>"},{"location":"development/contributing/#troubleshooting","title":"Troubleshooting","text":"<p>It may happen that one or multiple release jobs fail. If so, you should first try to simply re-run the failed jobs from the GitHub Actions UI.</p> <p>If that doesn't help, you will have to figure out what's wrong and commit a fix. Once your fix has made it to the <code>main</code> branch, simply re-trigger the release workflow.</p>"},{"location":"development/contributing/#license","title":"License","text":"<p>Any contributions you make to this project will fall under the MIT License that covers the Polars project.</p>"},{"location":"development/contributing/ci/","title":"Continuous integration","text":"<p>Polars uses GitHub Actions as its continuous integration (CI) tool. The setup is reasonably complex, as far as CI setups go. This page explains some of the design choices.</p>"},{"location":"development/contributing/ci/#goal","title":"Goal","text":"<p>Overall, the CI suite aims to achieve the following:</p> <ul> <li>Enforce code correctness by running automated tests.</li> <li>Enforce code quality by running automated linting checks.</li> <li>Enforce code performance by running benchmark tests.</li> <li>Enforce that code is properly documented.</li> <li>Allow maintainers to easily publish new releases.</li> </ul> <p>We rely on a wide range of tools to achieve this for both the Rust and the Python code base, and thus a lot of checks are triggered on each pull request.</p> <p>It's entirely possible that you submit a relatively trivial fix that subsequently fails a bunch of checks. Do not despair - check the logs to see what went wrong and try to fix it. You can run the failing command locally to verify that everything works correctly. If you can't figure it out, ask a maintainer for help!</p>"},{"location":"development/contributing/ci/#design","title":"Design","text":"<p>The CI setup is designed with the following requirements in mind:</p> <ul> <li>Get feedback on each step individually. We want to avoid our test job being cancelled because a linting check failed, only to find out later that we also have a failing test.</li> <li>Get feedback on each check as quickly as possible. We want to be able to iterate quickly if it turns out our code does not pass some of the checks.</li> <li>Only run checks when they need to be run. A change to the Rust code does not warrant a linting check of the Python code, for example.</li> </ul> <p>This results in a modular setup with many separate workflows and jobs that rely heavily on caching.</p>"},{"location":"development/contributing/ci/#modular-setup","title":"Modular setup","text":"<p>The repository consists of two main parts: the Rust code base and the Python code base. Both code bases are interdependent: Rust code is tested through Python tests, and the Python code relies on the Rust implementation for most functionality.</p> <p>To make sure CI jobs are only run when they need to be run, each workflow is triggered only when relevant files are modified.</p>"},{"location":"development/contributing/ci/#caching","title":"Caching","text":"<p>The main challenge is that the Rust code base for Polars is quite large, and consequently, compiling the project from scratch is slow. This is addressed by caching the Rust build artifacts.</p> <p>However, since GitHub Actions does not allow sharing caches between feature branches, we need to run the workflows on the main branch as well - at least the part that builds the Rust cache. This leads to many workflows that trigger both on pull request AND on push to the main branch, with individual steps of jobs enabled or disabled based on the branch it runs on.</p> <p>Care must also be taken not to exceed the maximum cache space of 10Gb allotted to open source GitHub repositories. Hence we do not do any caching on feature branches - we always use the cache available from the main branch. This also avoids any extra time that would be required to store the cache.</p>"},{"location":"development/contributing/ci/#releases","title":"Releases","text":"<p>The release jobs for Rust and Python are triggered manually. Refer to the contributing guide for the full release process.</p>"},{"location":"development/contributing/code-style/","title":"Code style","text":"<p>This page contains some guidance on code style.</p> <p>Info</p> <p>Additional information will be added to this page later.</p>"},{"location":"development/contributing/code-style/#rust","title":"Rust","text":""},{"location":"development/contributing/code-style/#naming-conventions","title":"Naming conventions","text":"<p>Naming conventions for variables:</p> <pre><code>let s: Series = ...\nlet ca: ChunkedArray = ...\nlet arr: ArrayRef = ...\nlet arr: PrimitiveArray = ...\nlet dtype: DataType = ...\nlet data_type: ArrowDataType = ...\n</code></pre>"},{"location":"development/contributing/code-style/#code-example","title":"Code example","text":"<pre><code>use std::ops::Add;\n\nuse polars::export::arrow::array::*;\nuse polars::export::arrow::compute::arity::binary;\nuse polars::export::arrow::types::NativeType;\nuse polars::prelude::*;\nuse polars_core::utils::{align_chunks_binary, combine_validities_or};\nuse polars_core::with_match_physical_numeric_polars_type;\n\n// Prefer to do the compute closest to the arrow arrays.\n// this will tend to be faster as iterators can work directly on slices and don't have\n// to go through boxed traits\nfn compute_kernel&lt;T&gt;(arr_1: &amp;PrimitiveArray&lt;T&gt;, arr_2: &amp;PrimitiveArray&lt;T&gt;) -&gt; PrimitiveArray&lt;T&gt;\nwhere\n    T: Add&lt;Output = T&gt; + NativeType,\n{\n    // process the null data separately\n    // this saves an expensive branch and bitoperation when iterating\n    let validity_1 = arr_1.validity();\n    let validity_2 = arr_2.validity();\n\n    let validity = combine_validities_or(validity_1, validity_2);\n\n    // process the numerical data as if there were no validities\n    let values_1: &amp;[T] = arr_1.values().as_slice();\n    let values_2: &amp;[T] = arr_2.values().as_slice();\n\n    let values = values_1\n        .iter()\n        .zip(values_2)\n        .map(|(a, b)| *a + *b)\n        .collect::&lt;Vec&lt;_&gt;&gt;();\n\n    PrimitiveArray::from_data_default(values.into(), validity)\n}\n\n// Same kernel as above, but uses the `binary` abstraction. Prefer this,\n#[allow(dead_code)]\nfn compute_kernel2&lt;T&gt;(arr_1: &amp;PrimitiveArray&lt;T&gt;, arr_2: &amp;PrimitiveArray&lt;T&gt;) -&gt; PrimitiveArray&lt;T&gt;\nwhere\n    T: Add&lt;Output = T&gt; + NativeType,\n{\n    binary(arr_1, arr_2, arr_1.data_type().clone(), |a, b| a + b)\n}\n\nfn compute_chunked_array_2_args&lt;T: PolarsNumericType&gt;(\n    ca_1: &amp;ChunkedArray&lt;T&gt;,\n    ca_2: &amp;ChunkedArray&lt;T&gt;,\n) -&gt; ChunkedArray&lt;T&gt; {\n    // This ensures both ChunkedArrays have the same number of chunks with the\n    // same offset and the same length.\n    let (ca_1, ca_2) = align_chunks_binary(ca_1, ca_2);\n    let chunks = ca_1\n        .downcast_iter()\n        .zip(ca_2.downcast_iter())\n        .map(|(arr_1, arr_2)| compute_kernel(arr_1, arr_2));\n    ChunkedArray::from_chunk_iter(ca_1.name(), chunks)\n}\n\npub fn compute_expr_2_args(arg_1: &amp;Series, arg_2: &amp;Series) -&gt; Series {\n    // Dispatch the numerical series to `compute_chunked_array_2_args`.\n    with_match_physical_numeric_polars_type!(arg_1.dtype(), |$T| {\n        let ca_1: &amp;ChunkedArray&lt;$T&gt; = arg_1.as_ref().as_ref().as_ref();\n        let ca_2: &amp;ChunkedArray&lt;$T&gt; = arg_2.as_ref().as_ref().as_ref();\n        compute_chunked_array_2_args(ca_1, ca_2).into_series()\n    })\n}\n</code></pre>"},{"location":"development/contributing/ide/","title":"IDE configuration","text":"<p>Using an integrated development environments (IDE) and configuring it properly will help you work on Polars more effectively. This page contains some recommendations for configuring popular IDEs.</p>"},{"location":"development/contributing/ide/#visual-studio-code","title":"Visual Studio Code","text":"<p>Make sure to configure VSCode to use the virtual environment created by the Makefile.</p>"},{"location":"development/contributing/ide/#extensions","title":"Extensions","text":"<p>The extensions below are recommended.</p>"},{"location":"development/contributing/ide/#rust-analyzer","title":"rust-analyzer","text":"<p>If you work on the Rust code at all, you will need the rust-analyzer extension. This extension provides code completion for the Rust code.</p> <p>For it to work well for the Polars code base, add the following settings to your <code>.vscode/settings.json</code>:</p> <pre><code>{\n    \"rust-analyzer.cargo.features\": \"all\",\n}\n</code></pre>"},{"location":"development/contributing/ide/#ruff","title":"Ruff","text":"<p>The Ruff extension will help you conform to the formatting requirements of the Python code. We use both the Ruff linter and formatter. It is recommended to configure the extension to use the Ruff installed in your environment. This will make it use the correct Ruff version and configuration.</p> <pre><code>{\n    \"ruff.importStrategy\": \"fromEnvironment\",\n}\n</code></pre>"},{"location":"development/contributing/ide/#codelldb","title":"CodeLLDB","text":"<p>The CodeLLDB extension is useful for debugging Rust code. You can also debug Rust code called from Python (see section below).</p>"},{"location":"development/contributing/ide/#debugging","title":"Debugging","text":"<p>Due to the way that Python and Rust interoperate, debugging the Rust side of development from Python calls can be difficult. This guide shows how to set up a debugging environment that makes debugging Rust code called from a Python script painless.</p>"},{"location":"development/contributing/ide/#preparation","title":"Preparation","text":"<p>Start by installing the CodeLLDB extension (see above). Then add the following two configurations to your <code>launch.json</code> file. This file is usually found in the <code>.vscode</code> folder of your project root. See the official VSCode documentation for more information about the <code>launch.json</code> file.</p> <code>launch.json</code> <pre><code>{\n    \"configurations\": [\n        {\n            \"name\": \"Debug Rust/Python\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\",\n            \"program\": \"${workspaceFolder}/py-polars/debug/launch.py\",\n            \"args\": [\n                \"${file}\"\n            ],\n            \"console\": \"internalConsole\",\n            \"justMyCode\": true,\n            \"serverReadyAction\": {\n                \"pattern\": \"pID = ([0-9]+)\",\n                \"action\": \"startDebugging\",\n                \"name\": \"Rust LLDB\"\n            }\n        },\n        {\n            \"name\": \"Rust LLDB\",\n            \"pid\": \"0\",\n            \"type\": \"lldb\",\n            \"request\": \"attach\",\n            \"program\": \"${workspaceFolder}/py-polars/.venv/bin/python\",\n            \"stopOnEntry\": false,\n            \"sourceLanguages\": [\n                \"rust\"\n            ],\n            \"presentation\": {\n                \"hidden\": true\n            }\n        }\n    ]\n}\n</code></pre> <p>Info</p> <p>On some systems, the LLDB debugger will not attach unless ptrace protection is disabled. To disable, run the following command:</p> <pre><code>echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n</code></pre>"},{"location":"development/contributing/ide/#running-the-debugger","title":"Running the debugger","text":"<ol> <li> <p>Create a Python script containing Polars code. Ensure that your virtual environment is activated.</p> </li> <li> <p>Set breakpoints in any <code>.rs</code> or <code>.py</code> file.</p> </li> <li> <p>In the <code>Run and Debug</code> panel on the left, select <code>Debug Rust/Python</code> from the drop-down menu on top and click the <code>Start Debugging</code> button.</p> </li> </ol> <p>At this point, your debugger should stop on breakpoints in any <code>.rs</code> file located within the codebase.</p>"},{"location":"development/contributing/ide/#details","title":"Details","text":"<p>The debugging feature runs via the specially-designed VSCode launch configuration shown above. The initial Python debugger is launched using a special launch script located at <code>py-polars/debug/launch.py</code> and passes the name of the script to be debugged (the target script) as an input argument. The launch script determines the process ID, writes this value into the <code>launch.json</code> configuration file, compiles the target script and runs it in the current environment. At this point, a second (Rust) debugger is attached to the Python debugger. The result is two simultaneous debuggers operating on the same running instance. Breakpoints in the Python code will stop on the Python debugger and breakpoints in the Rust code will stop on the Rust debugger.</p>"},{"location":"development/contributing/ide/#pycharm-rustrover-clion","title":"PyCharm / RustRover / CLion","text":"<p>Info</p> <p>More information needed.</p>"},{"location":"development/contributing/test/","title":"Test suite","text":"<p>Info</p> <p>Additional information on the Rust test suite will be added to this page later.</p> <p>The <code>py-polars/tests</code> folder contains the main Polars test suite. This page contains some information on the various components of the test suite, as well as guidelines for writing new tests.</p> <p>The test suite contains four main components, each confined to their own folder: unit tests, parametric tests, benchmark tests, and doctests.</p> <p>Note that this test suite is indirectly responsible for testing Rust Polars as well. The Rust test suite is kept small to reduce compilation times. A lot of the Rust functionality is tested here instead.</p>"},{"location":"development/contributing/test/#unit-tests","title":"Unit tests","text":"<p>The <code>unit</code> folder contains all regular unit tests. These tests are intended to make sure all Polars functionality works as intended.</p>"},{"location":"development/contributing/test/#running-unit-tests","title":"Running unit tests","text":"<p>Run unit tests by running <code>make test</code> from the <code>py-polars</code> folder. This will compile the Rust bindings and then run the unit tests.</p> <p>If you're working in the Python code only, you can avoid recompiling every time by simply running <code>pytest</code> instead from your virtual environment.</p> <p>By default, slow tests are skipped. Slow tests are marked as such using a custom pytest marker. If you wish to run slow tests, run <code>pytest -m slow</code>. Or run <code>pytest -m \"\"</code> to run all tests, regardless of marker.</p> <p>Tests can be run in parallel by running <code>pytest -n auto</code>. The parallelization is handled by <code>pytest-xdist</code>.</p>"},{"location":"development/contributing/test/#writing-unit-tests","title":"Writing unit tests","text":"<p>Whenever you add new functionality, you should also add matching unit tests. Add your tests to appropriate test module in the <code>unit</code> folder. Some guidelines to keep in mind:</p> <ul> <li>Try to fully cover all possible inputs and edge cases you can think of.</li> <li>Utilize pytest tools like <code>fixture</code> and <code>parametrize</code> where appropriate.</li> <li>Since many tests will require some data to be defined first, it can be efficient to run multiple checks in a single test. This can also be addressed using pytest fixtures.</li> <li>Unit tests should not depend on external factors, otherwise test parallelization will break.</li> </ul>"},{"location":"development/contributing/test/#parametric-tests","title":"Parametric tests","text":"<p>The <code>parametric</code> folder contains parametric tests written using the Hypothesis framework. These tests are intended to find and test edge cases by generating many random datapoints.</p>"},{"location":"development/contributing/test/#running-parametric-tests","title":"Running parametric tests","text":"<p>Run parametric tests by running <code>pytest -m hypothesis</code>.</p> <p>Note that parametric tests are excluded by default when running <code>pytest</code>. You must explicitly specify <code>-m hypothesis</code> to run them.</p> <p>These tests will be included when calculating test coverage, and will also be run as part of the <code>make test-all</code> make command.</p>"},{"location":"development/contributing/test/#doctests","title":"Doctests","text":"<p>The <code>docs</code> folder contains a script for running <code>doctest</code>. This folder does not contain any actual tests - rather, the script checks all docstrings in the Polars package for <code>Examples</code> sections, runs the code examples, and verifies the output.</p> <p>The aim of running <code>doctest</code> is to make sure the <code>Examples</code> sections in our docstrings are valid and remain up-to-date with code changes.</p>"},{"location":"development/contributing/test/#running-doctest","title":"Running <code>doctest</code>","text":"<p>To run the <code>doctest</code> module, run <code>make doctest</code> from the <code>py-polars</code> folder. You can also run the script directly from your virtual environment.</p> <p>Note that doctests are not run using pytest. While pytest does have the capability to run doc examples, configuration options are too limited for our purposes.</p> <p>Doctests will not count towards test coverage. They are not a substitute for unit tests, but rather intended to convey the intended use of the Polars API to the user.</p>"},{"location":"development/contributing/test/#writing-doc-examples","title":"Writing doc examples","text":"<p>Almost all classes/methods/functions that are part of Polars' public API should include code examples in their docstring. These examples help users understand basic usage and allow us to illustrate more advanced concepts as well. Some guidelines for writing a good docstring <code>Examples</code> section:</p> <ul> <li>Start with a minimal example that showcases the default functionality.</li> <li>Showcase the effect of its parameters.</li> <li>Showcase any special interactions when combined with other code.</li> <li>Keep it succinct and avoid multiple examples showcasing the same thing.</li> </ul> <p>There are many great docstring examples already, just check other code if you need inspiration!</p> <p>In addition to the regular options available when writing doctests, the script configuration allows for a new <code>IGNORE_RESULT</code> directive. Use this directive if you want to ensure the code runs, but the output may be random by design or not interesting to check.</p> <pre><code>&gt;&gt;&gt; df.sample(n=2)  # doctest: +IGNORE_RESULT\n</code></pre>"},{"location":"development/contributing/test/#benchmark-tests","title":"Benchmark tests","text":"<p>The <code>benchmark</code> folder contains code for running various benchmark tests. The aim of this part of the test suite is to spot performance regressions in the code, and to verify that Polars functionality works as expected when run on a release build or at a larger scale.</p> <p>Polars uses CodSpeed for tracking the performance of the benchmark tests.</p>"},{"location":"development/contributing/test/#generating-data","title":"Generating data","text":"<p>For most tests, a relatively large dataset must be generated first. This is done as part of the <code>pytest</code> setup process.</p> <p>The data generation logic was taken from the H2O.ai database benchmark, which is the foundation for many of the benchmark tests.</p>"},{"location":"development/contributing/test/#running-the-benchmark-tests","title":"Running the benchmark tests","text":"<p>The benchmark tests can be run using pytest. Run <code>pytest -m benchmark --durations 0 -v</code> to run these tests and report run duration.</p> <p>Note that benchmark tests are excluded by default when running <code>pytest</code>. You must explicitly specify <code>-m benchmark</code> to run them. They will also be excluded when calculating test coverage.</p> <p>These tests will be run as part of the <code>make test-all</code> make command.</p>"},{"location":"releases/changelog/","title":"Changelog","text":"<p>Polars uses GitHub to manage both Python and Rust releases.</p> <p>Refer to our GitHub releases page for the changelog associated with each new release.</p>"},{"location":"releases/upgrade/","title":"About","text":"<p>Polars releases an upgrade guide alongside each breaking release. This guide is intended to help you upgrade from an older Polars version to the new version.</p> <p>Each guide contains all breaking changes that were not previously deprecated, as well as any significant new deprecations.</p> <p>A full list of all changes is available in the changelog.</p> <p>Tip</p> <p>It can be useful to upgrade to the latest non-breaking version before upgrading to a new breaking version. This way, you can run your code and address any deprecation warnings. The upgrade to the new breaking version should then go much more smoothly!</p> <p>Tip</p> <p>One of our maintainers has created a tool for automatically upgrading your Polars code to a later version. It's based on the well-known pyupgrade tool. Try out polars-upgrade and let us know what you think!</p> <p>Note</p> <p>There are no upgrade guides yet for Rust releases. These will be added once the rate of breaking changes to the Rust API slows down and a deprecation policy is added.</p>"},{"location":"releases/upgrade/0.19/","title":"Version 0.19","text":""},{"location":"releases/upgrade/0.19/#breaking-changes","title":"Breaking changes","text":""},{"location":"releases/upgrade/0.19/#aggregation-functions-no-longer-support-horizontal-computation","title":"Aggregation functions no longer support horizontal computation","text":"<p>This impacts aggregation functions like <code>sum</code>, <code>min</code>, and <code>max</code>. These functions were overloaded to support both vertical and horizontal computation. Recently, new dedicated functionality for horizontal computation was released, and horizontal computation was deprecated.</p> <p>Restore the old behavior by using the horizontal variant, e.g. <code>sum_horizontal</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({'a': [1, 2], 'b': [11, 12]})\n&gt;&gt;&gt; df.select(pl.sum('a', 'b'))  # horizontal computation\nshape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sum \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 12  \u2502\n\u2502 14  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({'a': [1, 2], 'b': [11, 12]})\n&gt;&gt;&gt; df.select(pl.sum('a', 'b'))  # vertical computation\nshape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2506 23  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.19/#update-to-all-any","title":"Update to <code>all</code> / <code>any</code>","text":"<p><code>all</code> will now ignore null values by default, rather than treat them as <code>False</code>.</p> <p>For both <code>any</code> and <code>all</code>, the <code>drop_nulls</code> parameter has been renamed to <code>ignore_nulls</code> and is now keyword-only. Also fixed an issue when setting this parameter to <code>False</code> would erroneously result in <code>None</code> output in some cases.</p> <p>To restore the old behavior, set <code>ignore_nulls</code> to <code>False</code> and check for <code>None</code> output.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.Series([True, None]).all()\nFalse\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.Series([True, None]).all()\nTrue\n</code></pre>"},{"location":"releases/upgrade/0.19/#improved-error-types-for-many-methods","title":"Improved error types for many methods","text":"<p>Improving our error messages is an ongoing effort. We did a sweep of our Python code base and made many improvements to error messages and error types. Most notably, many <code>ValueError</code>s were changed to <code>TypeError</code>s.</p> <p>If your code relies on handling Polars exceptions, you may have to make some adjustments.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.Series(values=15)\n...\nValueError: Series constructor called with unsupported type; got 'int'\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.Series(values=15)\n...\nTypeError: Series constructor called with unsupported type 'int' for the `values` parameter\n</code></pre>"},{"location":"releases/upgrade/0.19/#updates-to-expression-input-parsing","title":"Updates to expression input parsing","text":"<p>Methods like <code>select</code> and <code>with_columns</code> accept one or more expressions. But they also accept strings, integers, lists, and other inputs that we try to interpret as expressions. We updated our internal logic to parse inputs more consistently.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.DataFrame({'a': [1, 2]}).with_columns(None)\nshape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2502\n\u2502 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.DataFrame({'a': [1, 2]}).with_columns(None)\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 literal \u2502\n\u2502 --- \u2506 ---     \u2502\n\u2502 i64 \u2506 null    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 null    \u2502\n\u2502 2   \u2506 null    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.19/#shuffle-sample-now-use-an-internal-polars-seed","title":"<code>shuffle</code> / <code>sample</code> now use an internal Polars seed","text":"<p>If you used the built-in Python <code>random.seed</code> function to control the randomness of Polars expressions, this will no longer work. Instead, use the new <code>set_random_seed</code> function.</p> <p>Example</p> <p>Before:</p> <pre><code>import random\n\nrandom.seed(1)\n</code></pre> <p>After:</p> <pre><code>import polars as pl\n\npl.set_random_seed(1)\n</code></pre>"},{"location":"releases/upgrade/0.19/#deprecations","title":"Deprecations","text":"<p>Creating a consistent and intuitive API is hard; finding the right name for each function, method, and parameter might be the hardest part. The new version comes with several naming changes, and you will most likely run into deprecation warnings when upgrading to <code>0.19</code>.</p> <p>If you want to upgrade without worrying about deprecation warnings right now, you can add the following snippet to your code:</p> <pre><code>import warnings\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n</code></pre>"},{"location":"releases/upgrade/0.19/#groupby-renamed-to-group_by","title":"<code>groupby</code> renamed to <code>group_by</code>","text":"<p>This is not a change we make lightly, as it will impact almost all our users. But \"group by\" is really two different words, and our naming strategy dictates that these should be separated by an underscore.</p> <p>Most likely, a simple search and replace will be enough to take care of this update:</p> <ul> <li>Search: <code>.groupby(</code></li> <li>Replace: <code>.group_by(</code></li> </ul>"},{"location":"releases/upgrade/0.19/#apply-renamed-to-map_","title":"<code>apply</code> renamed to <code>map_*</code>","text":"<p><code>apply</code> is probably the most misused part of our API. Many Polars users come from pandas, where <code>apply</code> has a completely different meaning.</p> <p>We now consolidate all our functionality for user-defined functions under the name <code>map</code>. This results in the following renaming:</p> Before After <code>Series/Expr.apply</code> <code>map_elements</code> <code>Series/Expr.rolling_apply</code> <code>rolling_map</code> <code>DataFrame.apply</code> <code>map_rows</code> <code>GroupBy.apply</code> <code>map_groups</code> <code>apply</code> <code>map_groups</code> <code>map</code> <code>map_batches</code>"},{"location":"releases/upgrade/0.20/","title":"Version 0.20","text":""},{"location":"releases/upgrade/0.20/#breaking-changes","title":"Breaking changes","text":""},{"location":"releases/upgrade/0.20/#change-default-join-behavior-with-regard-to-null-values","title":"Change default <code>join</code> behavior with regard to null values","text":"<p>Previously, null values in the join key were considered a value like any other value. This meant that null values in the left frame would be joined with null values in the right frame. This is expensive and does not match default behavior in SQL.</p> <p>Default behavior has now been changed to ignore null values in the join key. The previous behavior can be retained by setting <code>join_nulls=True</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df1 = pl.DataFrame({\"a\": [1, 2, None], \"b\": [4, 4, 4]})\n&gt;&gt;&gt; df2 = pl.DataFrame({\"a\": [None, 2, 3], \"c\": [5, 5, 5]})\n&gt;&gt;&gt; df1.join(df2, on=\"a\", how=\"inner\")\nshape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a    \u2506 b   \u2506 c   \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 i64  \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null \u2506 4   \u2506 5   \u2502\n\u2502 2    \u2506 4   \u2506 5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df1.join(df2, on=\"a\", how=\"inner\")\nshape: (1, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2506 c   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2   \u2506 4   \u2506 5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df1.join(df2, on=\"a\", how=\"inner\", join_nulls=True)  # Keeps previous behavior\nshape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a    \u2506 b   \u2506 c   \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 i64  \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null \u2506 4   \u2506 5   \u2502\n\u2502 2    \u2506 4   \u2506 5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.20/#preserve-left-and-right-join-keys-in-outer-joins","title":"Preserve left and right join keys in outer joins","text":"<p>Previously, the result of an outer join did not contain the join keys of the left and right frames. Rather, it contained a coalesced version of the left key and right key. This loses information and does not conform to default SQL behavior.</p> <p>The behavior has been changed to include the original join keys. Name clashes are solved by appending a suffix (<code>_right</code> by default) to the right join key name. The previous behavior can be retained by setting <code>how=\"outer_coalesce\"</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df1 = pl.DataFrame({\"L1\": [\"a\", \"b\", \"c\"], \"L2\": [1, 2, 3]})\n&gt;&gt;&gt; df2 = pl.DataFrame({\"L1\": [\"a\", \"c\", \"d\"], \"R2\": [7, 8, 9]})\n&gt;&gt;&gt; df1.join(df2, on=\"L1\", how=\"outer\")\nshape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 L1  \u2506 L2   \u2506 R2   \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 1    \u2506 7    \u2502\n\u2502 c   \u2506 3    \u2506 8    \u2502\n\u2502 d   \u2506 null \u2506 9    \u2502\n\u2502 b   \u2506 2    \u2506 null \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df1.join(df2, on=\"L1\", how=\"outer\")\nshape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 L1   \u2506 L2   \u2506 L1_right \u2506 R2   \u2502\n\u2502 ---  \u2506 ---  \u2506 ---      \u2506 ---  \u2502\n\u2502 str  \u2506 i64  \u2506 str      \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 1    \u2506 a        \u2506 7    \u2502\n\u2502 b    \u2506 2    \u2506 null     \u2506 null \u2502\n\u2502 c    \u2506 3    \u2506 c        \u2506 8    \u2502\n\u2502 null \u2506 null \u2506 d        \u2506 9    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df1.join(df2, on=\"a\", how=\"outer_coalesce\")  # Keeps previous behavior\nshape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 L1  \u2506 L2   \u2506 R2   \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 1    \u2506 7    \u2502\n\u2502 c   \u2506 3    \u2506 8    \u2502\n\u2502 d   \u2506 null \u2506 9    \u2502\n\u2502 b   \u2506 2    \u2506 null \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.20/#count-now-ignores-null-values","title":"<code>count</code> now ignores null values","text":"<p>The <code>count</code> method for <code>Expr</code> and <code>Series</code> now ignores null values. Use <code>len</code> to get the count with null values included.</p> <p>Note that <code>pl.count()</code> and <code>group_by(...).count()</code> are unchanged. These count the number of rows in the context, so nulls are not applicable in the same way.</p> <p>This brings behavior more in line with the SQL standard, where <code>COUNT(col)</code> ignores null values but <code>COUNT(*)</code> counts rows regardless of null values.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2, None]})\n&gt;&gt;&gt; df.select(pl.col(\"a\").count())\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df.select(pl.col(\"a\").count())\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.select(pl.col(\"a\").len())  # Mirrors previous behavior\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.20/#nan-values-are-now-considered-equal","title":"<code>NaN</code> values are now considered equal","text":"<p>Floating point <code>NaN</code> values were treated as unequal across Polars operations. This has been corrected to better match user expectation and existing standards.</p> <p>While this is considered a bug fix, it is included in this guide in order to draw attention to possible impact on user workflows that may contain <code>NaN</code> values.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series([1.0, float(\"nan\"), float(\"inf\")])\n&gt;&gt;&gt; s == s\nshape: (3,)\nSeries: '' [bool]\n[\n        true\n        false\n        true\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s == s\nshape: (3,)\nSeries: '' [bool]\n[\n        true\n        true\n        true\n]\n</code></pre>"},{"location":"releases/upgrade/0.20/#assertion-utils-updates-to-exact-checking-and-nan-equality","title":"Assertion utils updates to exact checking and <code>NaN</code> equality","text":"<p>The assertion utility functions <code>assert_frame_equal</code> and <code>assert_series_equal</code> would use the tolerance parameters <code>atol</code> and <code>rtol</code> to do approximate checking, unless <code>check_exact</code> was set to <code>True</code>. This could lead to some surprising behavior, as integers are generally thought of as exact values. Integer values are now always checked exactly. To do inexact checking, convert to float first.</p> <p>Additionally, the <code>nans_compare_equal</code> parameter has been removed and <code>NaN</code> values are now always considered equal, which was the previous default behavior. This parameter had previously been deprecated but has been removed before the end of the standard deprecation period to facilitate the change to <code>NaN</code> equality.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; from polars.testing import assert_frame_equal\n&gt;&gt;&gt; df1 = pl.DataFrame({\"id\": [123456]})\n&gt;&gt;&gt; df2 = pl.DataFrame({\"id\": [123457]})\n&gt;&gt;&gt; assert_frame_equal(df1, df2)  # Passes\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; assert_frame_equal(df1, df2)\n...\nAssertionError: DataFrames are different (value mismatch for column 'id')\n[left]:  [123456]\n[right]: [123457]\n</code></pre>"},{"location":"releases/upgrade/0.20/#allow-all-datatype-objects-to-be-instantiated","title":"Allow all <code>DataType</code> objects to be instantiated","text":"<p>Polars data types are subclasses of the <code>DataType</code> class. We had a 'hack' in place that automatically converted data types instantiated without any arguments to the <code>class</code>, rather than actually instantiating it. The idea was to allow specifying data types as <code>Int64</code> rather than <code>Int64()</code>, which is more succinct. However, this caused some unexpected behavior when working directly with data type objects, especially as there was a discrepancy with data types like <code>Datetime</code> which will be instantiated in many cases.</p> <p>Going forward, instantiating a data type will always return an instance of that class. Both classes an instances are handled by Polars, so the previous short syntax is still available. Methods that return data types like <code>Series.dtype</code> and <code>DataFrame.schema</code> now always return instantiated data types objects.</p> <p>You may have to update some of your data type checks if you were not already using the equality operator (<code>==</code>), as well as update some type hints.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series([1, 2, 3], dtype=pl.Int8)\n&gt;&gt;&gt; s.dtype == pl.Int8\nTrue\n&gt;&gt;&gt; s.dtype is pl.Int8\nTrue\n&gt;&gt;&gt; isinstance(s.dtype, pl.Int8)\nFalse\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.dtype == pl.Int8\nTrue\n&gt;&gt;&gt; s.dtype is pl.Int8\nFalse\n&gt;&gt;&gt; isinstance(s.dtype, pl.Int8)\nTrue\n</code></pre>"},{"location":"releases/upgrade/0.20/#update-constructors-for-decimal-and-array-data-types","title":"Update constructors for <code>Decimal</code> and <code>Array</code> data types","text":"<p>The data types <code>Decimal</code> and <code>Array</code> have had their parameters switched around. The new constructors should more closely match user expectations.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.Array(2, pl.Int16)\nArray(Int16, 2)\n&gt;&gt;&gt; pl.Decimal(5, 10)\nDecimal(precision=10, scale=5)\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.Array(pl.Int16, 2)\nArray(Int16, width=2)\n&gt;&gt;&gt; pl.Decimal(10, 5)\nDecimal(precision=10, scale=5)\n</code></pre>"},{"location":"releases/upgrade/0.20/#datatypeis_nested-changed-from-a-property-to-a-class-method","title":"<code>DataType.is_nested</code> changed from a property to a class method","text":"<p>This is a minor change, but a very important one to properly update. Failure to update accordingly may result in faulty logic, as Python will evaluate the method to <code>True</code>. For example, <code>if dtype.is_nested</code> will now evaluate to <code>True</code> regardless of the data type, because it returns the method, which Python considers truthy.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.List(pl.Int8).is_nested\nTrue\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.List(pl.Int8).is_nested()\nTrue\n</code></pre>"},{"location":"releases/upgrade/0.20/#smaller-integer-data-types-for-datetime-components-dtmonth-dtweek","title":"Smaller integer data types for datetime components <code>dt.month</code>, <code>dt.week</code>","text":"<p>Most datetime components such as <code>month</code> and <code>week</code> would previously return a <code>UInt32</code> type. This has been updated to the smallest appropriate signed integer type. This should reduce memory consumption.</p> Method Dtype old Dtype new year i32 i32 iso_year i32 i32 quarter u32 i8 month u32 i8 week u32 i8 day u32 i8 weekday u32 i8 ordinal_day u32 i16 hour u32 i8 minute u32 i8 second u32 i8 millisecond u32 i32* microsecond u32 i32 nanosecond u32 i32 <p>*Technically, <code>millisecond</code> can be an <code>i16</code>. This may be updated in the future.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; from datetime import date\n&gt;&gt;&gt; s = pl.Series([date(2023, 12, 31), date(2024, 1, 1)])\n&gt;&gt;&gt; s.dt.month()\nshape: (2,)\nSeries: '' [u32]\n[\n        12\n        1\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.dt.month()\nshape: (2,)\nSeries: '' [u8]\n[\n        12\n        1\n]\n</code></pre>"},{"location":"releases/upgrade/0.20/#series-now-defaults-to-null-data-type-when-no-data-is-present","title":"Series now defaults to <code>Null</code> data type when no data is present","text":"<p>This replaces the previous behavior of initializing as a <code>Float32</code> type.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; pl.Series(\"a\", [None])\nshape: (1,)\nSeries: 'a' [f32]\n[\n        null\n]\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; pl.Series(\"a\", [None])\nshape: (1,)\nSeries: 'a' [null]\n[\n        null\n]\n</code></pre>"},{"location":"releases/upgrade/0.20/#replace-reimplemented-with-slightly-different-behavior","title":"<code>replace</code> reimplemented with slightly different behavior","text":"<p>The new implementation is mostly backwards compatible. Please do note the following:</p> <ol> <li>The logic for determining the return data type has changed. You may want to specify <code>return_dtype</code> to override the inferred data type, or take advantage of the new function signature (separate <code>old</code> and <code>new</code> parameters) to influence the return type.</li> <li>The previous workaround for referencing other columns as default by using a struct column no longer works. It now simply works as expected, no workaround needed.</li> </ol> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\"a\": [1, 2, 2, 3], \"b\": [1.5, 2.5, 5.0, 1.0]}, schema={\"a\": pl.Int8, \"b\": pl.Float64})\n&gt;&gt;&gt; df.select(pl.col(\"a\").replace({2: 100}))\nshape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 i8  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2502\n\u2502 100 \u2502\n\u2502 100 \u2502\n\u2502 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.select(pl.struct(\"a\", \"b\").replace({2: 100}, default=pl.col(\"b\")))\nshape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a     \u2502\n\u2502 ---   \u2502\n\u2502 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.5   \u2502\n\u2502 100.0 \u2502\n\u2502 100.0 \u2502\n\u2502 1.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; df.select(pl.col(\"a\").replace({2: 100}))\nshape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2502\n\u2502 100 \u2502\n\u2502 100 \u2502\n\u2502 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.select(pl.col(\"a\").replace({2: 100}, default=pl.col(\"b\")))  # No struct needed\nshape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a     \u2502\n\u2502 ---   \u2502\n\u2502 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.5   \u2502\n\u2502 100.0 \u2502\n\u2502 100.0 \u2502\n\u2502 1.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.20/#value_counts-resulting-column-renamed-from-counts-to-count","title":"<code>value_counts</code> resulting column renamed from <code>counts</code> to <code>count</code>","text":"<p>The resulting struct field for the <code>value_counts</code> method has been renamed from <code>counts</code> to <code>count</code>.</p> <p>Example</p> <p>Before:</p> <pre><code>&gt;&gt;&gt; s = pl.Series(\"a\", [\"x\", \"x\", \"y\"])\n&gt;&gt;&gt; s.value_counts()\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 counts \u2502\n\u2502 --- \u2506 ---    \u2502\n\u2502 str \u2506 u32    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 x   \u2506 2      \u2502\n\u2502 y   \u2506 1      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>After:</p> <pre><code>&gt;&gt;&gt; s.value_counts()\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 count \u2502\n\u2502 --- \u2506 ---   \u2502\n\u2502 str \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 x   \u2506 2     \u2502\n\u2502 y   \u2506 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"releases/upgrade/0.20/#update-read_parquet-to-use-object-store-rather-than-fsspec","title":"Update <code>read_parquet</code> to use Object Store rather than fsspec","text":"<p>If you were using <code>read_parquet</code>, installing <code>fsspec</code> as an optional dependency is no longer required. The new Object Store implementation was already in use for <code>scan_parquet</code>. It may have slightly different behavior in certain cases, such as how credentials are detected and how downloads are performed.</p> <p>The resulting <code>DataFrame</code> should be identical between versions.</p>"},{"location":"releases/upgrade/0.20/#deprecations","title":"Deprecations","text":""},{"location":"releases/upgrade/0.20/#cumulative-functions-renamed-from-cum-to-cum_","title":"Cumulative functions renamed from <code>cum*</code> to <code>cum_*</code>","text":"<p>Technically, this deprecation was introduced in version <code>0.19.14</code>, but many users will first encounter it when upgrading to <code>0.20</code>. It's a relatively impactful change, which is why we mention it here.</p> Old name New name <code>cumfold</code> <code>cum_fold</code> <code>cumreduce</code> <code>cum_reduce</code> <code>cumsum</code> <code>cum_sum</code> <code>cumprod</code> <code>cum_prod</code> <code>cummin</code> <code>cum_min</code> <code>cummax</code> <code>cum_max</code> <code>cumcount</code> <code>cum_count</code>"},{"location":"user-guide/ecosystem/","title":"Ecosystem","text":""},{"location":"user-guide/ecosystem/#introduction","title":"Introduction","text":"<p>On this page you can find a non-exhaustive list of libraries and tools that support Polars. As the data ecosystem is evolving fast, more libraries will likely support Polars in the future. One of the main drivers is that Polars makes use of <code>Apache Arrow</code> in it's backend.</p>"},{"location":"user-guide/ecosystem/#table-of-contents","title":"Table of contents:","text":"<ul> <li>Apache Arrow</li> <li>Data visualisation</li> <li>IO</li> <li>Machine learning</li> <li>Other</li> </ul>"},{"location":"user-guide/ecosystem/#apache-arrow","title":"Apache Arrow","text":"<p>Apache Arrow enables zero-copy reads of data within the same process, meaning that data can be directly accessed in its in-memory format without the need for copying or serialisation. This enhances performance when integrating with different tools using Apache Arrow. Polars is compatible with a wide range of libraries that also make use of Apache Arrow, like Pandas and DuckDB.</p>"},{"location":"user-guide/ecosystem/#data-visualisation","title":"Data visualisation","text":""},{"location":"user-guide/ecosystem/#hvplot","title":"hvPlot","text":"<p>hvPlot is available as the default plotting backend for Polars making it simple to create interactive and static visualisations. You can use hvPlot by using the feature flag <code>plot</code> during installing.</p> <pre><code>pip install 'polars[plot]'\n</code></pre>"},{"location":"user-guide/ecosystem/#matplotlib","title":"Matplotlib","text":"<p>Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible.</p>"},{"location":"user-guide/ecosystem/#plotly","title":"Plotly","text":"<p>Plotly is an interactive, open-source, and browser-based graphing library for Python. Built on top of plotly.js, it ships with over 30 chart types, including scientific charts, 3D graphs, statistical charts, SVG maps, financial charts, and more.</p>"},{"location":"user-guide/ecosystem/#seaborn","title":"Seaborn","text":"<p>Seaborn is a Python data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.</p>"},{"location":"user-guide/ecosystem/#io","title":"IO","text":""},{"location":"user-guide/ecosystem/#delta-lake","title":"Delta Lake","text":"<p>The Delta Lake project aims to unlock the power of the Deltalake for as many users and projects as possible by providing native low-level APIs aimed at developers and integrators, as well as a high-level operations API that lets you query, inspect, and operate your Delta Lake with ease.</p> <p>Read how to use Delta Lake with Polars at Delta Lake.</p>"},{"location":"user-guide/ecosystem/#machine-learning","title":"Machine Learning","text":""},{"location":"user-guide/ecosystem/#scikit-learn","title":"Scikit Learn","text":"<p>Since Scikit Learn 1.4, all transformers support Polars output. See the change log for more details.</p>"},{"location":"user-guide/ecosystem/#other","title":"Other","text":""},{"location":"user-guide/ecosystem/#duckdb","title":"DuckDB","text":"<p>DuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs), and more. Read about integration with Polars on the DuckDB website.</p>"},{"location":"user-guide/ecosystem/#great-tables","title":"Great Tables","text":"<p>With Great Tables anyone can make wonderful-looking tables in Python. Here is a blog post on how to use Great Tables with Polars.</p>"},{"location":"user-guide/ecosystem/#lancedb","title":"LanceDB","text":"<p>LanceDB is a developer-friendly, serverless vector database for AI applications. They have added a direct integration with Polars. LanceDB can ingest Polars dataframes, return results as polars dataframes, and export the entire table as a polars lazyframe. You can find a quick tutorial in their blog LanceDB + Polars</p>"},{"location":"user-guide/ecosystem/#mage","title":"Mage","text":"<p>Mage is an open-source data pipeline tool for transforming and integrating data. Learn about integration between Polars and Mage at docs.mage.ai.</p>"},{"location":"user-guide/getting-started/","title":"Getting started","text":"<p>\u3053\u306e\u7ae0\u3067\u306f Polars \u306e\u306f\u3058\u3081\u65b9\u3092\u89e3\u8aac\u3057\u307e\u3059\u3002\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u57fa\u672c\u7684\u306a\u7279\u5fb4\u3084\u6a5f\u80fd\u3092\u7d39\u4ecb\u3057\u3001\u65b0\u3057\u3044\u30e6\u30fc\u30b6\u30fc\u304c\u6700\u521d\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304b\u3089\u30b3\u30a2\u306a\u6a5f\u80fd\u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u306a\u308b\u307e\u3067\u306e\u57fa\u790e\u3092\u7fd2\u5f97\u3057\u3084\u3059\u304f\u3057\u307e\u3059\u3002\u3082\u3057\u3042\u306a\u305f\u304c\u65e2\u306b\u767a\u5c55\u7684\u306a\u5185\u5bb9\u306b\u7fd2\u719f\u3057\u3066\u3044\u305f\u308a\u3001\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u99b4\u67d3\u307f\u304c\u3042\u308b\u306a\u3089\u3070\u3001\u6b21\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u7ae0\u306b\u30b9\u30ad\u30c3\u30d7\u3057\u3066\u3082\u554f\u984c\u3042\u308a\u307e\u305b\u3093\u3002</p>"},{"location":"user-guide/getting-started/#polars","title":"Polars \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"Python Rust <pre><code>pip install polars\n</code></pre> <pre><code>cargo add polars -F lazy\n\n# Or Cargo.toml\n[dependencies]\npolars = { version = \"x\", features = [\"lazy\", ...]}\n</code></pre>"},{"location":"user-guide/getting-started/#_1","title":"\u8aad\u307f\u8fbc\u307f\u3068\u66f8\u304d\u8fbc\u307f","text":"<p>Polars \u306f\u4e00\u822c\u7684\u306a\u30d5\u30a1\u30a4\u30eb\u5f62\u5f0f\uff08\u4f8b\uff1acsv, json, parquet\uff09\u3001\u30af\u30e9\u30a6\u30c9\u30b9\u30c8\u30ec\u30fc\u30b8\uff08\u4f8b\uff1aS3\u3001Azure Blob\u3001BigQuery\uff09\u304a\u3088\u3073\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\uff08\u4f8b\uff1apostgres, mysql\uff09\u306e\u8aad\u307f\u66f8\u304d\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\u4ee5\u4e0b\u306f\u30c7\u30a3\u30b9\u30af\u3078\u306e\u8aad\u307f\u66f8\u304d\u306e\u30b3\u30f3\u30bb\u30d7\u30c8\u3092\u793a\u3057\u307e\u3059\u3002</p>  Python Rust <p> <code>DataFrame</code> <pre><code>import polars as pl\nfrom datetime import datetime\n\ndf = pl.DataFrame(\n    {\n        \"integer\": [1, 2, 3],\n        \"date\": [\n            datetime(2025, 1, 1),\n            datetime(2025, 1, 2),\n            datetime(2025, 1, 3),\n        ],\n        \"float\": [4.0, 5.0, 6.0],\n        \"string\": [\"a\", \"b\", \"c\"],\n    }\n)\n\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use std::fs::File;\n\nuse chrono::prelude::*;\nuse polars::prelude::*;\n\nlet mut df: DataFrame = df!(\n    \"integer\" =&gt; &amp;[1, 2, 3],\n    \"date\" =&gt; &amp;[\n            NaiveDate::from_ymd_opt(2025, 1, 1).unwrap().and_hms_opt(0, 0, 0).unwrap(),\n            NaiveDate::from_ymd_opt(2025, 1, 2).unwrap().and_hms_opt(0, 0, 0).unwrap(),\n            NaiveDate::from_ymd_opt(2025, 1, 3).unwrap().and_hms_opt(0, 0, 0).unwrap(),\n    ],\n    \"float\" =&gt; &amp;[4.0, 5.0, 6.0],\n    \"string\" =&gt; &amp;[\"a\", \"b\", \"c\"],\n)\n.unwrap();\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2506 string \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2506 ---    \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2025-01-01 00:00:00 \u2506 4.0   \u2506 a      \u2502\n\u2502 2       \u2506 2025-01-02 00:00:00 \u2506 5.0   \u2506 b      \u2502\n\u2502 3       \u2506 2025-01-03 00:00:00 \u2506 6.0   \u2506 c      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>\u6b21\u306e\u4f8b\u3067\u306f\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092 <code>output.csv</code> \u3068\u3044\u3046\u540d\u524d\u306e csv \u30d5\u30a1\u30a4\u30eb\u306b\u51fa\u529b\u3057\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u5f8c\u306b <code>read_csv</code> \u3092\u7528\u3044\u3066\u518d\u5ea6\u305d\u308c\u3092\u8aad\u307f\u8fbc\u307f\u3001\u78ba\u8a8d\u306e\u305f\u3081\u306b <code>print</code> \u3067\u8868\u793a\u3057\u3066\u3044\u307e\u3059\u3001</p>  Python Rust <p> <code>read_csv</code> \u00b7 <code>write_csv</code> <pre><code>df.write_csv(\"docs/data/output.csv\")\ndf_csv = pl.read_csv(\"docs/data/output.csv\")\nprint(df_csv)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7 <code>CsvWriter</code> \u00b7  Available on feature csv <pre><code>let mut file = File::create(\"docs/data/output.csv\").expect(\"could not create file\");\nCsvWriter::new(&amp;mut file)\n    .include_header(true)\n    .with_separator(b',')\n    .finish(&amp;mut df)?;\nlet df_csv = CsvReader::from_path(\"docs/data/output.csv\")?\n    .infer_schema(None)\n    .has_header(true)\n    .finish()?;\nprintln!(\"{}\", df_csv);\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                       \u2506 float \u2506 string \u2502\n\u2502 ---     \u2506 ---                        \u2506 ---   \u2506 ---    \u2502\n\u2502 i64     \u2506 str                        \u2506 f64   \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2025-01-01T00:00:00.000000 \u2506 4.0   \u2506 a      \u2502\n\u2502 2       \u2506 2025-01-02T00:00:00.000000 \u2506 5.0   \u2506 b      \u2502\n\u2502 3       \u2506 2025-01-03T00:00:00.000000 \u2506 6.0   \u2506 c      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>CSV \u30d5\u30a1\u30a4\u30eb\u5f62\u5f0f\u3084\u4ed6\u306e\u30c7\u30fc\u30bf\u5f62\u5f0f\u306e\u4f8b\u3092\u3082\u3063\u3068\u78ba\u8a8d\u3057\u305f\u3044\u5834\u5408\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u30ac\u30a4\u30c9\u306eIO \u306e\u7ae0\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"user-guide/getting-started/#expressions","title":"Expressions","text":"<p><code>Expression</code> \u306f Polars \u306e\u30b3\u30a2\u306a\u5f37\u307f\u3067\u3059\u3002\u3053\u306e <code>Expression</code> \u306f\u5358\u7d14\u306a\u30b3\u30f3\u30bb\u30d7\u30c8\u3092\u7d44\u5408\u305b\u3066\u8907\u96d1\u306a\u30af\u30a8\u30ea\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3092\u53ef\u80fd\u306b\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u69cb\u9020\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002\u4ee5\u4e0b\u306f\u5168\u3066\u306e\u30af\u30a8\u30ea\u306e\u69cb\u6210\u8981\u7d20\u3092\u63d0\u4f9b\u3059\u308b\u57fa\u672c\u7684\u306a\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\uff08Polars \u306e\u7528\u8a9e\u3067 Context \u3068\u547c\u3076\uff09\u3067\u3059\uff1a</p> <ul> <li><code>select</code></li> <li><code>filter</code></li> <li><code>with_columns</code></li> <li><code>group_by</code></li> </ul> <p>Expression \u3084 Context \u304c\u3069\u306e\u3088\u3046\u306b\u6a5f\u80fd\u3059\u308b\u304b\u3092\u3088\u308a\u8a73\u3057\u304f\u5b66\u3076\u306b\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u30ac\u30a4\u30c9\u306e Contexts \u304a\u3088\u3073 Expressions \u30bb\u30af\u30b7\u30e7\u30f3\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"user-guide/getting-started/#select","title":"Select","text":"<p>\u30ab\u30e9\u30e0\u3092 select \u3059\u308b\u305f\u3081\u306b\u306f\uff12\u3064\u306e\u3053\u3068\u3092\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u5bfe\u8c61\u306e <code>DataFrame</code> \u3092\u5b9a\u7fa9\u3059\u308b</li> <li>\u5fc5\u8981\u306a\u30c7\u30fc\u30bf\u3092\u9078\u629e\u3059\u308b</li> </ol> <p>\u4ee5\u4e0b\u306e\u4f8b\u3067\u306f select \u6642\u306b <code>col('*')</code> \u3068\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\u30a2\u30b9\u30bf\u30ea\u30b9\u30af\u306f\u5168\u3066\u306e\u30ab\u30e9\u30e0\u3092\u610f\u5473\u3057\u307e\u3059\u3002</p>  Python Rust <p> <code>select</code> <pre><code>df.select(pl.col(\"*\"))\n</code></pre></p> <p> <code>select</code> <pre><code>let out = df.clone().lazy().select([col(\"*\")]).collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 c                   \u2506 d     \u2502\n\u2502 --- \u2506 ---      \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64 \u2506 f64      \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.851098 \u2506 2025-12-01 00:00:00 \u2506 1.0   \u2502\n\u2502 1   \u2506 0.207886 \u2506 2025-12-02 00:00:00 \u2506 2.0   \u2502\n\u2502 2   \u2506 0.758995 \u2506 2025-12-03 00:00:00 \u2506 NaN   \u2502\n\u2502 3   \u2506 0.818029 \u2506 2025-12-04 00:00:00 \u2506 -42.0 \u2502\n\u2502 4   \u2506 0.265473 \u2506 2025-12-05 00:00:00 \u2506 null  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>\u7279\u5b9a\u306e\u30ab\u30e9\u30e0\u3092\u6307\u5b9a\u3057\u3066\u53d6\u5f97\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002\u3053\u308c\u3092\u3059\u308b\u306b\u306f\uff12\u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002\uff11\u3064\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u30ab\u30e9\u30e0\u306e\u540d\u524d\u3092\u6e21\u3059\u65b9\u6cd5\u3067\u3059\u3002You can also specify the specific columns that you want to return. There are two ways to do this. The first option is to pass the column names, as seen below.</p>  Python Rust <p> <code>select</code> <pre><code>df.select(pl.col(\"a\", \"b\"))\n</code></pre></p> <p> <code>select</code> <pre><code>let out = df.clone().lazy().select([col(\"a\"), col(\"b\")]).collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2502\n\u2502 --- \u2506 ---      \u2502\n\u2502 i64 \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.851098 \u2502\n\u2502 1   \u2506 0.207886 \u2502\n\u2502 2   \u2506 0.758995 \u2502\n\u2502 3   \u2506 0.818029 \u2502\n\u2502 4   \u2506 0.265473 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Follow these links to other parts of the user guide to learn more about basic operations or column selections.</p>"},{"location":"user-guide/getting-started/#filter","title":"Filter","text":"<p>The <code>filter</code> option allows us to create a subset of the <code>DataFrame</code>. We use the same <code>DataFrame</code> as earlier and we filter between two specified dates.</p>  Python Rust <p> <code>filter</code> <pre><code>df.filter(\n    pl.col(\"c\").is_between(datetime(2025, 12, 2), datetime(2025, 12, 3)),\n)\n</code></pre></p> <p> <code>filter</code> <pre><code>let start_date = NaiveDate::from_ymd_opt(2025, 12, 2)\n    .unwrap()\n    .and_hms_opt(0, 0, 0)\n    .unwrap();\nlet end_date = NaiveDate::from_ymd_opt(2025, 12, 3)\n    .unwrap()\n    .and_hms_opt(0, 0, 0)\n    .unwrap();\nlet out = df\n    .clone()\n    .lazy()\n    .filter(\n        col(\"c\")\n            .gt_eq(lit(start_date))\n            .and(col(\"c\").lt_eq(lit(end_date))),\n    )\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (2, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 c                   \u2506 d   \u2502\n\u2502 --- \u2506 ---      \u2506 ---                 \u2506 --- \u2502\n\u2502 i64 \u2506 f64      \u2506 datetime[\u03bcs]        \u2506 f64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 0.207886 \u2506 2025-12-02 00:00:00 \u2506 2.0 \u2502\n\u2502 2   \u2506 0.758995 \u2506 2025-12-03 00:00:00 \u2506 NaN \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>With <code>filter</code> you can also create more complex filters that include multiple columns.</p>  Python Rust <p> <code>filter</code> <pre><code>df.filter((pl.col(\"a\") &lt;= 3) &amp; (pl.col(\"d\").is_not_nan()))\n</code></pre></p> <p> <code>filter</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .filter(col(\"a\").lt_eq(3).and(col(\"d\").is_not_null()))\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 c                   \u2506 d     \u2502\n\u2502 --- \u2506 ---      \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64 \u2506 f64      \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.851098 \u2506 2025-12-01 00:00:00 \u2506 1.0   \u2502\n\u2502 1   \u2506 0.207886 \u2506 2025-12-02 00:00:00 \u2506 2.0   \u2502\n\u2502 3   \u2506 0.818029 \u2506 2025-12-04 00:00:00 \u2506 -42.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/getting-started/#add-columns","title":"Add columns","text":"<p><code>with_columns</code> allows you to create new columns for your analyses. We create two new columns <code>e</code> and <code>b+42</code>. First we sum all values from column <code>b</code> and store the results in column <code>e</code>. After that we add <code>42</code> to the values of <code>b</code>. Creating a new column <code>b+42</code> to store these results.</p>  Python Rust <p> <code>with_columns</code> <pre><code>df.with_columns(pl.col(\"b\").sum().alias(\"e\"), (pl.col(\"b\") + 42).alias(\"b+42\"))\n</code></pre></p> <p> <code>with_columns</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .with_columns([\n        col(\"b\").sum().alias(\"e\"),\n        (col(\"b\") + lit(42)).alias(\"b+42\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 c        \u2506 d     \u2506 e       \u2506 b+42    \u2502\n\u2502 --- \u2506 ---      \u2506 ---      \u2506 ---   \u2506 ---     \u2506 ---     \u2502\n\u2502 i64 \u2506 f64      \u2506 datetime \u2506 f64   \u2506 f64     \u2506 f64     \u2502\n\u2502     \u2506          \u2506 [\u03bcs]     \u2506       \u2506         \u2506         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.851098 \u2506 2025-12- \u2506 1.0   \u2506 2.90148 \u2506 42.8510 \u2502\n\u2502     \u2506          \u2506 01       \u2506       \u2506 1       \u2506 98      \u2502\n\u2502     \u2506          \u2506 00:00:00 \u2506       \u2506         \u2506         \u2502\n\u2502 1   \u2506 0.207886 \u2506 2025-12- \u2506 2.0   \u2506 2.90148 \u2506 42.2078 \u2502\n\u2502     \u2506          \u2506 02       \u2506       \u2506 1       \u2506 86      \u2502\n\u2502     \u2506          \u2506 00:00:00 \u2506       \u2506         \u2506         \u2502\n\u2502 2   \u2506 0.758995 \u2506 2025-12- \u2506 NaN   \u2506 2.90148 \u2506 42.7589 \u2502\n\u2502     \u2506          \u2506 03       \u2506       \u2506 1       \u2506 95      \u2502\n\u2502     \u2506          \u2506 00:00:00 \u2506       \u2506         \u2506         \u2502\n\u2502 3   \u2506 0.818029 \u2506 2025-12- \u2506 -42.0 \u2506 2.90148 \u2506 42.8180 \u2502\n\u2502     \u2506          \u2506 04       \u2506       \u2506 1       \u2506 29      \u2502\n\u2502     \u2506          \u2506 00:00:00 \u2506       \u2506         \u2506         \u2502\n\u2502 4   \u2506 0.265473 \u2506 2025-12- \u2506 null  \u2506 2.90148 \u2506 42.2654 \u2502\n\u2502     \u2506          \u2506 05       \u2506       \u2506 1       \u2506 73      \u2502\n\u2502     \u2506          \u2506 00:00:00 \u2506       \u2506         \u2506         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/getting-started/#group-by","title":"Group by","text":"<p>We will create a new <code>DataFrame</code> for the Group by functionality. This new <code>DataFrame</code> will include several 'groups' that we want to group by.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df2 = pl.DataFrame(\n    {\n        \"x\": range(8),\n        \"y\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"X\", \"X\"],\n    }\n)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df2: DataFrame = df!(\"x\" =&gt; 0..8,\n    \"y\"=&gt; &amp;[\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"X\", \"X\"],\n)\n.expect(\"should not fail\");\nprintln!(\"{}\", df2);\n</code></pre></p> <pre><code>shape: (8, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x   \u2506 y   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 A   \u2502\n\u2502 1   \u2506 A   \u2502\n\u2502 2   \u2506 A   \u2502\n\u2502 3   \u2506 B   \u2502\n\u2502 4   \u2506 B   \u2502\n\u2502 5   \u2506 C   \u2502\n\u2502 6   \u2506 X   \u2502\n\u2502 7   \u2506 X   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust <p> <code>group_by</code> <pre><code>df2.group_by(\"y\", maintain_order=True).len()\n</code></pre></p> <p> <code>group_by</code> <pre><code>let out = df2.clone().lazy().group_by([\"y\"]).agg([len()]).collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 y   \u2506 len \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 str \u2506 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 3   \u2502\n\u2502 B   \u2506 2   \u2502\n\u2502 C   \u2506 1   \u2502\n\u2502 X   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust <p> <code>group_by</code> <pre><code>df2.group_by(\"y\", maintain_order=True).agg(\n    pl.col(\"*\").count().alias(\"count\"),\n    pl.col(\"*\").sum().alias(\"sum\"),\n)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let out = df2\n    .clone()\n    .lazy()\n    .group_by([\"y\"])\n    .agg([col(\"*\").count().alias(\"count\"), col(\"*\").sum().alias(\"sum\")])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 y   \u2506 count \u2506 sum \u2502\n\u2502 --- \u2506 ---   \u2506 --- \u2502\n\u2502 str \u2506 u32   \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 3     \u2506 3   \u2502\n\u2502 B   \u2506 2     \u2506 7   \u2502\n\u2502 C   \u2506 1     \u2506 5   \u2502\n\u2502 X   \u2506 2     \u2506 13  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/getting-started/#combination","title":"Combination","text":"<p>Below are some examples on how to combine operations to create the <code>DataFrame</code> you require.</p>  Python Rust <p> <code>select</code> \u00b7 <code>with_columns</code> <pre><code>df_x = df.with_columns((pl.col(\"a\") * pl.col(\"b\")).alias(\"a * b\")).select(\n    pl.all().exclude([\"c\", \"d\"])\n)\n\nprint(df_x)\n</code></pre></p> <p> <code>select</code> \u00b7 <code>with_columns</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .with_columns([(col(\"a\") * col(\"b\")).alias(\"a * b\")])\n    .select([col(\"*\").exclude([\"c\", \"d\"])])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 a * b    \u2502\n\u2502 --- \u2506 ---      \u2506 ---      \u2502\n\u2502 i64 \u2506 f64      \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.851098 \u2506 0.0      \u2502\n\u2502 1   \u2506 0.207886 \u2506 0.207886 \u2502\n\u2502 2   \u2506 0.758995 \u2506 1.517989 \u2502\n\u2502 3   \u2506 0.818029 \u2506 2.454087 \u2502\n\u2502 4   \u2506 0.265473 \u2506 1.061892 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust <p> <code>select</code> \u00b7 <code>with_columns</code> <pre><code>df_y = df.with_columns((pl.col(\"a\") * pl.col(\"b\")).alias(\"a * b\")).select(\n    pl.all().exclude(\"d\")\n)\n\nprint(df_y)\n</code></pre></p> <p> <code>select</code> \u00b7 <code>with_columns</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .with_columns([(col(\"a\") * col(\"b\")).alias(\"a * b\")])\n    .select([col(\"*\").exclude([\"d\"])])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 c                   \u2506 a * b    \u2502\n\u2502 --- \u2506 ---      \u2506 ---                 \u2506 ---      \u2502\n\u2502 i64 \u2506 f64      \u2506 datetime[\u03bcs]        \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.851098 \u2506 2025-12-01 00:00:00 \u2506 0.0      \u2502\n\u2502 1   \u2506 0.207886 \u2506 2025-12-02 00:00:00 \u2506 0.207886 \u2502\n\u2502 2   \u2506 0.758995 \u2506 2025-12-03 00:00:00 \u2506 1.517989 \u2502\n\u2502 3   \u2506 0.818029 \u2506 2025-12-04 00:00:00 \u2506 2.454087 \u2502\n\u2502 4   \u2506 0.265473 \u2506 2025-12-05 00:00:00 \u2506 1.061892 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/getting-started/#combining-dataframes","title":"Combining DataFrames","text":"<p>There are two ways <code>DataFrame</code>s can be combined depending on the use case: join and concat.</p>"},{"location":"user-guide/getting-started/#join","title":"Join","text":"<p>Polars supports all types of join (e.g. left, right, inner, outer). Let's have a closer look on how to <code>join</code> two <code>DataFrames</code> into a single <code>DataFrame</code>. Our two <code>DataFrames</code> both have an 'id'-like column: <code>a</code> and <code>x</code>. We can use those columns to <code>join</code> the <code>DataFrames</code> in this example.</p>  Python Rust <p> <code>join</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": range(8),\n        \"b\": np.random.rand(8),\n        \"d\": [1, 2.0, float(\"nan\"), float(\"nan\"), 0, -5, -42, None],\n    }\n)\n\ndf2 = pl.DataFrame(\n    {\n        \"x\": range(8),\n        \"y\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"X\", \"X\"],\n    }\n)\njoined = df.join(df2, left_on=\"a\", right_on=\"x\")\nprint(joined)\n</code></pre></p> <p> <code>join</code> <pre><code>use rand::Rng;\nlet mut rng = rand::thread_rng();\n\nlet df: DataFrame = df!(\n    \"a\" =&gt; 0..8,\n    \"b\"=&gt; (0..8).map(|_| rng.gen::&lt;f64&gt;()).collect::&lt;Vec&lt;f64&gt;&gt;(),\n    \"d\"=&gt; [Some(1.0), Some(2.0), None, None, Some(0.0), Some(-5.0), Some(-42.), None]\n)\n.unwrap();\nlet df2: DataFrame = df!(\n    \"x\" =&gt; 0..8,\n    \"y\"=&gt; &amp;[\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"X\", \"X\"],\n)\n.unwrap();\nlet joined = df.join(&amp;df2, [\"a\"], [\"x\"], JoinType::Left.into())?;\nprintln!(\"{}\", joined);\n</code></pre></p> <pre><code>shape: (8, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 d     \u2506 y   \u2502\n\u2502 --- \u2506 ---      \u2506 ---   \u2506 --- \u2502\n\u2502 i64 \u2506 f64      \u2506 f64   \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.440029 \u2506 1.0   \u2506 A   \u2502\n\u2502 1   \u2506 0.500986 \u2506 2.0   \u2506 A   \u2502\n\u2502 2   \u2506 0.147525 \u2506 NaN   \u2506 A   \u2502\n\u2502 3   \u2506 0.103038 \u2506 NaN   \u2506 B   \u2502\n\u2502 4   \u2506 0.978142 \u2506 0.0   \u2506 B   \u2502\n\u2502 5   \u2506 0.650417 \u2506 -5.0  \u2506 C   \u2502\n\u2502 6   \u2506 0.271039 \u2506 -42.0 \u2506 X   \u2502\n\u2502 7   \u2506 0.617083 \u2506 null  \u2506 X   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To see more examples with other types of joins, see the Transformations section in the user guide.</p>"},{"location":"user-guide/getting-started/#concat","title":"Concat","text":"<p>We can also <code>concatenate</code> two <code>DataFrames</code>. Vertical concatenation will make the <code>DataFrame</code> longer. Horizontal concatenation will make the <code>DataFrame</code> wider. Below you can see the result of an horizontal concatenation of our two <code>DataFrames</code>.</p>  Python Rust <p> <code>hstack</code> <pre><code>stacked = df.hstack(df2)\nprint(stacked)\n</code></pre></p> <p> <code>hstack</code> <pre><code>let stacked = df.hstack(df2.get_columns())?;\nprintln!(\"{}\", stacked);\n</code></pre></p> <pre><code>shape: (8, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 d     \u2506 x   \u2506 y   \u2502\n\u2502 --- \u2506 ---      \u2506 ---   \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 f64      \u2506 f64   \u2506 i64 \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.440029 \u2506 1.0   \u2506 0   \u2506 A   \u2502\n\u2502 1   \u2506 0.500986 \u2506 2.0   \u2506 1   \u2506 A   \u2502\n\u2502 2   \u2506 0.147525 \u2506 NaN   \u2506 2   \u2506 A   \u2502\n\u2502 3   \u2506 0.103038 \u2506 NaN   \u2506 3   \u2506 B   \u2502\n\u2502 4   \u2506 0.978142 \u2506 0.0   \u2506 4   \u2506 B   \u2502\n\u2502 5   \u2506 0.650417 \u2506 -5.0  \u2506 5   \u2506 C   \u2502\n\u2502 6   \u2506 0.271039 \u2506 -42.0 \u2506 6   \u2506 X   \u2502\n\u2502 7   \u2506 0.617083 \u2506 null  \u2506 7   \u2506 X   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>Polars is a library and installation is as simple as invoking the package manager of the corresponding programming language.</p>  Python Rust <pre><code>pip install polars\n\n# Or for legacy CPUs without AVX2 support\npip install polars-lts-cpu\n</code></pre> <pre><code>cargo add polars -F lazy\n\n# Or Cargo.toml\n[dependencies]\npolars = { version = \"x\", features = [\"lazy\", ...]}\n</code></pre>"},{"location":"user-guide/installation/#big-index","title":"Big Index","text":"<p>By default, polars is limited to 2^32 (~4.2 billion rows). To increase this limit 2^64 (~18 quintillion) by enabling big index:</p>  Python Rust <pre><code>pip install polars-u64-idx\n</code></pre> <pre><code>cargo add polars -F bigidx\n\n# Or Cargo.toml\n[dependencies]\npolars = { version = \"x\", features = [\"bigidx\", ...] }\n</code></pre>"},{"location":"user-guide/installation/#legacy-cpu","title":"Legacy CPU","text":"<p>To install polars on an old CPU without AVX support:</p>  Python <pre><code>pip install polars-lts-cpu\n</code></pre>"},{"location":"user-guide/installation/#importing","title":"Importing","text":"<p>To use the library import it into your project</p>  Python Rust <pre><code>import polars as pl\n</code></pre> <pre><code>use polars::prelude::*;\n</code></pre>"},{"location":"user-guide/installation/#feature-flags","title":"Feature Flags","text":"<p>By using the above command you install the core of Polars onto your system. However depending on your use case you might want to install the optional dependencies as well. These are made optional to minimize the footprint. The flags are different depending on the programming language. Throughout the user guide we will mention when a functionality is used that requires an additional dependency.</p>"},{"location":"user-guide/installation/#python","title":"Python","text":"<pre><code># For example\npip install 'polars[numpy,fsspec]'\n</code></pre> Tag Description all Install all optional dependencies (all of the following) pandas Install with Pandas for converting data to and from Pandas Dataframes/Series numpy Install with numpy for converting data to and from numpy arrays pyarrow Reading data formats using PyArrow fsspec Support for reading from remote file systems connectorx Support for reading from SQL databases xlsx2csv Support for reading from Excel files deltalake Support for reading from Delta Lake Tables plot Support for plotting Dataframes timezone Timezone support, only needed if 1. you are on Python &lt; 3.9 and/or 2. you are on Windows, otherwise no dependencies will be installed"},{"location":"user-guide/installation/#rust","title":"Rust","text":"<pre><code># Cargo.toml\n[dependencies]\npolars = { version = \"0.26.1\", features = [\"lazy\", \"temporal\", \"describe\", \"json\", \"parquet\", \"dtype-datetime\"] }\n</code></pre> <p>The opt-in features are:</p> <ul> <li>Additional data types:<ul> <li><code>dtype-date</code></li> <li><code>dtype-datetime</code></li> <li><code>dtype-time</code></li> <li><code>dtype-duration</code></li> <li><code>dtype-i8</code></li> <li><code>dtype-i16</code></li> <li><code>dtype-u8</code></li> <li><code>dtype-u16</code></li> <li><code>dtype-categorical</code></li> <li><code>dtype-struct</code></li> </ul> </li> <li><code>lazy</code> - Lazy API<ul> <li><code>regex</code> - Use regexes in column selection</li> <li><code>dot_diagram</code> - Create dot diagrams from lazy logical plans.</li> </ul> </li> <li><code>sql</code> - Pass SQL queries to polars.</li> <li><code>streaming</code> - Be able to process datasets that are larger than RAM.</li> <li><code>random</code> - Generate arrays with randomly sampled values</li> <li><code>ndarray</code>- Convert from <code>DataFrame</code> to <code>ndarray</code></li> <li><code>temporal</code> - Conversions between Chrono and Polars for temporal data types</li> <li><code>timezones</code> - Activate timezone support.</li> <li><code>strings</code> - Extra string utilities for <code>StringChunked</code><ul> <li><code>string_pad</code> - <code>pad_start</code>, <code>pad_end</code>, <code>zfill</code></li> <li><code>string_to_integer</code> - <code>parse_int</code></li> </ul> </li> <li><code>object</code> - Support for generic ChunkedArrays called <code>ObjectChunked&lt;T&gt;</code> (generic over <code>T</code>).   These are downcastable from Series through the Any trait.</li> <li>Performance related:<ul> <li><code>nightly</code> - Several nightly only features such as SIMD and specialization.</li> <li><code>performant</code> - more fast paths, slower compile times.</li> <li><code>bigidx</code> - Activate this feature if you expect &gt;&gt; 2^32 rows. This has not been needed by anyone. This allows polars to scale up way beyond that by using <code>u64</code> as an index. Polars will be a bit slower with this feature activated as many data structures are less cache efficient.</li> <li><code>cse</code> - Activate common subplan elimination optimization</li> </ul> </li> <li> <p>IO related:</p> <ul> <li><code>serde</code> - Support for serde serialization and deserialization. Can be used for JSON and more serde supported serialization formats.</li> <li><code>serde-lazy</code> - Support for serde serialization and deserialization. Can be used for JSON and more serde supported serialization formats.</li> <li><code>parquet</code> - Read Apache Parquet format</li> <li><code>json</code> - JSON serialization</li> <li><code>ipc</code> - Arrow's IPC format serialization</li> <li><code>decompress</code> - Automatically infer compression of csvs and decompress them. Supported compressions:</li> <li>zip</li> <li>gzip</li> </ul> </li> <li> <p><code>DataFrame</code> operations:</p> <ul> <li><code>dynamic_group_by</code> - Group by based on a time window instead of predefined keys. Also activates rolling window group by operations.</li> <li><code>sort_multiple</code> - Allow sorting a <code>DataFrame</code> on multiple columns</li> <li><code>rows</code> - Create <code>DataFrame</code> from rows and extract rows from <code>DataFrames</code>. And activates <code>pivot</code> and <code>transpose</code> operations</li> <li><code>join_asof</code> - Join ASOF, to join on nearest keys instead of exact equality match.</li> <li><code>cross_join</code> - Create the Cartesian product of two DataFrames.</li> <li><code>semi_anti_join</code> - SEMI and ANTI joins.</li> <li><code>row_hash</code> - Utility to hash DataFrame rows to UInt64Chunked</li> <li><code>diagonal_concat</code> - Concat diagonally thereby combining different schemas.</li> <li><code>dataframe_arithmetic</code> - Arithmetic on (Dataframe and DataFrames) and (DataFrame on Series)</li> <li><code>partition_by</code> - Split into multiple DataFrames partitioned by groups.</li> </ul> </li> <li><code>Series</code>/<code>Expression</code> operations:<ul> <li><code>is_in</code> - Check for membership in <code>Series</code></li> <li><code>zip_with</code> - Zip two Series/ ChunkedArrays</li> <li><code>round_series</code> - round underlying float types of <code>Series</code>.</li> <li><code>repeat_by</code> - [Repeat element in an Array N times, where N is given by another array.</li> <li><code>is_first_distinct</code> - Check if element is first unique value.</li> <li><code>is_last_distinct</code> - Check if element is last unique value.</li> <li><code>checked_arithmetic</code> - checked arithmetic/ returning <code>None</code> on invalid operations.</li> <li><code>dot_product</code> - Dot/inner product on Series and Expressions.</li> <li><code>concat_str</code> - Concat string data in linear time.</li> <li><code>reinterpret</code> - Utility to reinterpret bits to signed/unsigned</li> <li><code>take_opt_iter</code> - Take from a Series with <code>Iterator&lt;Item=Option&lt;usize&gt;&gt;</code></li> <li><code>mode</code> - Return the most occurring value(s)</li> <li><code>cum_agg</code> - cum_sum, cum_min, cum_max aggregation.</li> <li><code>rolling_window</code> - rolling window functions, like rolling_mean</li> <li><code>interpolate</code> interpolate None values</li> <li><code>extract_jsonpath</code> - Run jsonpath queries on StringChunked</li> <li><code>list</code> - List utils.</li> <li><code>list_gather</code> take sublist by multiple indices</li> <li><code>rank</code> - Ranking algorithms.</li> <li><code>moment</code> - kurtosis and skew statistics</li> <li><code>ewma</code> - Exponential moving average windows</li> <li><code>abs</code> - Get absolute values of Series</li> <li><code>arange</code> - Range operation on Series</li> <li><code>product</code> - Compute the product of a Series.</li> <li><code>diff</code> - <code>diff</code> operation.</li> <li><code>pct_change</code> - Compute change percentages.</li> <li><code>unique_counts</code> - Count unique values in expressions.</li> <li><code>log</code> - Logarithms for <code>Series</code>.</li> <li><code>list_to_struct</code> - Convert <code>List</code> to <code>Struct</code> dtypes.</li> <li><code>list_count</code> - Count elements in lists.</li> <li><code>list_eval</code> - Apply expressions over list elements.</li> <li><code>cumulative_eval</code> - Apply expressions over cumulatively increasing windows.</li> <li><code>arg_where</code> - Get indices where condition holds.</li> <li><code>search_sorted</code> - Find indices where elements should be inserted to maintain order.</li> <li><code>date_offset</code> Add an offset to dates that take months and leap years into account.</li> <li><code>trigonometry</code> Trigonometric functions.</li> <li><code>sign</code> Compute the element-wise sign of a Series.</li> <li><code>propagate_nans</code> NaN propagating min/max aggregations.</li> </ul> </li> <li><code>DataFrame</code> pretty printing<ul> <li><code>fmt</code> - Activate DataFrame formatting</li> </ul> </li> </ul>"},{"location":"user-guide/concepts/","title":"Concepts","text":"<p>The <code>Concepts</code> chapter describes the core concepts of the Polars API. Understanding these will help you optimise your queries on a daily basis. We will cover the following topics:</p> <ul> <li>Data Types: Overview</li> <li>Data Types: Categoricals</li> <li>Data structures</li> <li>Contexts</li> <li>Expressions</li> <li>Lazy vs eager</li> <li>Streaming</li> </ul>"},{"location":"user-guide/concepts/contexts/","title":"Contexts","text":"<p>Polars has developed its own Domain Specific Language (DSL) for transforming data. The language is very easy to use and allows for complex queries that remain human readable. The two core components of the language are Contexts and Expressions, the latter we will cover in the next section.</p> <p>A context, as implied by the name, refers to the context in which an expression needs to be evaluated. There are three main contexts <sup>1</sup>:</p> <ol> <li>Selection: <code>df.select(...)</code>, <code>df.with_columns(...)</code></li> <li>Filtering: <code>df.filter()</code></li> <li>Group by / Aggregation: <code>df.group_by(...).agg(...)</code></li> </ol> <p>The examples below are performed on the following <code>DataFrame</code>:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use rand::{thread_rng, Rng};\n\nlet mut arr = [0f64; 5];\nthread_rng().fill(&amp;mut arr);\n\nlet df = df! (\n    \"nrs\" =&gt; &amp;[Some(1), Some(2), Some(3), None, Some(5)],\n    \"names\" =&gt; &amp;[Some(\"foo\"), Some(\"ham\"), Some(\"spam\"), Some(\"eggs\"), None],\n    \"random\" =&gt; &amp;arr,\n    \"groups\" =&gt; &amp;[\"A\", \"A\", \"B\", \"C\", \"B\"],\n)?;\n\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2506 A      \u2502\n\u2502 2    \u2506 ham   \u2506 0.74005  \u2506 A      \u2502\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2506 B      \u2502\n\u2502 null \u2506 egg   \u2506 0.533739 \u2506 C      \u2502\n\u2502 5    \u2506 null  \u2506 0.014575 \u2506 B      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/contexts/#selection","title":"Selection","text":"<p>The selection context applies expressions over columns. A <code>select</code> may produce new columns that are aggregations, combinations of expressions, or literals.</p> <p>The expressions in a selection context must produce <code>Series</code> that are all the same length or have a length of 1. Literals are treated as length-1 <code>Series</code>.</p> <p>When some expressions produce length-1 <code>Series</code> and some do not, the length-1 <code>Series</code> will be broadcast to match the length of the remaining <code>Series</code>. Note that broadcasting can also occur within expressions: for instance, in <code>pl.col.value() / pl.col.value.sum()</code>, each element of the <code>value</code> column is divided by the column's sum.</p>  Python Rust <p> <code>select</code> <pre><code>out = df.select(\n    pl.sum(\"nrs\"),\n    pl.col(\"names\").sort(),\n    pl.col(\"names\").first().alias(\"first name\"),\n    (pl.mean(\"nrs\") * 10).alias(\"10xnrs\"),\n)\nprint(out)\n</code></pre></p> <p> <code>select</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([\n        sum(\"nrs\"),\n        col(\"names\").sort(Default::default()),\n        col(\"names\").first().alias(\"first name\"),\n        (mean(\"nrs\") * lit(10)).alias(\"10xnrs\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs \u2506 names \u2506 first name \u2506 10xnrs \u2502\n\u2502 --- \u2506 ---   \u2506 ---        \u2506 ---    \u2502\n\u2502 i64 \u2506 str   \u2506 str        \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11  \u2506 null  \u2506 foo        \u2506 27.5   \u2502\n\u2502 11  \u2506 egg   \u2506 foo        \u2506 27.5   \u2502\n\u2502 11  \u2506 foo   \u2506 foo        \u2506 27.5   \u2502\n\u2502 11  \u2506 ham   \u2506 foo        \u2506 27.5   \u2502\n\u2502 11  \u2506 spam  \u2506 foo        \u2506 27.5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>As you can see from the query, the selection context is very powerful and allows you to evaluate arbitrary expressions independent of (and in parallel to) each other.</p> <p>Similar to the <code>select</code> statement, the <code>with_columns</code> statement also enters into the selection context. The main difference between <code>with_columns</code> and <code>select</code> is that <code>with_columns</code> retains the original columns and adds new ones, whereas <code>select</code> drops the original columns.</p>  Python Rust <p> <code>with_columns</code> <pre><code>df = df.with_columns(\n    pl.sum(\"nrs\").alias(\"nrs_sum\"),\n    pl.col(\"random\").count().alias(\"count\"),\n)\nprint(df)\n</code></pre></p> <p> <code>with_columns</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .with_columns([\n        sum(\"nrs\").alias(\"nrs_sum\"),\n        col(\"random\").count().alias(\"count\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2506 nrs_sum \u2506 count \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2506 ---     \u2506 ---   \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2506 i64     \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2506 A      \u2506 11      \u2506 5     \u2502\n\u2502 2    \u2506 ham   \u2506 0.74005  \u2506 A      \u2506 11      \u2506 5     \u2502\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2506 B      \u2506 11      \u2506 5     \u2502\n\u2502 null \u2506 egg   \u2506 0.533739 \u2506 C      \u2506 11      \u2506 5     \u2502\n\u2502 5    \u2506 null  \u2506 0.014575 \u2506 B      \u2506 11      \u2506 5     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/contexts/#filtering","title":"Filtering","text":"<p>The filtering context filters a <code>DataFrame</code> based on one or more expressions that evaluate to the <code>Boolean</code> data type.</p>  Python Rust <p> <code>filter</code> <pre><code>out = df.filter(pl.col(\"nrs\") &gt; 2)\nprint(out)\n</code></pre></p> <p> <code>filter</code> <pre><code>let out = df.clone().lazy().filter(col(\"nrs\").gt(lit(2))).collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (2, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs \u2506 names \u2506 random   \u2506 groups \u2506 nrs_sum \u2506 count \u2502\n\u2502 --- \u2506 ---   \u2506 ---      \u2506 ---    \u2506 ---     \u2506 ---   \u2502\n\u2502 i64 \u2506 str   \u2506 f64      \u2506 str    \u2506 i64     \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2506 spam  \u2506 0.263315 \u2506 B      \u2506 11      \u2506 5     \u2502\n\u2502 5   \u2506 null  \u2506 0.014575 \u2506 B      \u2506 11      \u2506 5     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/contexts/#group-by-aggregation","title":"Group by / aggregation","text":"<p>In the <code>group_by</code> context, expressions work on groups and thus may yield results of any length (a group may have many members).</p>  Python Rust <p> <code>group_by</code> <pre><code>out = df.group_by(\"groups\").agg(\n    pl.sum(\"nrs\"),  # sum nrs by groups\n    pl.col(\"random\").count().alias(\"count\"),  # count group members\n    # sum random where name != null\n    pl.col(\"random\").filter(pl.col(\"names\").is_not_null()).sum().name.suffix(\"_sum\"),\n    pl.col(\"names\").reverse().alias(\"reversed names\"),\n)\nprint(out)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let out = df\n    .lazy()\n    .group_by([col(\"groups\")])\n    .agg([\n        sum(\"nrs\"),                           // sum nrs by groups\n        col(\"random\").count().alias(\"count\"), // count group members\n        // sum random where name != null\n        col(\"random\")\n            .filter(col(\"names\").is_not_null())\n            .sum()\n            .name()\n            .suffix(\"_sum\"),\n        col(\"names\").reverse().alias(\"reversed names\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (3, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 groups \u2506 nrs \u2506 count \u2506 random_sum \u2506 reversed names \u2502\n\u2502 ---    \u2506 --- \u2506 ---   \u2506 ---        \u2506 ---            \u2502\n\u2502 str    \u2506 i64 \u2506 u32   \u2506 f64        \u2506 list[str]      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 B      \u2506 8   \u2506 2     \u2506 0.263315   \u2506 [null, \"spam\"] \u2502\n\u2502 C      \u2506 0   \u2506 1     \u2506 0.533739   \u2506 [\"egg\"]        \u2502\n\u2502 A      \u2506 3   \u2506 2     \u2506 0.894213   \u2506 [\"ham\", \"foo\"] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>As you can see from the result all expressions are applied to the group defined by the <code>group_by</code> context. Besides the standard <code>group_by</code>, <code>group_by_dynamic</code>, and <code>group_by_rolling</code> are also entrances to the group by context.</p> <ol> <li> <p>There are additional List and SQL contexts which are covered later in this guide. But for simplicity, we leave them out of scope for now.\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/concepts/data-structures/","title":"Data structures","text":"<p>The core base data structures provided by Polars are <code>Series</code> and <code>DataFrame</code>.</p>"},{"location":"user-guide/concepts/data-structures/#series","title":"Series","text":"<p>Series are a 1-dimensional data structure. Within a series all elements have the same Data Type . The snippet below shows how to create a simple named <code>Series</code> object.</p>  Python Rust <p> <code>Series</code> <pre><code>import polars as pl\n\ns = pl.Series(\"a\", [1, 2, 3, 4, 5])\nprint(s)\n</code></pre></p> <p> <code>Series</code> <pre><code>use polars::prelude::*;\n\nlet s = Series::new(\"a\", &amp;[1, 2, 3, 4, 5]);\n\nprintln!(\"{}\", s);\n</code></pre></p> <pre><code>shape: (5,)\nSeries: 'a' [i64]\n[\n    1\n    2\n    3\n    4\n    5\n]\n</code></pre>"},{"location":"user-guide/concepts/data-structures/#dataframe","title":"DataFrame","text":"<p>A <code>DataFrame</code> is a 2-dimensional data structure that is backed by a <code>Series</code>, and it can be seen as an abstraction of a collection (e.g. list) of <code>Series</code>. Operations that can be executed on a <code>DataFrame</code> are very similar to what is done in a <code>SQL</code> like query. You can <code>GROUP BY</code>, <code>JOIN</code>, <code>PIVOT</code>, but also define custom functions.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>from datetime import datetime\n\ndf = pl.DataFrame(\n    {\n        \"integer\": [1, 2, 3, 4, 5],\n        \"date\": [\n            datetime(2022, 1, 1),\n            datetime(2022, 1, 2),\n            datetime(2022, 1, 3),\n            datetime(2022, 1, 4),\n            datetime(2022, 1, 5),\n        ],\n        \"float\": [4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use chrono::NaiveDate;\n\nlet df: DataFrame = df!(\n    \"integer\" =&gt; &amp;[1, 2, 3, 4, 5],\n    \"date\" =&gt; &amp;[\n        NaiveDate::from_ymd_opt(2025, 1, 1).unwrap().and_hms_opt(0, 0, 0).unwrap(),\n        NaiveDate::from_ymd_opt(2025, 1, 2).unwrap().and_hms_opt(0, 0, 0).unwrap(),\n        NaiveDate::from_ymd_opt(2025, 1, 3).unwrap().and_hms_opt(0, 0, 0).unwrap(),\n        NaiveDate::from_ymd_opt(2025, 1, 4).unwrap().and_hms_opt(0, 0, 0).unwrap(),\n        NaiveDate::from_ymd_opt(2025, 1, 5).unwrap().and_hms_opt(0, 0, 0).unwrap(),\n    ],\n    \"float\" =&gt; &amp;[4.0, 5.0, 6.0, 7.0, 8.0]\n)\n.unwrap();\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02 00:00:00 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2502 4       \u2506 2022-01-04 00:00:00 \u2506 7.0   \u2502\n\u2502 5       \u2506 2022-01-05 00:00:00 \u2506 8.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-structures/#viewing-data","title":"Viewing data","text":"<p>This part focuses on viewing data in a <code>DataFrame</code>. We will use the <code>DataFrame</code> from the previous example as a starting point.</p>"},{"location":"user-guide/concepts/data-structures/#head","title":"Head","text":"<p>The <code>head</code> function shows by default the first 5 rows of a <code>DataFrame</code>. You can specify the number of rows you want to see (e.g. <code>df.head(10)</code>).</p>  Python Rust <p> <code>head</code> <pre><code>print(df.head(3))\n</code></pre></p> <p> <code>head</code> <pre><code>let df_head = df.head(Some(3));\n\nprintln!(\"{}\", df_head);\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02 00:00:00 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-structures/#tail","title":"Tail","text":"<p>The <code>tail</code> function shows the last 5 rows of a <code>DataFrame</code>. You can also specify the number of rows you want to see, similar to <code>head</code>.</p>  Python Rust <p> <code>tail</code> <pre><code>print(df.tail(3))\n</code></pre></p> <p> <code>tail</code> <pre><code>let df_tail = df.tail(Some(3));\n\nprintln!(\"{}\", df_tail);\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2502 4       \u2506 2022-01-04 00:00:00 \u2506 7.0   \u2502\n\u2502 5       \u2506 2022-01-05 00:00:00 \u2506 8.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-structures/#sample","title":"Sample","text":"<p>If you want to get an impression of the data of your <code>DataFrame</code>, you can also use <code>sample</code>. With <code>sample</code> you get an n number of random rows from the <code>DataFrame</code>.</p>  Python Rust <p> <code>sample</code> <pre><code>print(df.sample(2))\n</code></pre></p> <p> <code>sample_n</code> <pre><code>let n = Series::new(\"\", &amp;[2]);\nlet sampled_df = df.sample_n(&amp;n, false, false, None).unwrap();\n\nprintln!(\"{}\", sampled_df);\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2502 5       \u2506 2022-01-05 00:00:00 \u2506 8.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-structures/#describe","title":"Describe","text":"<p><code>Describe</code> returns summary statistics of your <code>DataFrame</code>. It will provide several quick statistics if possible.</p>  Python Rust <p> <code>describe</code> <pre><code>print(df.describe())\n</code></pre></p> <p> <code>describe</code> \u00b7  Available on feature describe <pre><code>// Not available in Rust\n</code></pre></p> <pre><code>shape: (9, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 statistic  \u2506 integer  \u2506 date       \u2506 float    \u2502\n\u2502 ---        \u2506 ---      \u2506 ---        \u2506 ---      \u2502\n\u2502 str        \u2506 f64      \u2506 str        \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 count      \u2506 5.0      \u2506 5          \u2506 5.0      \u2502\n\u2502 null_count \u2506 0.0      \u2506 0          \u2506 0.0      \u2502\n\u2502 mean       \u2506 3.0      \u2506 2022-01-03 \u2506 6.0      \u2502\n\u2502            \u2506          \u2506 00:00:00   \u2506          \u2502\n\u2502 std        \u2506 1.581139 \u2506 null       \u2506 1.581139 \u2502\n\u2502 min        \u2506 1.0      \u2506 2022-01-01 \u2506 4.0      \u2502\n\u2502            \u2506          \u2506 00:00:00   \u2506          \u2502\n\u2502 25%        \u2506 2.0      \u2506 2022-01-02 \u2506 5.0      \u2502\n\u2502            \u2506          \u2506 00:00:00   \u2506          \u2502\n\u2502 50%        \u2506 3.0      \u2506 2022-01-03 \u2506 6.0      \u2502\n\u2502            \u2506          \u2506 00:00:00   \u2506          \u2502\n\u2502 75%        \u2506 4.0      \u2506 2022-01-04 \u2506 7.0      \u2502\n\u2502            \u2506          \u2506 00:00:00   \u2506          \u2502\n\u2502 max        \u2506 5.0      \u2506 2022-01-05 \u2506 8.0      \u2502\n\u2502            \u2506          \u2506 00:00:00   \u2506          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/expressions/","title":"Expressions","text":"<p>Polars has a powerful concept called expressions that is central to its very fast performance.</p> <p>Expressions are at the core of many data science operations:</p> <ul> <li>taking a sample of rows from a column</li> <li>multiplying values in a column</li> <li>extracting a column of years from dates</li> <li>convert a column of strings to lowercase</li> <li>and so on!</li> </ul> <p>However, expressions are also used within other operations:</p> <ul> <li>taking the mean of a group in a <code>group_by</code> operation</li> <li>calculating the size of groups in a <code>group_by</code> operation</li> <li>taking the sum horizontally across columns</li> </ul> <p>Polars performs these core data transformations very quickly by:</p> <ul> <li>automatic query optimization on each expression</li> <li>automatic parallelization of expressions on many columns</li> </ul> <p>An expression is a tree of operations that describe how to construct one or more Series. As the outputs are Series, it is straightforward to apply a sequence of expressions (similar to method chaining in pandas) each of which transforms the output from the previous step.</p> <p>If this seems abstract and confusing - don't worry! People quickly develop an intuition for expressions just by looking at a few examples. We'll do that next!</p>"},{"location":"user-guide/concepts/expressions/#examples","title":"Examples","text":"<p>The following is an expression:</p>  Python Rust <p> <code>col</code> \u00b7 <code>sort</code> \u00b7 <code>head</code> <pre><code>pl.col(\"foo\").sort().head(2)\n</code></pre></p> <p> <code>col</code> \u00b7 <code>sort</code> \u00b7 <code>head</code> <pre><code>let _ = col(\"foo\").sort(Default::default()).head(Some(2));\n</code></pre></p> <p>The snippet above says:</p> <ol> <li>Select column \"foo\"</li> <li>Then sort the column (not in reversed order)</li> <li>Then take the first two values of the sorted output</li> </ol> <p>The power of expressions is that every expression produces a new expression, and that they can be piped together. You can run an expression by passing them to one of Polars execution contexts.</p> <p>Here we run two expressions by running <code>df.select</code>:</p>  Python Rust <p> <code>select</code> <pre><code>df.select(pl.col(\"foo\").sort().head(2), pl.col(\"bar\").filter(pl.col(\"foo\") == 1).sum())\n</code></pre></p> <p> <code>select</code> <pre><code>df.clone()\n    .lazy()\n    .select([\n        col(\"foo\").sort(Default::default()).head(Some(2)),\n        col(\"bar\").filter(col(\"foo\").eq(lit(1))).sum(),\n    ])\n    .collect()?;\n</code></pre></p> <p>All expressions are run in parallel, meaning that separate Polars expressions are embarrassingly parallel. Note that within an expression there may be more parallelization going on.</p>"},{"location":"user-guide/concepts/expressions/#conclusion","title":"Conclusion","text":"<p>This is the tip of the iceberg in terms of possible expressions. There are a ton more, and they can be combined in a variety of ways. This page is intended to get you familiar with the concept of expressions, in the section on expressions we will dive deeper.</p>"},{"location":"user-guide/concepts/lazy-vs-eager/","title":"Lazy / eager API","text":"<p>Polars supports two modes of operation: lazy and eager. In the eager API the query is executed immediately while in the lazy API the query is only evaluated once it is 'needed'. Deferring the execution to the last minute can have significant performance advantages and is why the Lazy API is preferred in most cases. Let us demonstrate this with an example:</p>  Python Rust <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"docs/data/iris.csv\")\ndf_small = df.filter(pl.col(\"sepal_length\") &gt; 5)\ndf_agg = df_small.group_by(\"species\").agg(pl.col(\"sepal_width\").mean())\nprint(df_agg)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>let df = CsvReader::from_path(\"docs/data/iris.csv\")\n    .unwrap()\n    .finish()\n    .unwrap();\nlet mask = df.column(\"sepal_length\")?.f64()?.gt(5.0);\nlet df_small = df.filter(&amp;mask)?;\n#[allow(deprecated)]\nlet df_agg = df_small\n    .group_by([\"species\"])?\n    .select([\"sepal_width\"])\n    .mean()?;\nprintln!(\"{}\", df_agg);\n</code></pre></p> <p>In this example we use the eager API to:</p> <ol> <li>Read the iris dataset.</li> <li>Filter the dataset based on sepal length</li> <li>Calculate the mean of the sepal width per species</li> </ol> <p>Every step is executed immediately returning the intermediate results. This can be very wasteful as we might do work or load extra data that is not being used. If we instead used the lazy API and waited on execution until all the steps are defined then the query planner could perform various optimizations. In this case:</p> <ul> <li>Predicate pushdown: Apply filters as early as possible while reading the dataset, thus only reading rows with sepal length greater than 5.</li> <li>Projection pushdown: Select only the columns that are needed while reading the dataset, thus removing the need to load additional columns (e.g. petal length &amp; petal width)</li> </ul>  Python Rust <p> <code>scan_csv</code> <pre><code>q = (\n    pl.scan_csv(\"docs/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .group_by(\"species\")\n    .agg(pl.col(\"sepal_width\").mean())\n)\n\ndf = q.collect()\n</code></pre></p> <p> <code>LazyCsvReader</code> \u00b7  Available on feature csv <pre><code>let q = LazyCsvReader::new(\"docs/data/iris.csv\")\n    .has_header(true)\n    .finish()?\n    .filter(col(\"sepal_length\").gt(lit(5)))\n    .group_by(vec![col(\"species\")])\n    .agg([col(\"sepal_width\").mean()]);\nlet df = q.collect()?;\nprintln!(\"{}\", df);\n</code></pre></p> <p>These will significantly lower the load on memory &amp; CPU thus allowing you to fit bigger datasets in memory and process faster. Once the query is defined you call <code>collect</code> to inform Polars that you want to execute it. In the section on Lazy API we will go into more details on its implementation.</p> <p>Eager API</p> <p>In many cases the eager API is actually calling the lazy API under the hood and immediately collecting the result. This has the benefit that within the query itself optimization(s) made by the query planner can still take place.</p>"},{"location":"user-guide/concepts/lazy-vs-eager/#when-to-use-which","title":"When to use which","text":"<p>In general the lazy API should be preferred unless you are either interested in the intermediate results or are doing exploratory work and don't know yet what your query is going to look like.</p>"},{"location":"user-guide/concepts/streaming/","title":"Streaming API","text":"<p>One additional benefit of the lazy API is that it allows queries to be executed in a streaming manner. Instead of processing the data all-at-once Polars can execute the query in batches allowing you to process datasets that are larger-than-memory.</p> <p>To tell Polars we want to execute a query in streaming mode we pass the <code>streaming=True</code> argument to <code>collect</code></p>  Python Rust <p> <code>collect</code> <pre><code>q1 = (\n    pl.scan_csv(\"docs/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .group_by(\"species\")\n    .agg(pl.col(\"sepal_width\").mean())\n)\ndf = q1.collect(streaming=True)\n</code></pre></p> <p> <code>collect</code> \u00b7  Available on feature streaming <pre><code>let q1 = LazyCsvReader::new(\"docs/data/iris.csv\")\n    .has_header(true)\n    .finish()?\n    .filter(col(\"sepal_length\").gt(lit(5)))\n    .group_by(vec![col(\"species\")])\n    .agg([col(\"sepal_width\").mean()]);\n\nlet df = q1.clone().with_streaming(true).collect()?;\nprintln!(\"{}\", df);\n</code></pre></p>"},{"location":"user-guide/concepts/streaming/#when-is-streaming-available","title":"When is streaming available?","text":"<p>Streaming is still in development. We can ask Polars to execute any lazy query in streaming mode. However, not all lazy operations support streaming. If there is an operation for which streaming is not supported Polars will run the query in non-streaming mode.</p> <p>Streaming is supported for many operations including:</p> <ul> <li><code>filter</code>,<code>slice</code>,<code>head</code>,<code>tail</code></li> <li><code>with_columns</code>,<code>select</code></li> <li><code>group_by</code></li> <li><code>join</code></li> <li><code>unique</code></li> <li><code>sort</code></li> <li><code>explode</code>,<code>melt</code></li> <li><code>scan_csv</code>,<code>scan_parquet</code>,<code>scan_ipc</code></li> </ul> <p>This list is not exhaustive. Polars is in active development, and more operations can be added without explicit notice.</p>"},{"location":"user-guide/concepts/streaming/#example-with-supported-operations","title":"Example with supported operations","text":"<p>To determine which parts of your query are streaming, use the <code>explain</code> method. Below is an example that demonstrates how to inspect the query plan. More information about the query plan can be found in the chapter on the Lazy API.</p>  Python Rust <p> <code>explain</code> <pre><code>print(q1.explain(streaming=True))\n</code></pre></p> <p> <code>explain</code> <pre><code>let query_plan = q1.with_streaming(true).explain(true)?;\nprintln!(\"{}\", query_plan);\n</code></pre></p> <pre><code>--- STREAMING\nAGGREGATE\n    [col(\"sepal_width\").mean()] BY [col(\"species\")] FROM\n\n    Csv SCAN docs/data/iris.csv\n    PROJECT 3/5 COLUMNS\n    SELECTION: [(col(\"sepal_length\")) &gt; (5.0)]  --- END STREAMING\n\n  DF []; PROJECT */0 COLUMNS; SELECTION: \"None\"\n</code></pre>"},{"location":"user-guide/concepts/streaming/#example-with-non-streaming-operations","title":"Example with non-streaming operations","text":"Python Rust <p> <code>explain</code> <pre><code>q2 = pl.scan_csv(\"docs/data/iris.csv\").with_columns(\n    pl.col(\"sepal_length\").mean().over(\"species\")\n)\n\nprint(q2.explain(streaming=True))\n</code></pre></p> <p> <code>explain</code> <pre><code>let q2 = LazyCsvReader::new(\"docs/data/iris.csv\")\n    .finish()?\n    .with_columns(vec![col(\"sepal_length\")\n        .mean()\n        .over(vec![col(\"species\")])\n        .alias(\"sepal_length_mean\")]);\n\nlet query_plan = q2.with_streaming(true).explain(true)?;\nprintln!(\"{}\", query_plan);\n</code></pre></p> <pre><code> WITH_COLUMNS:\n [col(\"sepal_length\").mean().over([col(\"species\")])]\n  --- STREAMING\n\n  Csv SCAN docs/data/iris.csv\n  PROJECT */5 COLUMNS  --- END STREAMING\n\n    DF []; PROJECT */0 COLUMNS; SELECTION: \"None\"\n</code></pre>"},{"location":"user-guide/concepts/data-types/categoricals/","title":"Categorical data","text":"<p>Categorical data represents string data where the values in the column have a finite set of values (usually way smaller than the length of the column). You can think about columns on gender, countries, currency pairings, etc. Storing these values as plain strings is a waste of memory and performance as we will be repeating the same string over and over again. Additionally, in the case of joins we are stuck with expensive string comparisons.</p> <p>That is why Polars supports encoding string values in dictionary format. Working with categorical data in Polars can be done with two different DataTypes: <code>Enum</code>,<code>Categorical</code>. Both have their own use cases which we will explain further on this page. First we will look at what a categorical is in Polars.</p> <p>In Polars a categorical is defined as a string column which is encoded by a dictionary. A string column would be split into two elements: encodings and the actual string values.</p> String Column Categorical Column Series Polar Bear Panda Bear Brown Bear Panda Bear Brown Bear Brown Bear Polar Bear Physical 0 1 2 1 2 2 0 Categories Polar Bear Panda Bear Brown Bear <p>The physical <code>0</code> in this case encodes (or maps) to the value 'Polar Bear', the value <code>1</code> encodes to 'Panda Bear' and the value <code>2</code> to 'Brown Bear'. This encoding has the benefit of only storing the string values once. Additionally, when we perform operations (e.g. sorting, counting) we can work directly on the physical representation which is much faster than the working with string data.</p>"},{"location":"user-guide/concepts/data-types/categoricals/#enum-vs-categorical","title":"<code>Enum</code> vs <code>Categorical</code>","text":"<p>Polars supports two different DataTypes for working with categorical data: <code>Enum</code> and <code>Categorical</code>. When the categories are known up front use <code>Enum</code>. When you don't know the categories or they are not fixed then you use <code>Categorical</code>. In case your requirements change along the way you can always cast from one to the other.</p>  Python <pre><code>enum_dtype = pl.Enum([\"Polar\", \"Panda\", \"Brown\"])\nenum_series = pl.Series([\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"], dtype=enum_dtype)\ncat_series = pl.Series(\n    [\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"], dtype=pl.Categorical\n)\n</code></pre> <p>From the code block above you can see that the <code>Enum</code> data type requires the upfront while the categorical data type infers the categories.</p>"},{"location":"user-guide/concepts/data-types/categoricals/#categorical-data-type","title":"<code>Categorical</code> data type","text":"<p>The <code>Categorical</code> data type is a flexible one. Polars will add categories on the fly if it sees them. This sounds like a strictly better version compared to the <code>Enum</code> data type as we can simply infer the categories, however inferring comes at a cost. The main cost here is we have no control over our encodings.</p> <p>Consider the following scenario where we append the following two categorical <code>Series</code></p>  Python <pre><code>cat_series = pl.Series(\n    [\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"], dtype=pl.Categorical\n)\ncat2_series = pl.Series(\n    [\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"], dtype=pl.Categorical\n)\n# Triggers a CategoricalRemappingWarning: Local categoricals have different encodings, expensive re-encoding is done\nprint(cat_series.append(cat2_series))\n</code></pre> <p>Polars encodes the string values in order as they appear. So the series would look like this:</p> cat_series cat2_series Physical 0 1 2 2 0 Categories Polar Panda Brown Physical 0 1 1 2 2 Categories Panda Brown Polar <p>Combining the <code>Series</code> becomes a non-trivial task which is expensive as the physical value of <code>0</code> represents something different in both <code>Series</code>. Polars does support these types of operations for convenience, however in general these should be avoided due to its slower performance as it requires making both encodings compatible first before doing any merge operations.</p>"},{"location":"user-guide/concepts/data-types/categoricals/#using-the-global-string-cache","title":"Using the global string cache","text":"<p>One way to handle this problem is to enable a <code>StringCache</code>. When you enable the <code>StringCache</code> strings are no longer encoded in the order they appear on a per-column basis. Instead, the string cache ensures a single encoding for each string. The string <code>Polar</code> will always map the same physical for all categorical columns made under the string cache. Merge operations (e.g. appends, joins) are cheap as there is no need to make the encodings compatible first, solving the problem we had above.</p>  Python <pre><code>with pl.StringCache():\n    cat_series = pl.Series(\n        [\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"], dtype=pl.Categorical\n    )\n    cat2_series = pl.Series(\n        [\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"], dtype=pl.Categorical\n    )\n    print(cat_series.append(cat2_series))\n</code></pre> <p>However, the string cache does come at a small performance hit during construction of the <code>Series</code> as we need to look up / insert the string value in the cache. Therefore, it is preferred to use the <code>Enum</code> Data Type if you know your categories in advance.</p>"},{"location":"user-guide/concepts/data-types/categoricals/#enum-data-type","title":"<code>Enum data type</code>","text":"<p>In the <code>Enum</code> data type we specify the categories in advance. This way we ensure categoricals from different columns or different datasets have the same encoding and there is no need for expensive re-encoding or cache lookups.</p>  Python <pre><code>dtype = pl.Enum([\"Polar\", \"Panda\", \"Brown\"])\ncat_series = pl.Series([\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"], dtype=dtype)\ncat2_series = pl.Series([\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"], dtype=dtype)\nprint(cat_series.append(cat2_series))\n</code></pre> <p>Polars will raise an <code>OutOfBounds</code> error when a value is encountered which is not specified in the <code>Enum</code>.</p>  Python <pre><code>dtype = pl.Enum([\"Polar\", \"Panda\", \"Brown\"])\ntry:\n    cat_series = pl.Series([\"Polar\", \"Panda\", \"Brown\", \"Black\"], dtype=dtype)\nexcept Exception as e:\n    print(e)\n</code></pre> <pre><code>conversion from `str` to `enum` failed in column '' for 1 out of 4 values: [\"Black\"]\n\nEnsure that all values in the input column are present in the categories of the enum datatype.\n</code></pre>"},{"location":"user-guide/concepts/data-types/categoricals/#comparisons","title":"Comparisons","text":"<p>The following types of comparisons operators are allowed for categorical data:</p> <ul> <li>Categorical vs Categorical</li> <li>Categorical vs String</li> </ul>"},{"location":"user-guide/concepts/data-types/categoricals/#categorical-type","title":"<code>Categorical</code> Type","text":"<p>For the <code>Categorical</code> type comparisons are valid if they have the same global cache set or if they have the same underlying categories in the same order.</p>  Python <pre><code>with pl.StringCache():\n    cat_series = pl.Series([\"Brown\", \"Panda\", \"Polar\"], dtype=pl.Categorical)\n    cat_series2 = pl.Series([\"Polar\", \"Panda\", \"Black\"], dtype=pl.Categorical)\n    print(cat_series == cat_series2)\n</code></pre> <pre><code>shape: (3,)\nSeries: '' [bool]\n[\n    false\n    true\n    false\n]\n</code></pre> <p>For <code>Categorical</code> vs <code>String</code> comparisons Polars uses lexical ordering to determine the result:</p>  Python <pre><code>cat_series = pl.Series([\"Brown\", \"Panda\", \"Polar\"], dtype=pl.Categorical)\nprint(cat_series &lt;= \"Cat\")\n</code></pre> <pre><code>shape: (3,)\nSeries: '' [bool]\n[\n    true\n    false\n    false\n]\n</code></pre>  Python <pre><code>cat_series = pl.Series([\"Brown\", \"Panda\", \"Polar\"], dtype=pl.Categorical)\ncat_series_utf = pl.Series([\"Panda\", \"Panda\", \"Polar\"])\nprint(cat_series &lt;= cat_series_utf)\n</code></pre> <pre><code>shape: (3,)\nSeries: '' [bool]\n[\n    true\n    true\n    true\n]\n</code></pre>"},{"location":"user-guide/concepts/data-types/categoricals/#enum-type","title":"<code>Enum</code> Type","text":"<p>For <code>Enum</code> type comparisons are valid if they have the same categories.</p>  Python <pre><code>dtype = pl.Enum([\"Polar\", \"Panda\", \"Brown\"])\ncat_series = pl.Series([\"Brown\", \"Panda\", \"Polar\"], dtype=dtype)\ncat_series2 = pl.Series([\"Polar\", \"Panda\", \"Brown\"], dtype=dtype)\nprint(cat_series == cat_series2)\n</code></pre> <pre><code>shape: (3,)\nSeries: '' [bool]\n[\n    false\n    true\n    false\n]\n</code></pre> <p>For <code>Enum</code> vs <code>String</code> comparisons the order within the categories is used instead of lexical ordering. In order for a comparison to be valid all values in the <code>String</code> column should be present in the <code>Enum</code> categories list.</p>  Python <pre><code>try:\n    cat_series = pl.Series(\n        [\"Low\", \"Medium\", \"High\"], dtype=pl.Enum([\"Low\", \"Medium\", \"High\"])\n    )\n    cat_series &lt;= \"Excellent\"\nexcept Exception as e:\n    print(e)\n</code></pre> <pre><code>conversion from `str` to `enum` failed in column '' for 1 out of 1 values: [\"Excellent\"]\n\nEnsure that all values in the input column are present in the categories of the enum datatype.\n</code></pre>  Python <pre><code>dtype = pl.Enum([\"Low\", \"Medium\", \"High\"])\ncat_series = pl.Series([\"Low\", \"Medium\", \"High\"], dtype=dtype)\nprint(cat_series &lt;= \"Medium\")\n</code></pre> <pre><code>shape: (3,)\nSeries: '' [bool]\n[\n    true\n    true\n    false\n]\n</code></pre>  Python <pre><code>dtype = pl.Enum([\"Low\", \"Medium\", \"High\"])\ncat_series = pl.Series([\"Low\", \"Medium\", \"High\"], dtype=dtype)\ncat_series2 = pl.Series([\"High\", \"High\", \"Low\"], dtype=dtype)\nprint(cat_series &lt;= cat_series2)\n</code></pre> <pre><code>shape: (3,)\nSeries: '' [bool]\n[\n    true\n    true\n    false\n]\n</code></pre>"},{"location":"user-guide/concepts/data-types/overview/","title":"Overview","text":"<p>Polars is entirely based on Arrow data types and backed by Arrow memory arrays. This makes data processing cache-efficient and well-supported for Inter Process Communication. Most data types follow the exact implementation from Arrow, with the exception of <code>String</code> (this is actually <code>LargeUtf8</code>), <code>Categorical</code>, and <code>Object</code> (support is limited). The data types are:</p> Group Type Details Numeric <code>Int8</code> 8-bit signed integer. <code>Int16</code> 16-bit signed integer. <code>Int32</code> 32-bit signed integer. <code>Int64</code> 64-bit signed integer. <code>UInt8</code> 8-bit unsigned integer. <code>UInt16</code> 16-bit unsigned integer. <code>UInt32</code> 32-bit unsigned integer. <code>UInt64</code> 64-bit unsigned integer. <code>Float32</code> 32-bit floating point. <code>Float64</code> 64-bit floating point. Nested <code>Struct</code> A struct array is represented as a <code>Vec&lt;Series&gt;</code> and is useful to pack multiple/heterogeneous values in a single column. <code>List</code> A list array contains a child array containing the list values and an offset array. (this is actually Arrow <code>LargeList</code> internally). Temporal <code>Date</code> Date representation, internally represented as days since UNIX epoch encoded by a 32-bit signed integer. <code>Datetime</code> Datetime representation, internally represented as microseconds since UNIX epoch encoded by a 64-bit signed integer. <code>Duration</code> A timedelta type, internally represented as microseconds. Created when subtracting <code>Date/Datetime</code>. <code>Time</code> Time representation, internally represented as nanoseconds since midnight. Other <code>Boolean</code> Boolean type effectively bit packed. <code>String</code> String data (this is actually Arrow <code>LargeUtf8</code> internally). <code>Binary</code> Store data as bytes. <code>Object</code> A limited supported data type that can be any value. <code>Categorical</code> A categorical encoding of a set of strings. <code>Enum</code> A fixed categorical encoding of a set of strings. <p>To learn more about the internal representation of these data types, check the Arrow columnar format.</p>"},{"location":"user-guide/concepts/data-types/overview/#floating-point","title":"Floating Point","text":"<p>Polars generally follows the IEEE 754 floating point standard for <code>Float32</code> and <code>Float64</code>, with some exceptions:</p> <ul> <li>Any NaN compares equal to any other NaN, and greater than any non-NaN value.</li> <li>Operations do not guarantee any particular behavior on the sign of zero or NaN,   nor on the payload of NaN values. This is not just limited to arithmetic operations,   e.g. a sort or group by operation may canonicalize all zeroes to +0 and all NaNs   to a positive NaN without payload for efficient equality checks.</li> </ul> <p>Polars always attempts to provide reasonably accurate results for floating point computations but does not provide guarantees on the error unless mentioned otherwise. Generally speaking 100% accurate results are infeasibly expensive to acquire (requiring much larger internal representations than 64-bit floats), and thus some error is always to be expected.</p>"},{"location":"user-guide/expressions/","title":"Expressions","text":"<p>In the <code>Contexts</code> sections we outlined what <code>Expressions</code> are and how they are invaluable. In this section we will focus on the <code>Expressions</code> themselves. Each section gives an overview of what they do and provide additional examples.</p> <ul> <li>Operators</li> <li>Column selections</li> <li>Functions</li> <li>Casting</li> <li>Strings</li> <li>Aggregation</li> <li>Missing data</li> <li>Window</li> <li>Folds</li> <li>Lists</li> <li>Plugins</li> <li>User-defined functions</li> <li>Structs</li> <li>Numpy</li> </ul>"},{"location":"user-guide/expressions/aggregation/","title":"Aggregation","text":"<p>Polars implements a powerful syntax defined not only in its lazy API, but also in its eager API. Let's take a look at what that means.</p> <p>We can start with the simple US congress <code>dataset</code>.</p>  Python Rust <p> <code>DataFrame</code> \u00b7 <code>Categorical</code> <pre><code>url = \"https://theunitedstates.io/congress-legislators/legislators-historical.csv\"\n\ndtypes = {\n    \"first_name\": pl.Categorical,\n    \"gender\": pl.Categorical,\n    \"type\": pl.Categorical,\n    \"state\": pl.Categorical,\n    \"party\": pl.Categorical,\n}\n\ndataset = pl.read_csv(url, dtypes=dtypes).with_columns(\n    pl.col(\"birthday\").str.to_date(strict=False)\n)\n</code></pre></p> <p> <code>DataFrame</code> \u00b7 <code>Categorical</code> \u00b7  Available on feature dtype-categorical <pre><code>use std::io::Cursor;\n\nuse reqwest::blocking::Client;\n\nlet url = \"https://theunitedstates.io/congress-legislators/legislators-historical.csv\";\n\nlet mut schema = Schema::new();\nschema.with_column(\n    \"first_name\".into(),\n    DataType::Categorical(None, Default::default()),\n);\nschema.with_column(\n    \"gender\".into(),\n    DataType::Categorical(None, Default::default()),\n);\nschema.with_column(\n    \"type\".into(),\n    DataType::Categorical(None, Default::default()),\n);\nschema.with_column(\n    \"state\".into(),\n    DataType::Categorical(None, Default::default()),\n);\nschema.with_column(\n    \"party\".into(),\n    DataType::Categorical(None, Default::default()),\n);\nschema.with_column(\"birthday\".into(), DataType::Date);\n\nlet data: Vec&lt;u8&gt; = Client::new().get(url).send()?.text()?.bytes().collect();\n\nlet dataset = CsvReader::new(Cursor::new(data))\n    .has_header(true)\n    .with_dtypes(Some(Arc::new(schema)))\n    .with_try_parse_dates(true)\n    .finish()?;\n\nprintln!(\"{}\", &amp;dataset);\n</code></pre></p>"},{"location":"user-guide/expressions/aggregation/#basic-aggregations","title":"Basic aggregations","text":"<p>You can easily combine different aggregations by adding multiple expressions in a <code>list</code>. There is no upper bound on the number of aggregations you can do, and you can make any combination you want. In the snippet below we do the following aggregations:</p> <p>Per GROUP <code>\"first_name\"</code> we</p> <ul> <li>count the number of rows in the group:<ul> <li>short form: <code>pl.count(\"party\")</code></li> <li>full form: <code>pl.col(\"party\").count()</code></li> </ul> </li> <li>aggregate the gender values groups:<ul> <li>full form: <code>pl.col(\"gender\")</code></li> </ul> </li> <li>get the first value of column <code>\"last_name\"</code> in the group:<ul> <li>short form: <code>pl.first(\"last_name\")</code> (not available in Rust)</li> <li>full form: <code>pl.col(\"last_name\").first()</code></li> </ul> </li> </ul> <p>Besides the aggregation, we immediately sort the result and limit to the top <code>5</code> so that we have a nice summary overview.</p>  Python Rust <p> <code>group_by</code> <pre><code>q = (\n    dataset.lazy()\n    .group_by(\"first_name\")\n    .agg(\n        pl.len(),\n        pl.col(\"gender\"),\n        pl.first(\"last_name\"),\n    )\n    .sort(\"len\", descending=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .clone()\n    .lazy()\n    .group_by([\"first_name\"])\n    .agg([len(), col(\"gender\"), col(\"last_name\").first()])\n    .sort(\n        [\"len\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 first_name \u2506 len  \u2506 gender            \u2506 last_name \u2502\n\u2502 ---        \u2506 ---  \u2506 ---               \u2506 ---       \u2502\n\u2502 cat        \u2506 u32  \u2506 list[cat]         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 John       \u2506 1256 \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Walker    \u2502\n\u2502 William    \u2506 1022 \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Few       \u2502\n\u2502 James      \u2506 714  \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Armstrong \u2502\n\u2502 Thomas     \u2506 453  \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Tucker    \u2502\n\u2502 Charles    \u2506 439  \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Carroll   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/aggregation/#conditionals","title":"Conditionals","text":"<p>It's that easy! Let's turn it up a notch. Let's say we want to know how many delegates of a \"state\" are \"Pro\" or \"Anti\" administration. We could directly query that in the aggregation without the need of a <code>lambda</code> or grooming the <code>DataFrame</code>.</p>  Python Rust <p> <code>group_by</code> <pre><code>q = (\n    dataset.lazy()\n    .group_by(\"state\")\n    .agg(\n        (pl.col(\"party\") == \"Anti-Administration\").sum().alias(\"anti\"),\n        (pl.col(\"party\") == \"Pro-Administration\").sum().alias(\"pro\"),\n    )\n    .sort(\"pro\", descending=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .clone()\n    .lazy()\n    .group_by([\"state\"])\n    .agg([\n        (col(\"party\").eq(lit(\"Anti-Administration\")))\n            .sum()\n            .alias(\"anti\"),\n        (col(\"party\").eq(lit(\"Pro-Administration\")))\n            .sum()\n            .alias(\"pro\"),\n    ])\n    .sort(\n        [\"pro\"],\n        SortMultipleOptions::default().with_order_descending(true),\n    )\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 anti \u2506 pro \u2502\n\u2502 ---   \u2506 ---  \u2506 --- \u2502\n\u2502 cat   \u2506 u32  \u2506 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 CT    \u2506 0    \u2506 3   \u2502\n\u2502 NJ    \u2506 0    \u2506 3   \u2502\n\u2502 NC    \u2506 1    \u2506 2   \u2502\n\u2502 PA    \u2506 1    \u2506 1   \u2502\n\u2502 VA    \u2506 3    \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Similarly, this could also be done with a nested GROUP BY, but that doesn't help show off some of these nice features. \ud83d\ude09</p>  Python Rust <p> <code>group_by</code> <pre><code>q = (\n    dataset.lazy()\n    .group_by(\"state\", \"party\")\n    .agg(pl.count(\"party\").alias(\"count\"))\n    .filter(\n        (pl.col(\"party\") == \"Anti-Administration\")\n        | (pl.col(\"party\") == \"Pro-Administration\")\n    )\n    .sort(\"count\", descending=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .clone()\n    .lazy()\n    .group_by([\"state\", \"party\"])\n    .agg([col(\"party\").count().alias(\"count\")])\n    .filter(\n        col(\"party\")\n            .eq(lit(\"Anti-Administration\"))\n            .or(col(\"party\").eq(lit(\"Pro-Administration\"))),\n    )\n    .sort(\n        [\"count\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 party               \u2506 count \u2502\n\u2502 ---   \u2506 ---                 \u2506 ---   \u2502\n\u2502 cat   \u2506 cat                 \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 NJ    \u2506 Pro-Administration  \u2506 3     \u2502\n\u2502 CT    \u2506 Pro-Administration  \u2506 3     \u2502\n\u2502 VA    \u2506 Anti-Administration \u2506 3     \u2502\n\u2502 NC    \u2506 Pro-Administration  \u2506 2     \u2502\n\u2502 VT    \u2506 Anti-Administration \u2506 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/aggregation/#filtering","title":"Filtering","text":"<p>We can also filter the groups. Let's say we want to compute a mean per group, but we don't want to include all values from that group, and we also don't want to filter the rows from the <code>DataFrame</code> (because we need those rows for another aggregation).</p> <p>In the example below we show how this can be done.</p> <p>Note</p> <p>Note that we can make Python functions for clarity. These functions don't cost us anything. That is because we only create Polars expressions, we don't apply a custom function over a <code>Series</code> during runtime of the query. Of course, you can make functions that return expressions in Rust, too.</p>  Python Rust <p> <code>group_by</code> <pre><code>from datetime import date\n\n\ndef compute_age():\n    return date.today().year - pl.col(\"birthday\").dt.year()\n\n\ndef avg_birthday(gender: str) -&gt; pl.Expr:\n    return (\n        compute_age()\n        .filter(pl.col(\"gender\") == gender)\n        .mean()\n        .alias(f\"avg {gender} birthday\")\n    )\n\n\nq = (\n    dataset.lazy()\n    .group_by(\"state\")\n    .agg(\n        avg_birthday(\"M\"),\n        avg_birthday(\"F\"),\n        (pl.col(\"gender\") == \"M\").sum().alias(\"# male\"),\n        (pl.col(\"gender\") == \"F\").sum().alias(\"# female\"),\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>fn compute_age() -&gt; Expr {\n    lit(2022) - col(\"birthday\").dt().year()\n}\n\nfn avg_birthday(gender: &amp;str) -&gt; Expr {\n    compute_age()\n        .filter(col(\"gender\").eq(lit(gender)))\n        .mean()\n        .alias(&amp;format!(\"avg {} birthday\", gender))\n}\n\nlet df = dataset\n    .clone()\n    .lazy()\n    .group_by([\"state\"])\n    .agg([\n        avg_birthday(\"M\"),\n        avg_birthday(\"F\"),\n        (col(\"gender\").eq(lit(\"M\"))).sum().alias(\"# male\"),\n        (col(\"gender\").eq(lit(\"F\"))).sum().alias(\"# female\"),\n    ])\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 avg M birthday \u2506 avg F birthday \u2506 # male \u2506 # female \u2502\n\u2502 ---   \u2506 ---            \u2506 ---            \u2506 ---    \u2506 ---      \u2502\n\u2502 cat   \u2506 f64            \u2506 f64            \u2506 u32    \u2506 u32      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AK    \u2506 123.411765     \u2506 null           \u2506 17     \u2506 0        \u2502\n\u2502 PA    \u2506 182.724846     \u2506 94.857143      \u2506 1050   \u2506 7        \u2502\n\u2502 MD    \u2506 190.280899     \u2506 96.375         \u2506 298    \u2506 8        \u2502\n\u2502 GA    \u2506 175.106529     \u2506 96.777778      \u2506 310    \u2506 9        \u2502\n\u2502 MI    \u2506 153.244828     \u2506 85.75          \u2506 292    \u2506 8        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/aggregation/#sorting","title":"Sorting","text":"<p>It's common to see a <code>DataFrame</code> being sorted for the sole purpose of managing the ordering during a GROUP BY operation. Let's say that we want to get the names of the oldest and youngest politicians per state. We could SORT and GROUP BY.</p>  Python Rust <p> <code>group_by</code> <pre><code>def get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\", descending=True)\n    .group_by(\"state\")\n    .agg(\n        get_person().first().alias(\"youngest\"),\n        get_person().last().alias(\"oldest\"),\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>fn get_person() -&gt; Expr {\n    col(\"first_name\") + lit(\" \") + col(\"last_name\")\n}\n\nlet df = dataset\n    .clone()\n    .lazy()\n    .sort(\n        [\"birthday\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .group_by([\"state\"])\n    .agg([\n        get_person().first().alias(\"youngest\"),\n        get_person().last().alias(\"oldest\"),\n    ])\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 youngest            \u2506 oldest          \u2502\n\u2502 ---   \u2506 ---                 \u2506 ---             \u2502\n\u2502 cat   \u2506 str                 \u2506 str             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 KS    \u2506 Steven Watkins      \u2506 James Lane      \u2502\n\u2502 SC    \u2506 Ralph Izard         \u2506 Thomas Sumter   \u2502\n\u2502 PR    \u2506 An\u00edbal Acevedo-Vil\u00e1 \u2506 Tulio Larrinaga \u2502\n\u2502 MO    \u2506 Spencer Pettis      \u2506 Rufus Easton    \u2502\n\u2502 DK    \u2506 George Mathews      \u2506 John Todd       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>However, if we also want to sort the names alphabetically, this breaks. Luckily we can sort in a <code>group_by</code> context separate from the <code>DataFrame</code>.</p>  Python Rust <p> <code>group_by</code> <pre><code>def get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\", descending=True)\n    .group_by(\"state\")\n    .agg(\n        get_person().first().alias(\"youngest\"),\n        get_person().last().alias(\"oldest\"),\n        get_person().sort().first().alias(\"alphabetical_first\"),\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .clone()\n    .lazy()\n    .sort(\n        [\"birthday\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .group_by([\"state\"])\n    .agg([\n        get_person().first().alias(\"youngest\"),\n        get_person().last().alias(\"oldest\"),\n        get_person()\n            .sort(Default::default())\n            .first()\n            .alias(\"alphabetical_first\"),\n    ])\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 youngest          \u2506 oldest          \u2506 alphabetical_first \u2502\n\u2502 ---   \u2506 ---               \u2506 ---             \u2506 ---                \u2502\n\u2502 cat   \u2506 str               \u2506 str             \u2506 str                \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 AK    \u2506 Mark Begich       \u2506 Thomas Cale     \u2506 Anthony Dimond     \u2502\n\u2502 MD    \u2506 Benjamin Contee   \u2506 William Smith   \u2506 Albert Blakeney    \u2502\n\u2502 MI    \u2506 Edward Bradley    \u2506 Gabriel Richard \u2506 Aaron Bliss        \u2502\n\u2502 PA    \u2506 Thomas Fitzsimons \u2506 Israel Jacobs   \u2506 Aaron Kreider      \u2502\n\u2502 IN    \u2506 Waller Taylor     \u2506 John Test       \u2506 Abraham Brick      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can even sort by another column in the <code>group_by</code> context. If we want to know if the alphabetically sorted name is male or female we could add: <code>pl.col(\"gender\").sort_by(\"first_name\").first().alias(\"gender\")</code></p>  Python Rust <p> <code>group_by</code> <pre><code>def get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\", descending=True)\n    .group_by(\"state\")\n    .agg(\n        get_person().first().alias(\"youngest\"),\n        get_person().last().alias(\"oldest\"),\n        get_person().sort().first().alias(\"alphabetical_first\"),\n        pl.col(\"gender\")\n        .sort_by(pl.col(\"first_name\").cast(pl.Categorical(\"lexical\")))\n        .first(),\n    )\n    .sort(\"state\")\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>group_by</code> <pre><code>let df = dataset\n    .clone()\n    .lazy()\n    .sort(\n        [\"birthday\"],\n        SortMultipleOptions::default()\n            .with_order_descending(true)\n            .with_nulls_last(true),\n    )\n    .group_by([\"state\"])\n    .agg([\n        get_person().first().alias(\"youngest\"),\n        get_person().last().alias(\"oldest\"),\n        get_person()\n            .sort(Default::default())\n            .first()\n            .alias(\"alphabetical_first\"),\n        col(\"gender\")\n            .sort_by([\"first_name\"], SortMultipleOptions::default())\n            .first()\n            .alias(\"gender\"),\n    ])\n    .sort([\"state\"], SortMultipleOptions::default())\n    .limit(5)\n    .collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 youngest              \u2506 oldest          \u2506 alphabetical_first \u2506 gender \u2502\n\u2502 ---   \u2506 ---                   \u2506 ---             \u2506 ---                \u2506 ---    \u2502\n\u2502 cat   \u2506 str                   \u2506 str             \u2506 str                \u2506 cat    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 NY    \u2506 Cornelius Schoonmaker \u2506 Philip Schuyler \u2506 A. Foster          \u2506 M      \u2502\n\u2502 FL    \u2506 Charles Downing       \u2506 Joseph White    \u2506 Abijah Gilbert     \u2506 M      \u2502\n\u2502 NJ    \u2506 Lambert Cadwalader    \u2506 Abraham Clark   \u2506 Aaron Kitchell     \u2506 M      \u2502\n\u2502 CT    \u2506 Henry Edwards         \u2506 Roger Sherman   \u2506 Abner Sibal        \u2506 M      \u2502\n\u2502 PA    \u2506 Thomas Fitzsimons     \u2506 Israel Jacobs   \u2506 Aaron Kreider      \u2506 M      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/aggregation/#do-not-kill-parallelization","title":"Do not kill parallelization","text":"<p>Python Users Only</p> <p>The following section is specific to Python, and doesn't apply to Rust. Within Rust, blocks and closures (lambdas) can, and will, be executed concurrently.</p> <p>We have all heard that Python is slow, and does \"not scale.\" Besides the overhead of running \"slow\" bytecode, Python has to remain within the constraints of the Global Interpreter Lock (GIL). This means that if you were to use a <code>lambda</code> or a custom Python function to apply during a parallelized phase, Polars speed is capped running Python code preventing any multiple threads from executing the function.</p> <p>This all feels terribly limiting, especially because we often need those <code>lambda</code> functions in a <code>.group_by()</code> step, for example. This approach is still supported by Polars, but keeping in mind bytecode and the GIL costs have to be paid. It is recommended to try to solve your queries using the expression syntax before moving to <code>lambdas</code>. If you want to learn more about using <code>lambdas</code>, go to the user defined functions section.</p>"},{"location":"user-guide/expressions/aggregation/#conclusion","title":"Conclusion","text":"<p>In the examples above we've seen that we can do a lot by combining expressions. By doing so we delay the use of custom Python functions that slow down the queries (by the slow nature of Python AND the GIL).</p> <p>If we are missing a type expression let us know by opening a feature request!</p>"},{"location":"user-guide/expressions/casting/","title":"Casting","text":"<p>Casting converts the underlying <code>DataType</code> of a column to a new one. Polars uses Arrow to manage the data in memory and relies on the compute kernels in the Rust implementation to do the conversion. Casting is available with the <code>cast()</code> method.</p> <p>The <code>cast</code> method includes a <code>strict</code> parameter that determines how Polars behaves when it encounters a value that can't be converted from the source <code>DataType</code> to the target <code>DataType</code>. By default, <code>strict=True</code>, which means that Polars will throw an error to notify the user of the failed conversion and provide details on the values that couldn't be cast. On the other hand, if <code>strict=False</code>, any values that can't be converted to the target <code>DataType</code> will be quietly converted to <code>null</code>.</p>"},{"location":"user-guide/expressions/casting/#numerics","title":"Numerics","text":"<p>Let's take a look at the following <code>DataFrame</code> which contains both integers and floating point numbers.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"integers\": [1, 2, 3, 4, 5],\n        \"big_integers\": [1, 10000002, 3, 10000004, 10000005],\n        \"floats\": [4.0, 5.0, 6.0, 7.0, 8.0],\n        \"floats_with_decimal\": [4.532, 5.5, 6.5, 7.5, 8.5],\n    }\n)\n\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = df! (\n    \"integers\"=&gt; &amp;[1, 2, 3, 4, 5],\n    \"big_integers\"=&gt; &amp;[1, 10000002, 3, 10000004, 10000005],\n    \"floats\"=&gt; &amp;[4.0, 5.0, 6.0, 7.0, 8.0],\n    \"floats_with_decimal\"=&gt; &amp;[4.532, 5.5, 6.5, 7.5, 8.5],\n)?;\n\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers \u2506 big_integers \u2506 floats \u2506 floats_with_decimal \u2502\n\u2502 ---      \u2506 ---          \u2506 ---    \u2506 ---                 \u2502\n\u2502 i64      \u2506 i64          \u2506 f64    \u2506 f64                 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 1            \u2506 4.0    \u2506 4.532               \u2502\n\u2502 2        \u2506 10000002     \u2506 5.0    \u2506 5.5                 \u2502\n\u2502 3        \u2506 3            \u2506 6.0    \u2506 6.5                 \u2502\n\u2502 4        \u2506 10000004     \u2506 7.0    \u2506 7.5                 \u2502\n\u2502 5        \u2506 10000005     \u2506 8.0    \u2506 8.5                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To perform casting operations between floats and integers, or vice versa, we can invoke the <code>cast()</code> function.</p>  Python Rust <p> <code>cast</code> <pre><code>out = df.select(\n    pl.col(\"integers\").cast(pl.Float32).alias(\"integers_as_floats\"),\n    pl.col(\"floats\").cast(pl.Int32).alias(\"floats_as_integers\"),\n    pl.col(\"floats_with_decimal\")\n    .cast(pl.Int32)\n    .alias(\"floats_with_decimal_as_integers\"),\n)\nprint(out)\n</code></pre></p> <p> <code>cast</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"integers\")\n            .cast(DataType::Float32)\n            .alias(\"integers_as_floats\"),\n        col(\"floats\")\n            .cast(DataType::Int32)\n            .alias(\"floats_as_integers\"),\n        col(\"floats_with_decimal\")\n            .cast(DataType::Int32)\n            .alias(\"floats_with_decimal_as_integers\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers_as_floats \u2506 floats_as_integers \u2506 floats_with_decimal_as_integer\u2026 \u2502\n\u2502 ---                \u2506 ---                \u2506 ---                             \u2502\n\u2502 f32                \u2506 i32                \u2506 i32                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.0                \u2506 4                  \u2506 4                               \u2502\n\u2502 2.0                \u2506 5                  \u2506 5                               \u2502\n\u2502 3.0                \u2506 6                  \u2506 6                               \u2502\n\u2502 4.0                \u2506 7                  \u2506 7                               \u2502\n\u2502 5.0                \u2506 8                  \u2506 8                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note that in the case of decimal values these are rounded downwards when casting to an integer.</p>"},{"location":"user-guide/expressions/casting/#downcast","title":"Downcast","text":"<p>Reducing the memory footprint is also achievable by modifying the number of bits allocated to an element. As an illustration, the code below demonstrates how casting from <code>Int64</code> to <code>Int16</code> and from <code>Float64</code> to <code>Float32</code> can be used to lower memory usage.</p>  Python Rust <p> <code>cast</code> <pre><code>out = df.select(\n    pl.col(\"integers\").cast(pl.Int16).alias(\"integers_smallfootprint\"),\n    pl.col(\"floats\").cast(pl.Float32).alias(\"floats_smallfootprint\"),\n)\nprint(out)\n</code></pre></p> <p> <code>cast</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"integers\")\n            .cast(DataType::Int16)\n            .alias(\"integers_smallfootprint\"),\n        col(\"floats\")\n            .cast(DataType::Float32)\n            .alias(\"floats_smallfootprint\"),\n    ])\n    .collect();\nmatch out {\n    Ok(out) =&gt; println!(\"{}\", &amp;out),\n    Err(e) =&gt; println!(\"{:?}\", e),\n};\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers_smallfootprint \u2506 floats_smallfootprint \u2502\n\u2502 ---                     \u2506 ---                   \u2502\n\u2502 i16                     \u2506 f32                   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1                       \u2506 4.0                   \u2502\n\u2502 2                       \u2506 5.0                   \u2502\n\u2502 3                       \u2506 6.0                   \u2502\n\u2502 4                       \u2506 7.0                   \u2502\n\u2502 5                       \u2506 8.0                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/casting/#overflow","title":"Overflow","text":"<p>When performing downcasting, it is crucial to ensure that the chosen number of bits (such as 64, 32, or 16) is sufficient to accommodate the largest and smallest numbers in the column. For example, using a 32-bit signed integer (<code>Int32</code>) allows handling integers within the range of -2147483648 to +2147483647, while using <code>Int8</code> covers integers between -128 to 127. Attempting to cast to a <code>DataType</code> that is too small will result in a <code>ComputeError</code> thrown by Polars, as the operation is not supported.</p>  Python Rust <p> <code>cast</code> <pre><code>try:\n    out = df.select(pl.col(\"big_integers\").cast(pl.Int8))\n    print(out)\nexcept Exception as e:\n    print(e)\n</code></pre></p> <p> <code>cast</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([col(\"big_integers\").strict_cast(DataType::Int8)])\n    .collect();\nmatch out {\n    Ok(out) =&gt; println!(\"{}\", &amp;out),\n    Err(e) =&gt; println!(\"{:?}\", e),\n};\n</code></pre></p> <pre><code>conversion from `i64` to `i8` failed in column 'big_integers' for 3 out of 5 values: [10000002, 10000004, 10000005]\n</code></pre> <p>You can set the <code>strict</code> parameter to <code>False</code>, this converts values that are overflowing to null values.</p>  Python Rust <p> <code>cast</code> <pre><code>out = df.select(pl.col(\"big_integers\").cast(pl.Int8, strict=False))\nprint(out)\n</code></pre></p> <p> <code>cast</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([col(\"big_integers\").cast(DataType::Int8)])\n    .collect();\nmatch out {\n    Ok(out) =&gt; println!(\"{}\", &amp;out),\n    Err(e) =&gt; println!(\"{:?}\", e),\n};\n</code></pre></p> <pre><code>shape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 big_integers \u2502\n\u2502 ---          \u2502\n\u2502 i8           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1            \u2502\n\u2502 null         \u2502\n\u2502 3            \u2502\n\u2502 null         \u2502\n\u2502 null         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/casting/#strings","title":"Strings","text":"<p>Strings can be casted to numerical data types and vice versa:</p>  Python Rust <p> <code>cast</code> <pre><code>df = pl.DataFrame(\n    {\n        \"integers\": [1, 2, 3, 4, 5],\n        \"float\": [4.0, 5.03, 6.0, 7.0, 8.0],\n        \"floats_as_string\": [\"4.0\", \"5.0\", \"6.0\", \"7.0\", \"8.0\"],\n    }\n)\n\nout = df.select(\n    pl.col(\"integers\").cast(pl.String),\n    pl.col(\"float\").cast(pl.String),\n    pl.col(\"floats_as_string\").cast(pl.Float64),\n)\nprint(out)\n</code></pre></p> <p> <code>cast</code> <pre><code>let df = df! (\n        \"integers\" =&gt; &amp;[1, 2, 3, 4, 5],\n        \"float\" =&gt; &amp;[4.0, 5.03, 6.0, 7.0, 8.0],\n        \"floats_as_string\" =&gt; &amp;[\"4.0\", \"5.0\", \"6.0\", \"7.0\", \"8.0\"],\n)?;\n\nlet out = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"integers\").cast(DataType::String),\n        col(\"float\").cast(DataType::String),\n        col(\"floats_as_string\").cast(DataType::Float64),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers \u2506 float \u2506 floats_as_string \u2502\n\u2502 ---      \u2506 ---   \u2506 ---              \u2502\n\u2502 str      \u2506 str   \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 4.0   \u2506 4.0              \u2502\n\u2502 2        \u2506 5.03  \u2506 5.0              \u2502\n\u2502 3        \u2506 6.0   \u2506 6.0              \u2502\n\u2502 4        \u2506 7.0   \u2506 7.0              \u2502\n\u2502 5        \u2506 8.0   \u2506 8.0              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In case the column contains a non-numerical value, Polars will throw a <code>ComputeError</code> detailing the conversion error. Setting <code>strict=False</code> will convert the non float value to <code>null</code>.</p>  Python Rust <p> <code>cast</code> <pre><code>df = pl.DataFrame({\"strings_not_float\": [\"4.0\", \"not_a_number\", \"6.0\", \"7.0\", \"8.0\"]})\ntry:\n    out = df.select(pl.col(\"strings_not_float\").cast(pl.Float64))\n    print(out)\nexcept Exception as e:\n    print(e)\n</code></pre></p> <p> <code>cast</code> <pre><code>let df = df! (\"strings_not_float\"=&gt; [\"4.0\", \"not_a_number\", \"6.0\", \"7.0\", \"8.0\"])?;\n\nlet out = df\n    .clone()\n    .lazy()\n    .select([col(\"strings_not_float\").cast(DataType::Float64)])\n    .collect();\nmatch out {\n    Ok(out) =&gt; println!(\"{}\", &amp;out),\n    Err(e) =&gt; println!(\"{:?}\", e),\n};\n</code></pre></p> <pre><code>conversion from `str` to `f64` failed in column 'strings_not_float' for 1 out of 5 values: [\"not_a_number\"]\n</code></pre>"},{"location":"user-guide/expressions/casting/#booleans","title":"Booleans","text":"<p>Booleans can be expressed as either 1 (<code>True</code>) or 0 (<code>False</code>). It's possible to perform casting operations between a numerical <code>DataType</code> and a boolean, and vice versa. However, keep in mind that casting from a string (<code>String</code>) to a boolean is not permitted.</p>  Python Rust <p> <code>cast</code> <pre><code>df = pl.DataFrame(\n    {\n        \"integers\": [-1, 0, 2, 3, 4],\n        \"floats\": [0.0, 1.0, 2.0, 3.0, 4.0],\n        \"bools\": [True, False, True, False, True],\n    }\n)\n\nout = df.select(pl.col(\"integers\").cast(pl.Boolean), pl.col(\"floats\").cast(pl.Boolean))\nprint(out)\n</code></pre></p> <p> <code>cast</code> <pre><code>let df = df! (\n        \"integers\"=&gt; &amp;[-1, 0, 2, 3, 4],\n        \"floats\"=&gt; &amp;[0.0, 1.0, 2.0, 3.0, 4.0],\n        \"bools\"=&gt; &amp;[true, false, true, false, true],\n)?;\n\nlet out = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"integers\").cast(DataType::Boolean),\n        col(\"floats\").cast(DataType::Boolean),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers \u2506 floats \u2502\n\u2502 ---      \u2506 ---    \u2502\n\u2502 bool     \u2506 bool   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 true     \u2506 false  \u2502\n\u2502 false    \u2506 true   \u2502\n\u2502 true     \u2506 true   \u2502\n\u2502 true     \u2506 true   \u2502\n\u2502 true     \u2506 true   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/casting/#dates","title":"Dates","text":"<p>Temporal data types such as <code>Date</code> or <code>Datetime</code> are represented as the number of days (<code>Date</code>) and microseconds (<code>Datetime</code>) since epoch. Therefore, casting between the numerical types and the temporal data types is allowed.</p>  Python Rust <p> <code>cast</code> <pre><code>from datetime import date, datetime\n\ndf = pl.DataFrame(\n    {\n        \"date\": pl.date_range(date(2022, 1, 1), date(2022, 1, 5), eager=True),\n        \"datetime\": pl.datetime_range(\n            datetime(2022, 1, 1), datetime(2022, 1, 5), eager=True\n        ),\n    }\n)\n\nout = df.select(pl.col(\"date\").cast(pl.Int64), pl.col(\"datetime\").cast(pl.Int64))\nprint(out)\n</code></pre></p> <p> <code>cast</code> <pre><code>use chrono::prelude::*;\n\nlet date = polars::time::date_range(\n    \"date\",\n    NaiveDate::from_ymd_opt(2022, 1, 1)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    NaiveDate::from_ymd_opt(2022, 1, 5)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    Duration::parse(\"1d\"),\n    ClosedWindow::Both,\n    TimeUnit::Milliseconds,\n    None,\n)?\n.cast(&amp;DataType::Date)?;\n\nlet datetime = polars::time::date_range(\n    \"datetime\",\n    NaiveDate::from_ymd_opt(2022, 1, 1)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    NaiveDate::from_ymd_opt(2022, 1, 5)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    Duration::parse(\"1d\"),\n    ClosedWindow::Both,\n    TimeUnit::Milliseconds,\n    None,\n)?;\n\nlet df = df! (\n    \"date\" =&gt; date,\n    \"datetime\" =&gt; datetime,\n)?;\n\nlet out = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"date\").cast(DataType::Int64),\n        col(\"datetime\").cast(DataType::Int64),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date  \u2506 datetime         \u2502\n\u2502 ---   \u2506 ---              \u2502\n\u2502 i64   \u2506 i64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 18993 \u2506 1640995200000000 \u2502\n\u2502 18994 \u2506 1641081600000000 \u2502\n\u2502 18995 \u2506 1641168000000000 \u2502\n\u2502 18996 \u2506 1641254400000000 \u2502\n\u2502 18997 \u2506 1641340800000000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To convert between strings and <code>Dates</code>/<code>Datetimes</code>, <code>dt.to_string</code> and <code>str.to_datetime</code> are utilized. Polars adopts the chrono format syntax for formatting. It's worth noting that <code>str.to_datetime</code> features additional options that support timezone functionality. Refer to the API documentation for further information.</p>  Python Rust <p> <code>dt.to_string</code> \u00b7 <code>str.to_date</code> <pre><code>df = pl.DataFrame(\n    {\n        \"date\": pl.date_range(date(2022, 1, 1), date(2022, 1, 5), eager=True),\n        \"string\": [\n            \"2022-01-01\",\n            \"2022-01-02\",\n            \"2022-01-03\",\n            \"2022-01-04\",\n            \"2022-01-05\",\n        ],\n    }\n)\n\nout = df.select(\n    pl.col(\"date\").dt.to_string(\"%Y-%m-%d\"),\n    pl.col(\"string\").str.to_datetime(\"%Y-%m-%d\"),\n)\nprint(out)\n</code></pre></p> <p> <code>dt.to_string</code> \u00b7 <code>str.replace_all</code> \u00b7  Available on feature dtype-date \u00b7  Available on feature temporal <pre><code>let date = polars::time::date_range(\n    \"date\",\n    NaiveDate::from_ymd_opt(2022, 1, 1)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    NaiveDate::from_ymd_opt(2022, 1, 5)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    Duration::parse(\"1d\"),\n    ClosedWindow::Both,\n    TimeUnit::Milliseconds,\n    None,\n)?;\n\nlet df = df! (\n        \"date\" =&gt; date,\n        \"string\" =&gt; &amp;[\n            \"2022-01-01\",\n            \"2022-01-02\",\n            \"2022-01-03\",\n            \"2022-01-04\",\n            \"2022-01-05\",\n        ],\n)?;\n\nlet out = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"date\").dt().to_string(\"%Y-%m-%d\"),\n        col(\"string\").str().to_datetime(\n            Some(TimeUnit::Microseconds),\n            None,\n            StrptimeOptions::default(),\n            lit(\"raise\"),\n        ),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date       \u2506 string              \u2502\n\u2502 ---        \u2506 ---                 \u2502\n\u2502 str        \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2022-01-01 \u2506 2022-01-01 00:00:00 \u2502\n\u2502 2022-01-02 \u2506 2022-01-02 00:00:00 \u2502\n\u2502 2022-01-03 \u2506 2022-01-03 00:00:00 \u2502\n\u2502 2022-01-04 \u2506 2022-01-04 00:00:00 \u2502\n\u2502 2022-01-05 \u2506 2022-01-05 00:00:00 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/column-selections/","title":"Column selections","text":"<p>Let's create a dataset to use in this section:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>from datetime import date, datetime\n\nimport polars as pl\n\ndf = pl.DataFrame(\n    {\n        \"id\": [9, 4, 2],\n        \"place\": [\"Mars\", \"Earth\", \"Saturn\"],\n        \"date\": pl.date_range(date(2022, 1, 1), date(2022, 1, 3), \"1d\", eager=True),\n        \"sales\": [33.4, 2142134.1, 44.7],\n        \"has_people\": [False, True, False],\n        \"logged_at\": pl.datetime_range(\n            datetime(2022, 12, 1), datetime(2022, 12, 1, 0, 0, 2), \"1s\", eager=True\n        ),\n    }\n).with_row_index(\"index\")\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>    use chrono::prelude::*;\n    use polars::time::*;\n\n    let df = df!(\n            \"id\" =&gt; &amp;[9, 4, 2],\n            \"place\" =&gt; &amp;[\"Mars\", \"Earth\", \"Saturn\"],\n        \"date\" =&gt; date_range(\"date\",\n                NaiveDate::from_ymd_opt(2022, 1, 1).unwrap().and_hms_opt(0, 0, 0).unwrap(), NaiveDate::from_ymd_opt(2022, 1, 3).unwrap().and_hms_opt(0, 0, 0).unwrap(), Duration::parse(\"1d\"),ClosedWindow::Both, TimeUnit::Milliseconds, None)?,\n            \"sales\" =&gt; &amp;[33.4, 2142134.1, 44.7],\n            \"has_people\" =&gt; &amp;[false, true, false],\n            \"logged_at\" =&gt; date_range(\"logged_at\",\n                NaiveDate::from_ymd_opt(2022, 1, 1).unwrap().and_hms_opt(0, 0, 0).unwrap(), NaiveDate::from_ymd_opt(2022, 1, 1).unwrap().and_hms_opt(0, 0, 2).unwrap(), Duration::parse(\"1s\"),ClosedWindow::Both, TimeUnit::Milliseconds, None)?,\n    )?\n    .with_row_index(\"index\", None)?;\n    println!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (3, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 index \u2506 id  \u2506 place  \u2506 date       \u2506 sales     \u2506 has_people \u2506 logged_at           \u2502\n\u2502 ---   \u2506 --- \u2506 ---    \u2506 ---        \u2506 ---       \u2506 ---        \u2506 ---                 \u2502\n\u2502 u32   \u2506 i64 \u2506 str    \u2506 date       \u2506 f64       \u2506 bool       \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0     \u2506 9   \u2506 Mars   \u2506 2022-01-01 \u2506 33.4      \u2506 false      \u2506 2022-12-01 00:00:00 \u2502\n\u2502 1     \u2506 4   \u2506 Earth  \u2506 2022-01-02 \u2506 2142134.1 \u2506 true       \u2506 2022-12-01 00:00:01 \u2502\n\u2502 2     \u2506 2   \u2506 Saturn \u2506 2022-01-03 \u2506 44.7      \u2506 false      \u2506 2022-12-01 00:00:02 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/column-selections/#expression-expansion","title":"Expression expansion","text":"<p>As we've seen in the previous section, we can select specific columns using the <code>pl.col</code> method. It can also select multiple columns - both as a means of convenience, and to expand the expression.</p> <p>This kind of convenience feature isn't just decorative or syntactic sugar. It allows for a very powerful application of DRY principles in your code: a single expression that specifies multiple columns expands into a list of expressions (depending on the DataFrame schema), resulting in being able to select multiple columns + run computation on them!</p>"},{"location":"user-guide/expressions/column-selections/#select-all-or-all-but-some","title":"Select all, or all but some","text":"<p>We can select all columns in the <code>DataFrame</code> object by providing the argument <code>*</code>:</p>  Python Rust <p> <code>all</code> <pre><code>out = df.select(pl.col(\"*\"))\n\n# Is equivalent to\nout = df.select(pl.all())\nprint(out)\n</code></pre></p> <p> <code>all</code> <pre><code>let out = df.clone().lazy().select([col(\"*\")]).collect()?;\nprintln!(\"{}\", &amp;out);\n\n// Is equivalent to\nlet out = df.clone().lazy().select([all()]).collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (3, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 index \u2506 id  \u2506 place  \u2506 date       \u2506 sales     \u2506 has_people \u2506 logged_at           \u2502\n\u2502 ---   \u2506 --- \u2506 ---    \u2506 ---        \u2506 ---       \u2506 ---        \u2506 ---                 \u2502\n\u2502 u32   \u2506 i64 \u2506 str    \u2506 date       \u2506 f64       \u2506 bool       \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0     \u2506 9   \u2506 Mars   \u2506 2022-01-01 \u2506 33.4      \u2506 false      \u2506 2022-12-01 00:00:00 \u2502\n\u2502 1     \u2506 4   \u2506 Earth  \u2506 2022-01-02 \u2506 2142134.1 \u2506 true       \u2506 2022-12-01 00:00:01 \u2502\n\u2502 2     \u2506 2   \u2506 Saturn \u2506 2022-01-03 \u2506 44.7      \u2506 false      \u2506 2022-12-01 00:00:02 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Often, we don't just want to include all columns, but include all while excluding a few. This can be done easily as well:</p>  Python Rust <p> <code>exclude</code> <pre><code>out = df.select(pl.col(\"*\").exclude(\"logged_at\", \"index\"))\nprint(out)\n</code></pre></p> <p> <code>exclude</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([col(\"*\").exclude([\"logged_at\", \"index\"])])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (3, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 place  \u2506 date       \u2506 sales     \u2506 has_people \u2502\n\u2502 --- \u2506 ---    \u2506 ---        \u2506 ---       \u2506 ---        \u2502\n\u2502 i64 \u2506 str    \u2506 date       \u2506 f64       \u2506 bool       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 9   \u2506 Mars   \u2506 2022-01-01 \u2506 33.4      \u2506 false      \u2502\n\u2502 4   \u2506 Earth  \u2506 2022-01-02 \u2506 2142134.1 \u2506 true       \u2502\n\u2502 2   \u2506 Saturn \u2506 2022-01-03 \u2506 44.7      \u2506 false      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/column-selections/#by-multiple-strings","title":"By multiple strings","text":"<p>Specifying multiple strings allows expressions to expand to all matching columns:</p>  Python Rust <p> <code>dt.to_string</code> <pre><code>out = df.select(pl.col(\"date\", \"logged_at\").dt.to_string(\"%Y-%h-%d\"))\nprint(out)\n</code></pre></p> <p> <code>dt.to_string</code> \u00b7  Available on feature temporal <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([cols([\"date\", \"logged_at\"]).dt().to_string(\"%Y-%h-%d\")])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date        \u2506 logged_at   \u2502\n\u2502 ---         \u2506 ---         \u2502\n\u2502 str         \u2506 str         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2022-Jan-01 \u2506 2022-Dec-01 \u2502\n\u2502 2022-Jan-02 \u2506 2022-Dec-01 \u2502\n\u2502 2022-Jan-03 \u2506 2022-Dec-01 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/column-selections/#by-regular-expressions","title":"By regular expressions","text":"<p>Multiple column selection is possible by regular expressions also, by making sure to wrap the regex by <code>^</code> and <code>$</code> to let <code>pl.col</code> know that a regex selection is expected:</p>  Python Rust <pre><code>out = df.select(pl.col(\"^.*(as|sa).*$\"))\nprint(out)\n</code></pre> <pre><code>let out = df.clone().lazy().select([col(\"^.*(as|sa).*$\")]).collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sales     \u2506 has_people \u2502\n\u2502 ---       \u2506 ---        \u2502\n\u2502 f64       \u2506 bool       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 33.4      \u2506 false      \u2502\n\u2502 2142134.1 \u2506 true       \u2502\n\u2502 44.7      \u2506 false      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/column-selections/#by-data-type","title":"By data type","text":"<p><code>pl.col</code> can select multiple columns using Polars data types:</p>  Python Rust <p> <code>n_unique</code> <pre><code>out = df.select(pl.col(pl.Int64, pl.UInt32, pl.Boolean).n_unique())\nprint(out)\n</code></pre></p> <p> <code>n_unique</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([dtype_cols([DataType::Int64, DataType::UInt32, DataType::Boolean]).n_unique()])\n    .collect()?;\n// gives different result than python as the id col is i32 in rust\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (1, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 index \u2506 id  \u2506 has_people \u2502\n\u2502 ---   \u2506 --- \u2506 ---        \u2502\n\u2502 u32   \u2506 u32 \u2506 u32        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3     \u2506 3   \u2506 2          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/column-selections/#using-selectors","title":"Using <code>selectors</code>","text":"<p>Polars also allows for the use of intuitive selections for columns based on their name, <code>dtype</code> or other properties; and this is built on top of existing functionality outlined in <code>col</code> used above. It is recommended to use them by importing and aliasing <code>polars.selectors</code> as <code>cs</code>.</p>"},{"location":"user-guide/expressions/column-selections/#by-dtype","title":"By <code>dtype</code>","text":"<p>To select just the integer and string columns, we can do:</p>  Python Rust <p> <code>selectors</code> <pre><code>import polars.selectors as cs\n\nout = df.select(cs.integer(), cs.string())\nprint(out)\n</code></pre></p> <p> <code>selectors</code> <pre><code>// Not available in Rust, refer the following link\n// https://github.com/pola-rs/polars/issues/10594\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 index \u2506 id  \u2506 place  \u2502\n\u2502 ---   \u2506 --- \u2506 ---    \u2502\n\u2502 u32   \u2506 i64 \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0     \u2506 9   \u2506 Mars   \u2502\n\u2502 1     \u2506 4   \u2506 Earth  \u2502\n\u2502 2     \u2506 2   \u2506 Saturn \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/column-selections/#applying-set-operations","title":"Applying set operations","text":"<p>These selectors also allow for set based selection operations. For instance, to select the numeric columns except the first column that indicates row numbers:</p>  Python Rust <p> <code>cs.first</code> \u00b7 <code>cs.numeric</code> <pre><code>out = df.select(cs.numeric() - cs.first())\nprint(out)\n</code></pre></p> <p> <code>cs.first</code> \u00b7 <code>cs.numeric</code> <pre><code>// Not available in Rust, refer the following link\n// https://github.com/pola-rs/polars/issues/10594\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 sales     \u2502\n\u2502 --- \u2506 ---       \u2502\n\u2502 i64 \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 9   \u2506 33.4      \u2502\n\u2502 4   \u2506 2142134.1 \u2502\n\u2502 2   \u2506 44.7      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can also select the row number by name and any non-numeric columns:</p>  Python Rust <p> <code>cs.by_name</code> \u00b7 <code>cs.numeric</code> <pre><code>out = df.select(cs.by_name(\"index\") | ~cs.numeric())\nprint(out)\n</code></pre></p> <p> <code>cs.by_name</code> \u00b7 <code>cs.numeric</code> <pre><code>// Not available in Rust, refer the following link\n// https://github.com/pola-rs/polars/issues/10594\n</code></pre></p> <pre><code>shape: (3, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 index \u2506 place  \u2506 date       \u2506 has_people \u2506 logged_at           \u2502\n\u2502 ---   \u2506 ---    \u2506 ---        \u2506 ---        \u2506 ---                 \u2502\n\u2502 u32   \u2506 str    \u2506 date       \u2506 bool       \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0     \u2506 Mars   \u2506 2022-01-01 \u2506 false      \u2506 2022-12-01 00:00:00 \u2502\n\u2502 1     \u2506 Earth  \u2506 2022-01-02 \u2506 true       \u2506 2022-12-01 00:00:01 \u2502\n\u2502 2     \u2506 Saturn \u2506 2022-01-03 \u2506 false      \u2506 2022-12-01 00:00:02 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/column-selections/#by-patterns-and-substrings","title":"By patterns and substrings","text":"<p>Selectors can also be matched by substring and regex patterns:</p>  Python Rust <p> <code>cs.contains</code> \u00b7 <code>cs.matches</code> <pre><code>out = df.select(cs.contains(\"index\"), cs.matches(\".*_.*\"))\nprint(out)\n</code></pre></p> <p> <code>cs.contains</code> \u00b7 <code>cs.matches</code> <pre><code>// Not available in Rust, refer the following link\n// https://github.com/pola-rs/polars/issues/1059\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 index \u2506 has_people \u2506 logged_at           \u2502\n\u2502 ---   \u2506 ---        \u2506 ---                 \u2502\n\u2502 u32   \u2506 bool       \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0     \u2506 false      \u2506 2022-12-01 00:00:00 \u2502\n\u2502 1     \u2506 true       \u2506 2022-12-01 00:00:01 \u2502\n\u2502 2     \u2506 false      \u2506 2022-12-01 00:00:02 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/column-selections/#converting-to-expressions","title":"Converting to expressions","text":"<p>What if we want to apply a specific operation on the selected columns (i.e. get back to representing them as expressions to operate upon)? We can simply convert them using <code>as_expr</code> and then proceed as normal:</p>  Python Rust <p> <code>cs.temporal</code> <pre><code>out = df.select(cs.temporal().as_expr().dt.to_string(\"%Y-%h-%d\"))\nprint(out)\n</code></pre></p> <p> <code>cs.temporal</code> <pre><code>// Not available in Rust, refer the following link\n// https://github.com/pola-rs/polars/issues/10594\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date        \u2506 logged_at   \u2502\n\u2502 ---         \u2506 ---         \u2502\n\u2502 str         \u2506 str         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2022-Jan-01 \u2506 2022-Dec-01 \u2502\n\u2502 2022-Jan-02 \u2506 2022-Dec-01 \u2502\n\u2502 2022-Jan-03 \u2506 2022-Dec-01 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/column-selections/#debugging-selectors","title":"Debugging <code>selectors</code>","text":"<p>Polars also provides two helpful utility functions to aid with using selectors: <code>is_selector</code> and <code>expand_selector</code>:</p>  Python Rust <p> <code>is_selector</code> <pre><code>from polars.selectors import is_selector\n\nout = cs.temporal()\nprint(is_selector(out))\n</code></pre></p> <p> <code>is_selector</code> <pre><code>// Not available in Rust, refer the following link\n// https://github.com/pola-rs/polars/issues/10594\n</code></pre></p> <pre><code>True\n</code></pre> <p>To predetermine the column names that are selected, which is especially useful for a LazyFrame object:</p>  Python Rust <p> <code>expand_selector</code> <pre><code>from polars.selectors import expand_selector\n\nout = cs.temporal().as_expr().dt.to_string(\"%Y-%h-%d\")\nprint(expand_selector(df, out))\n</code></pre></p> <p> <code>expand_selector</code> <pre><code>// Not available in Rust, refer the following link\n// https://github.com/pola-rs/polars/issues/10594\n</code></pre></p> <pre><code>('date', 'logged_at')\n</code></pre>"},{"location":"user-guide/expressions/folds/","title":"Folds","text":"<p>Polars provides expressions/methods for horizontal aggregations like <code>sum</code>,<code>min</code>, <code>mean</code>, etc. However, when you need a more complex aggregation the default methods Polars supplies may not be sufficient. That's when <code>folds</code> come in handy.</p> <p>The <code>fold</code> expression operates on columns for maximum speed. It utilizes the data layout very efficiently and often has vectorized execution.</p>"},{"location":"user-guide/expressions/folds/#manual-sum","title":"Manual sum","text":"<p>Let's start with an example by implementing the <code>sum</code> operation ourselves, with a <code>fold</code>.</p>  Python Rust <p> <code>fold</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": [1, 2, 3],\n        \"b\": [10, 20, 30],\n    }\n)\n\nout = df.select(\n    pl.fold(acc=pl.lit(0), function=lambda acc, x: acc + x, exprs=pl.all()).alias(\n        \"sum\"\n    ),\n)\nprint(out)\n</code></pre></p> <p> <code>fold_exprs</code> <pre><code>let df = df!(\n    \"a\" =&gt; &amp;[1, 2, 3],\n    \"b\" =&gt; &amp;[10, 20, 30],\n)?;\n\nlet out = df\n    .lazy()\n    .select([fold_exprs(lit(0), |acc, x| Ok(Some(acc + x)), [col(\"*\")]).alias(\"sum\")])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sum \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11  \u2502\n\u2502 22  \u2502\n\u2502 33  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The snippet above recursively applies the function <code>f(acc, x) -&gt; acc</code> to an accumulator <code>acc</code> and a new column <code>x</code>. The function operates on columns individually and can take advantage of cache efficiency and vectorization.</p>"},{"location":"user-guide/expressions/folds/#conditional","title":"Conditional","text":"<p>In the case where you'd want to apply a condition/predicate on all columns in a <code>DataFrame</code> a <code>fold</code> operation can be a very concise way to express this.</p>  Python Rust <p> <code>fold</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": [1, 2, 3],\n        \"b\": [0, 1, 2],\n    }\n)\n\nout = df.filter(\n    pl.fold(\n        acc=pl.lit(True),\n        function=lambda acc, x: acc &amp; x,\n        exprs=pl.col(\"*\") &gt; 1,\n    )\n)\nprint(out)\n</code></pre></p> <p> <code>fold_exprs</code> <pre><code>let df = df!(\n    \"a\" =&gt; &amp;[1, 2, 3],\n    \"b\" =&gt; &amp;[0, 1, 2],\n)?;\n\nlet out = df\n    .lazy()\n    .filter(fold_exprs(\n        lit(true),\n        |acc, x| acc.bitand(&amp;x).map(Some),\n        [col(\"*\").gt(1)],\n    ))\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In the snippet we filter all rows where each column value is <code>&gt; 1</code>.</p>"},{"location":"user-guide/expressions/folds/#folds-and-string-data","title":"Folds and string data","text":"<p>Folds could be used to concatenate string data. However, due to the materialization of intermediate columns, this operation will have squared complexity.</p> <p>Therefore, we recommend using the <code>concat_str</code> expression for this.</p>  Python Rust <p> <code>concat_str</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": [\"a\", \"b\", \"c\"],\n        \"b\": [1, 2, 3],\n    }\n)\n\nout = df.select(pl.concat_str([\"a\", \"b\"]))\nprint(out)\n</code></pre></p> <p> <code>concat_str</code> \u00b7  Available on feature concat_str <pre><code>let df = df!(\n    \"a\" =&gt; &amp;[\"a\", \"b\", \"c\"],\n    \"b\" =&gt; &amp;[1, 2, 3],\n)?;\n\nlet out = df\n    .lazy()\n    .select([concat_str([col(\"a\"), col(\"b\")], \"\", false)])\n    .collect()?;\nprintln!(\"{:?}\", out);\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a1  \u2502\n\u2502 b2  \u2502\n\u2502 c3  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/functions/","title":"Functions","text":"<p>Polars expressions have a large number of built in functions. These allow you to create complex queries without the need for user defined functions. There are too many to go through here, but we will cover some of the more popular use cases. If you want to view all the functions go to the API Reference for your programming language.</p> <p>In the examples below we will use the following <code>DataFrame</code>:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", \"spam\"],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use rand::{thread_rng, Rng};\n\nlet mut arr = [0f64; 5];\nthread_rng().fill(&amp;mut arr);\n\nlet df = df! (\n    \"nrs\" =&gt; &amp;[Some(1), Some(2), Some(3), None, Some(5)],\n    \"names\" =&gt; &amp;[\"foo\", \"ham\", \"spam\", \"egg\", \"spam\"],\n    \"random\" =&gt; &amp;arr,\n    \"groups\" =&gt; &amp;[\"A\", \"A\", \"B\", \"C\", \"B\"],\n)?;\n\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2506 A      \u2502\n\u2502 2    \u2506 ham   \u2506 0.74005  \u2506 A      \u2502\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2506 B      \u2502\n\u2502 null \u2506 egg   \u2506 0.533739 \u2506 C      \u2502\n\u2502 5    \u2506 spam  \u2506 0.014575 \u2506 B      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/functions/#column-naming","title":"Column naming","text":"<p>By default if you perform an expression it will keep the same name as the original column. In the example below we perform an expression on the <code>nrs</code> column. Note that the output <code>DataFrame</code> still has the same name.</p>  Python Rust <pre><code>df_samename = df.select(pl.col(\"nrs\") + 5)\nprint(df_samename)\n</code></pre> <pre><code>let df_samename = df.clone().lazy().select([col(\"nrs\") + lit(5)]).collect()?;\nprintln!(\"{}\", &amp;df_samename);\n</code></pre> <pre><code>shape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2502\n\u2502 ---  \u2502\n\u2502 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6    \u2502\n\u2502 7    \u2502\n\u2502 8    \u2502\n\u2502 null \u2502\n\u2502 10   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This might get problematic in the case you use the same column multiple times in your expression as the output columns will get duplicated. For example, the following query will fail.</p>  Python Rust <pre><code>try:\n    df_samename2 = df.select(pl.col(\"nrs\") + 5, pl.col(\"nrs\") - 5)\n    print(df_samename2)\nexcept Exception as e:\n    print(e)\n</code></pre> <pre><code>let df_samename2 = df\n    .clone()\n    .lazy()\n    .select([col(\"nrs\") + lit(5), col(\"nrs\") - lit(5)])\n    .collect();\nmatch df_samename2 {\n    Ok(df) =&gt; println!(\"{}\", &amp;df),\n    Err(e) =&gt; println!(\"{:?}\", &amp;e),\n};\n</code></pre> <pre><code>the name: 'nrs' is duplicate\n\nIt's possible that multiple expressions are returning the same default column name. If this is the case, try renaming the columns with `.alias(\"new_name\")` to avoid duplicate column names.\n</code></pre> <p>You can change the output name of an expression by using the <code>alias</code> function</p>  Python Rust <p> <code>alias</code> <pre><code>df_alias = df.select(\n    (pl.col(\"nrs\") + 5).alias(\"nrs + 5\"),\n    (pl.col(\"nrs\") - 5).alias(\"nrs - 5\"),\n)\nprint(df_alias)\n</code></pre></p> <p> <code>alias</code> <pre><code>let df_alias = df\n    .clone()\n    .lazy()\n    .select([\n        (col(\"nrs\") + lit(5)).alias(\"nrs + 5\"),\n        (col(\"nrs\") - lit(5)).alias(\"nrs - 5\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;df_alias);\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs + 5 \u2506 nrs - 5 \u2502\n\u2502 ---     \u2506 ---     \u2502\n\u2502 i64     \u2506 i64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6       \u2506 -4      \u2502\n\u2502 7       \u2506 -3      \u2502\n\u2502 8       \u2506 -2      \u2502\n\u2502 null    \u2506 null    \u2502\n\u2502 10      \u2506 0       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In case of multiple columns for example when using <code>all()</code> or <code>col(*)</code> you can apply a mapping function <code>name.map</code> to change the original column name into something else. In case you want to add a suffix (<code>name.suffix()</code>) or prefix (<code>name.prefix()</code>) these are also built in.</p>  Python <p> <code>name.prefix</code> <code>name.suffix</code> <code>name.map</code></p>"},{"location":"user-guide/expressions/functions/#count-unique-values","title":"Count unique values","text":"<p>There are two ways to count unique values in Polars: an exact methodology and an approximation. The approximation uses the HyperLogLog++ algorithm to approximate the cardinality and is especially useful for very large datasets where an approximation is good enough.</p>  Python Rust <p> <code>n_unique</code> \u00b7 <code>approx_n_unique</code> <pre><code>df_alias = df.select(\n    pl.col(\"names\").n_unique().alias(\"unique\"),\n    pl.approx_n_unique(\"names\").alias(\"unique_approx\"),\n)\nprint(df_alias)\n</code></pre></p> <p> <code>n_unique</code> \u00b7 <code>approx_n_unique</code> <pre><code>let df_alias = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"names\").n_unique().alias(\"unique\"),\n        // Following query shows there isn't anything in Rust API\n        // https://docs.rs/polars/latest/polars/?search=approx_n_unique\n        // col(\"names\").approx_n_unique().alias(\"unique_approx\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;df_alias);\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 unique \u2506 unique_approx \u2502\n\u2502 ---    \u2506 ---           \u2502\n\u2502 u32    \u2506 u32           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 4      \u2506 4             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/functions/#conditionals","title":"Conditionals","text":"<p>Polars supports if-else like conditions in expressions with the <code>when</code>, <code>then</code>, <code>otherwise</code> syntax. The predicate is placed in the <code>when</code> clause and when this evaluates to <code>true</code> the <code>then</code> expression is applied otherwise the <code>otherwise</code> expression is applied (row-wise).</p>  Python Rust <p> <code>when</code> <pre><code>df_conditional = df.select(\n    pl.col(\"nrs\"),\n    pl.when(pl.col(\"nrs\") &gt; 2)\n    .then(pl.lit(True))\n    .otherwise(pl.lit(False))\n    .alias(\"conditional\"),\n)\nprint(df_conditional)\n</code></pre></p> <p> <code>when</code> <pre><code>let df_conditional = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"nrs\"),\n        when(col(\"nrs\").gt(2))\n            .then(lit(true))\n            .otherwise(lit(false))\n            .alias(\"conditional\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;df_conditional);\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 conditional \u2502\n\u2502 ---  \u2506 ---         \u2502\n\u2502 i64  \u2506 bool        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 false       \u2502\n\u2502 2    \u2506 false       \u2502\n\u2502 3    \u2506 true        \u2502\n\u2502 null \u2506 false       \u2502\n\u2502 5    \u2506 true        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/lists/","title":"Lists and Arrays","text":"<p>Polars has first-class support for <code>List</code> columns: that is, columns where each row is a list of homogeneous elements, of varying lengths. Polars also has an <code>Array</code> datatype, which is analogous to NumPy's <code>ndarray</code> objects, where the length is identical across rows.</p> <p>Note: this is different from Python's <code>list</code> object, where the elements can be of any type. Polars can store these within columns, but as a generic <code>Object</code> datatype that doesn't have the special list manipulation features that we're about to discuss.</p>"},{"location":"user-guide/expressions/lists/#powerful-list-manipulation","title":"Powerful <code>List</code> manipulation","text":"<p>Let's say we had the following data from different weather stations across a state. When the weather station is unable to get a result, an error code is recorded instead of the actual temperature at that time.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>weather = pl.DataFrame(\n    {\n        \"station\": [\"Station \" + str(x) for x in range(1, 6)],\n        \"temperatures\": [\n            \"20 5 5 E1 7 13 19 9 6 20\",\n            \"18 8 16 11 23 E2 8 E2 E2 E2 90 70 40\",\n            \"19 24 E9 16 6 12 10 22\",\n            \"E2 E0 15 7 8 10 E1 24 17 13 6\",\n            \"14 8 E0 16 22 24 E1\",\n        ],\n    }\n)\nprint(weather)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let stns: Vec&lt;String&gt; = (1..6).map(|i| format!(\"Station {i}\")).collect();\nlet weather = df!(\n        \"station\"=&gt; &amp;stns,\n        \"temperatures\"=&gt; &amp;[\n            \"20 5 5 E1 7 13 19 9 6 20\",\n            \"18 8 16 11 23 E2 8 E2 E2 E2 90 70 40\",\n            \"19 24 E9 16 6 12 10 22\",\n            \"E2 E0 15 7 8 10 E1 24 17 13 6\",\n            \"14 8 E0 16 22 24 E1\",\n        ],\n)?;\nprintln!(\"{}\", &amp;weather);\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures                    \u2502\n\u2502 ---       \u2506 ---                             \u2502\n\u2502 str       \u2506 str                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 20 5 5 E1 7 13 19 9 6 20        \u2502\n\u2502 Station 2 \u2506 18 8 16 11 23 E2 8 E2 E2 E2 90\u2026 \u2502\n\u2502 Station 3 \u2506 19 24 E9 16 6 12 10 22          \u2502\n\u2502 Station 4 \u2506 E2 E0 15 7 8 10 E1 24 17 13 6   \u2502\n\u2502 Station 5 \u2506 14 8 E0 16 22 24 E1             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/lists/#creating-a-list-column","title":"Creating a <code>List</code> column","text":"<p>For the <code>weather</code> <code>DataFrame</code> created above, it's very likely we need to run some analysis on the temperatures that are captured by each station. To make this happen, we need to first be able to get individual temperature measurements. This is done by:</p>  Python Rust <p> <code>str.split</code> <pre><code>out = weather.with_columns(pl.col(\"temperatures\").str.split(\" \"))\nprint(out)\n</code></pre></p> <p> <code>str.split</code> <pre><code>let out = weather\n    .clone()\n    .lazy()\n    .with_columns([col(\"temperatures\").str().split(lit(\" \"))])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures         \u2502\n\u2502 ---       \u2506 ---                  \u2502\n\u2502 str       \u2506 list[str]            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 [\"20\", \"5\", \u2026 \"20\"]  \u2502\n\u2502 Station 2 \u2506 [\"18\", \"8\", \u2026 \"40\"]  \u2502\n\u2502 Station 3 \u2506 [\"19\", \"24\", \u2026 \"22\"] \u2502\n\u2502 Station 4 \u2506 [\"E2\", \"E0\", \u2026 \"6\"]  \u2502\n\u2502 Station 5 \u2506 [\"14\", \"8\", \u2026 \"E1\"]  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>One way we could go post this would be to convert each temperature measurement into its own row:</p>  Python Rust <p> <code>DataFrame.explode</code> <pre><code>out = weather.with_columns(pl.col(\"temperatures\").str.split(\" \")).explode(\n    \"temperatures\"\n)\nprint(out)\n</code></pre></p> <p> <code>DataFrame.explode</code> <pre><code>let out = weather\n    .clone()\n    .lazy()\n    .with_columns([col(\"temperatures\").str().split(lit(\" \"))])\n    .explode([\"temperatures\"])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (49, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures \u2502\n\u2502 ---       \u2506 ---          \u2502\n\u2502 str       \u2506 str          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 20           \u2502\n\u2502 Station 1 \u2506 5            \u2502\n\u2502 Station 1 \u2506 5            \u2502\n\u2502 Station 1 \u2506 E1           \u2502\n\u2502 Station 1 \u2506 7            \u2502\n\u2502 \u2026         \u2506 \u2026            \u2502\n\u2502 Station 5 \u2506 E0           \u2502\n\u2502 Station 5 \u2506 16           \u2502\n\u2502 Station 5 \u2506 22           \u2502\n\u2502 Station 5 \u2506 24           \u2502\n\u2502 Station 5 \u2506 E1           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>However, in Polars, we often do not need to do this to operate on the <code>List</code> elements.</p>"},{"location":"user-guide/expressions/lists/#operating-on-list-columns","title":"Operating on <code>List</code> columns","text":"<p>Polars provides several standard operations on <code>List</code> columns. If we want the first three measurements, we can do a <code>head(3)</code>. The last three can be obtained via a <code>tail(3)</code>, or alternately, via <code>slice</code> (negative indexing is supported). We can also identify the number of observations via <code>lengths</code>. Let's see them in action:</p>  Python Rust <p> <code>Expr.list</code> <pre><code>out = weather.with_columns(pl.col(\"temperatures\").str.split(\" \")).with_columns(\n    pl.col(\"temperatures\").list.head(3).alias(\"top3\"),\n    pl.col(\"temperatures\").list.slice(-3, 3).alias(\"bottom_3\"),\n    pl.col(\"temperatures\").list.len().alias(\"obs\"),\n)\nprint(out)\n</code></pre></p> <p> <code>Expr.list</code> <pre><code>let out = weather\n    .clone()\n    .lazy()\n    .with_columns([col(\"temperatures\").str().split(lit(\" \"))])\n    .with_columns([\n        col(\"temperatures\").list().head(lit(3)).alias(\"top3\"),\n        col(\"temperatures\")\n            .list()\n            .slice(lit(-3), lit(3))\n            .alias(\"bottom_3\"),\n        col(\"temperatures\").list().len().alias(\"obs\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures         \u2506 top3               \u2506 bottom_3           \u2506 obs \u2502\n\u2502 ---       \u2506 ---                  \u2506 ---                \u2506 ---                \u2506 --- \u2502\n\u2502 str       \u2506 list[str]            \u2506 list[str]          \u2506 list[str]          \u2506 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 [\"20\", \"5\", \u2026 \"20\"]  \u2506 [\"20\", \"5\", \"5\"]   \u2506 [\"9\", \"6\", \"20\"]   \u2506 10  \u2502\n\u2502 Station 2 \u2506 [\"18\", \"8\", \u2026 \"40\"]  \u2506 [\"18\", \"8\", \"16\"]  \u2506 [\"90\", \"70\", \"40\"] \u2506 13  \u2502\n\u2502 Station 3 \u2506 [\"19\", \"24\", \u2026 \"22\"] \u2506 [\"19\", \"24\", \"E9\"] \u2506 [\"12\", \"10\", \"22\"] \u2506 8   \u2502\n\u2502 Station 4 \u2506 [\"E2\", \"E0\", \u2026 \"6\"]  \u2506 [\"E2\", \"E0\", \"15\"] \u2506 [\"17\", \"13\", \"6\"]  \u2506 11  \u2502\n\u2502 Station 5 \u2506 [\"14\", \"8\", \u2026 \"E1\"]  \u2506 [\"14\", \"8\", \"E0\"]  \u2506 [\"22\", \"24\", \"E1\"] \u2506 7   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p><code>arr</code> then, <code>list</code> now</p> <p>If you find references to the <code>arr</code> API on Stackoverflow or other sources, just replace <code>arr</code> with <code>list</code>, this was the old accessor for the <code>List</code> datatype. <code>arr</code> now refers to the newly introduced <code>Array</code> datatype (see below).</p>"},{"location":"user-guide/expressions/lists/#element-wise-computation-within-lists","title":"Element-wise computation within <code>List</code>s","text":"<p>If we need to identify the stations that are giving the most number of errors from the starting <code>DataFrame</code>, we need to:</p> <ol> <li>Parse the string input as a <code>List</code> of string values (already done).</li> <li>Identify those strings that can be converted to numbers.</li> <li>Identify the number of non-numeric values (i.e. <code>null</code> values) in the list, by row.</li> <li>Rename this output as <code>errors</code> so that we can easily identify the stations.</li> </ol> <p>The third step requires a casting (or alternately, a regex pattern search) operation to be perform on each element of the list. We can do this using by applying the operation on each element by first referencing them in the <code>pl.element()</code> context, and then calling a suitable Polars expression on them. Let's see how:</p>  Python Rust <p> <code>Expr.list</code> \u00b7 <code>element</code> <pre><code>out = weather.with_columns(\n    pl.col(\"temperatures\")\n    .str.split(\" \")\n    .list.eval(pl.element().cast(pl.Int64, strict=False).is_null())\n    .list.sum()\n    .alias(\"errors\")\n)\nprint(out)\n</code></pre></p> <p> <code>Expr.list</code> \u00b7 <code>element</code> <pre><code>let out = weather\n    .clone()\n    .lazy()\n    .with_columns([col(\"temperatures\")\n        .str()\n        .split(lit(\" \"))\n        .list()\n        .eval(col(\"\").cast(DataType::Int64).is_null(), false)\n        .list()\n        .sum()\n        .alias(\"errors\")])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures                    \u2506 errors \u2502\n\u2502 ---       \u2506 ---                             \u2506 ---    \u2502\n\u2502 str       \u2506 str                             \u2506 u32    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 20 5 5 E1 7 13 19 9 6 20        \u2506 1      \u2502\n\u2502 Station 2 \u2506 18 8 16 11 23 E2 8 E2 E2 E2 90\u2026 \u2506 4      \u2502\n\u2502 Station 3 \u2506 19 24 E9 16 6 12 10 22          \u2506 1      \u2502\n\u2502 Station 4 \u2506 E2 E0 15 7 8 10 E1 24 17 13 6   \u2506 3      \u2502\n\u2502 Station 5 \u2506 14 8 E0 16 22 24 E1             \u2506 2      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>What if we chose the regex route (i.e. recognizing the presence of any alphabetical character?)</p>  Python Rust <p> <code>str.contains</code> <pre><code>out = weather.with_columns(\n    pl.col(\"temperatures\")\n    .str.split(\" \")\n    .list.eval(pl.element().str.contains(\"(?i)[a-z]\"))\n    .list.sum()\n    .alias(\"errors\")\n)\nprint(out)\n</code></pre></p> <p> <code>str.contains</code> \u00b7  Available on feature regex <pre><code>let out = weather\n    .clone()\n    .lazy()\n    .with_columns([col(\"temperatures\")\n        .str()\n        .split(lit(\" \"))\n        .list()\n        .eval(col(\"\").str().contains(lit(\"(?i)[a-z]\"), false), false)\n        .list()\n        .sum()\n        .alias(\"errors\")])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station   \u2506 temperatures                    \u2506 errors \u2502\n\u2502 ---       \u2506 ---                             \u2506 ---    \u2502\n\u2502 str       \u2506 str                             \u2506 u32    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1 \u2506 20 5 5 E1 7 13 19 9 6 20        \u2506 1      \u2502\n\u2502 Station 2 \u2506 18 8 16 11 23 E2 8 E2 E2 E2 90\u2026 \u2506 4      \u2502\n\u2502 Station 3 \u2506 19 24 E9 16 6 12 10 22          \u2506 1      \u2502\n\u2502 Station 4 \u2506 E2 E0 15 7 8 10 E1 24 17 13 6   \u2506 3      \u2502\n\u2502 Station 5 \u2506 14 8 E0 16 22 24 E1             \u2506 2      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If you're unfamiliar with the <code>(?i)</code>, it's a good time to look at the documentation for the <code>str.contains</code> function in Polars! The Rust regex crate provides a lot of additional regex flags that might come in handy.</p>"},{"location":"user-guide/expressions/lists/#row-wise-computations","title":"Row-wise computations","text":"<p>This context is ideal for computing in row orientation.</p> <p>We can apply any Polars operations on the elements of the list with the <code>list.eval</code> (<code>list().eval</code> in Rust) expression! These expressions run entirely on Polars' query engine and can run in parallel, so will be well optimized. Let's say we have another set of weather data across three days, for different stations:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>weather_by_day = pl.DataFrame(\n    {\n        \"station\": [\"Station \" + str(x) for x in range(1, 11)],\n        \"day_1\": [17, 11, 8, 22, 9, 21, 20, 8, 8, 17],\n        \"day_2\": [15, 11, 10, 8, 7, 14, 18, 21, 15, 13],\n        \"day_3\": [16, 15, 24, 24, 8, 23, 19, 23, 16, 10],\n    }\n)\nprint(weather_by_day)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let stns: Vec&lt;String&gt; = (1..11).map(|i| format!(\"Station {i}\")).collect();\nlet weather_by_day = df!(\n        \"station\" =&gt; &amp;stns,\n        \"day_1\" =&gt; &amp;[17, 11, 8, 22, 9, 21, 20, 8, 8, 17],\n        \"day_2\" =&gt; &amp;[15, 11, 10, 8, 7, 14, 18, 21, 15, 13],\n        \"day_3\" =&gt; &amp;[16, 15, 24, 24, 8, 23, 19, 23, 16, 10],\n)?;\nprintln!(\"{}\", &amp;weather_by_day);\n</code></pre></p> <pre><code>shape: (10, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station    \u2506 day_1 \u2506 day_2 \u2506 day_3 \u2502\n\u2502 ---        \u2506 ---   \u2506 ---   \u2506 ---   \u2502\n\u2502 str        \u2506 i64   \u2506 i64   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1  \u2506 17    \u2506 15    \u2506 16    \u2502\n\u2502 Station 2  \u2506 11    \u2506 11    \u2506 15    \u2502\n\u2502 Station 3  \u2506 8     \u2506 10    \u2506 24    \u2502\n\u2502 Station 4  \u2506 22    \u2506 8     \u2506 24    \u2502\n\u2502 Station 5  \u2506 9     \u2506 7     \u2506 8     \u2502\n\u2502 Station 6  \u2506 21    \u2506 14    \u2506 23    \u2502\n\u2502 Station 7  \u2506 20    \u2506 18    \u2506 19    \u2502\n\u2502 Station 8  \u2506 8     \u2506 21    \u2506 23    \u2502\n\u2502 Station 9  \u2506 8     \u2506 15    \u2506 16    \u2502\n\u2502 Station 10 \u2506 17    \u2506 13    \u2506 10    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Let's do something interesting, where we calculate the percentage rank of the temperatures by day, measured across stations. Pandas allows you to compute the percentages of the <code>rank</code> values. Polars doesn't provide a special function to do this directly, but because expressions are so versatile we can create our own percentage rank expression for highest temperature. Let's try that!</p>  Python Rust <p> <code>list.eval</code> <pre><code>rank_pct = (pl.element().rank(descending=True) / pl.col(\"*\").count()).round(2)\n\nout = weather_by_day.with_columns(\n    # create the list of homogeneous data\n    pl.concat_list(pl.all().exclude(\"station\")).alias(\"all_temps\")\n).select(\n    # select all columns except the intermediate list\n    pl.all().exclude(\"all_temps\"),\n    # compute the rank by calling `list.eval`\n    pl.col(\"all_temps\").list.eval(rank_pct, parallel=True).alias(\"temps_rank\"),\n)\n\nprint(out)\n</code></pre></p> <p> <code>list.eval</code> \u00b7  Available on feature list_eval <pre><code>let rank_pct = (col(\"\")\n    .rank(\n        RankOptions {\n            method: RankMethod::Average,\n            descending: true,\n        },\n        None,\n    )\n    .cast(DataType::Float32)\n    / col(\"*\").count().cast(DataType::Float32))\n.round(2);\n\nlet out = weather_by_day\n    .clone()\n    .lazy()\n    .with_columns(\n        // create the list of homogeneous data\n        [concat_list([all().exclude([\"station\"])])?.alias(\"all_temps\")],\n    )\n    .select(\n        // select all columns except the intermediate list\n        [\n            all().exclude([\"all_temps\"]),\n            // compute the rank by calling `list.eval`\n            col(\"all_temps\")\n                .list()\n                .eval(rank_pct, true)\n                .alias(\"temps_rank\"),\n        ],\n    )\n    .collect()?;\n\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (10, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 station    \u2506 day_1 \u2506 day_2 \u2506 day_3 \u2506 temps_rank         \u2502\n\u2502 ---        \u2506 ---   \u2506 ---   \u2506 ---   \u2506 ---                \u2502\n\u2502 str        \u2506 i64   \u2506 i64   \u2506 i64   \u2506 list[f64]          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Station 1  \u2506 17    \u2506 15    \u2506 16    \u2506 [0.33, 1.0, 0.67]  \u2502\n\u2502 Station 2  \u2506 11    \u2506 11    \u2506 15    \u2506 [0.83, 0.83, 0.33] \u2502\n\u2502 Station 3  \u2506 8     \u2506 10    \u2506 24    \u2506 [1.0, 0.67, 0.33]  \u2502\n\u2502 Station 4  \u2506 22    \u2506 8     \u2506 24    \u2506 [0.67, 1.0, 0.33]  \u2502\n\u2502 Station 5  \u2506 9     \u2506 7     \u2506 8     \u2506 [0.33, 1.0, 0.67]  \u2502\n\u2502 Station 6  \u2506 21    \u2506 14    \u2506 23    \u2506 [0.67, 1.0, 0.33]  \u2502\n\u2502 Station 7  \u2506 20    \u2506 18    \u2506 19    \u2506 [0.33, 1.0, 0.67]  \u2502\n\u2502 Station 8  \u2506 8     \u2506 21    \u2506 23    \u2506 [1.0, 0.67, 0.33]  \u2502\n\u2502 Station 9  \u2506 8     \u2506 15    \u2506 16    \u2506 [1.0, 0.67, 0.33]  \u2502\n\u2502 Station 10 \u2506 17    \u2506 13    \u2506 10    \u2506 [0.33, 0.67, 1.0]  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/lists/#polars-arrays","title":"Polars <code>Array</code>s","text":"<p><code>Array</code>s are a new data type that was recently introduced, and are still pretty nascent in features that it offers. The major difference between a <code>List</code> and an <code>Array</code> is that the latter is limited to having the same number of elements per row, while a <code>List</code> can have a variable number of elements. Both still require that each element's data type is the same.</p> <p>We can define <code>Array</code> columns in this manner:</p>  Python Rust <p> <code>Array</code> <pre><code>array_df = pl.DataFrame(\n    [\n        pl.Series(\"Array_1\", [[1, 3], [2, 5]]),\n        pl.Series(\"Array_2\", [[1, 7, 3], [8, 1, 0]]),\n    ],\n    schema={\n        \"Array_1\": pl.Array(pl.Int64, 2),\n        \"Array_2\": pl.Array(pl.Int64, 3),\n    },\n)\nprint(array_df)\n</code></pre></p> <p> <code>Array</code> <pre><code>let mut col1: ListPrimitiveChunkedBuilder&lt;Int32Type&gt; =\n    ListPrimitiveChunkedBuilder::new(\"Array_1\", 8, 8, DataType::Int32);\ncol1.append_slice(&amp;[1, 3]);\ncol1.append_slice(&amp;[2, 5]);\nlet mut col2: ListPrimitiveChunkedBuilder&lt;Int32Type&gt; =\n    ListPrimitiveChunkedBuilder::new(\"Array_2\", 8, 8, DataType::Int32);\ncol2.append_slice(&amp;[1, 7, 3]);\ncol2.append_slice(&amp;[8, 1, 0]);\nlet array_df = DataFrame::new([col1.finish(), col2.finish()].into())?;\n\nprintln!(\"{}\", &amp;array_df);\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Array_1       \u2506 Array_2       \u2502\n\u2502 ---           \u2506 ---           \u2502\n\u2502 array[i64, 2] \u2506 array[i64, 3] \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [1, 3]        \u2506 [1, 7, 3]     \u2502\n\u2502 [2, 5]        \u2506 [8, 1, 0]     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Basic operations are available on it:</p>  Python Rust <p> <code>Series.arr</code> <pre><code>out = array_df.select(\n    pl.col(\"Array_1\").arr.min().name.suffix(\"_min\"),\n    pl.col(\"Array_2\").arr.sum().name.suffix(\"_sum\"),\n)\nprint(out)\n</code></pre></p> <p> <code>Series.arr</code> <pre><code>let out = array_df\n    .clone()\n    .lazy()\n    .select([\n        col(\"Array_1\").list().min().name().suffix(\"_min\"),\n        col(\"Array_2\").list().sum().name().suffix(\"_sum\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Array_1_min \u2506 Array_2_sum \u2502\n\u2502 ---         \u2506 ---         \u2502\n\u2502 i64         \u2506 i64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 11          \u2502\n\u2502 2           \u2506 9           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Polars <code>Array</code>s are still being actively developed, so this section will likely change in the future.</p>"},{"location":"user-guide/expressions/missing-data/","title":"Missing data","text":"<p>This page sets out how missing data is represented in Polars and how missing data can be filled.</p>"},{"location":"user-guide/expressions/missing-data/#null-and-nan-values","title":"<code>null</code> and <code>NaN</code> values","text":"<p>Each column in a <code>DataFrame</code> (or equivalently a <code>Series</code>) is an Arrow array or a collection of Arrow arrays based on the Apache Arrow format. Missing data is represented in Arrow and Polars with a <code>null</code> value. This <code>null</code> missing value applies for all data types including numerical values.</p> <p>Polars also allows <code>NotaNumber</code> or <code>NaN</code> values for float columns. These <code>NaN</code> values are considered to be a type of floating point data rather than missing data. We discuss <code>NaN</code> values separately below.</p> <p>You can manually define a missing value with the python <code>None</code> value:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"value\": [1, None],\n    },\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = df! (\n    \"value\" =&gt; &amp;[Some(1), None],\n)?;\n\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2502\n\u2502 null  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Info</p> <p>In pandas the value for missing data depends on the dtype of the column. In Polars missing data is always represented as a <code>null</code> value.</p>"},{"location":"user-guide/expressions/missing-data/#missing-data-metadata","title":"Missing data metadata","text":"<p>Each Arrow array used by Polars stores two kinds of metadata related to missing data. This metadata allows Polars to quickly show how many missing values there are and which values are missing.</p> <p>The first piece of metadata is the <code>null_count</code> - this is the number of rows with <code>null</code> values in the column:</p>  Python Rust <p> <code>null_count</code> <pre><code>null_count_df = df.null_count()\nprint(null_count_df)\n</code></pre></p> <p> <code>null_count</code> <pre><code>let null_count_df = df.null_count();\nprintln!(\"{}\", &amp;null_count_df);\n</code></pre></p> <pre><code>shape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The <code>null_count</code> method can be called on a <code>DataFrame</code>, a column from a <code>DataFrame</code> or a <code>Series</code>. The <code>null_count</code> method is a cheap operation as <code>null_count</code> is already calculated for the underlying Arrow array.</p> <p>The second piece of metadata is an array called a validity bitmap that indicates whether each data value is valid or missing. The validity bitmap is memory efficient as it is bit encoded - each value is either a 0 or a 1. This bit encoding means the memory overhead per array is only (array length / 8) bytes. The validity bitmap is used by the <code>is_null</code> method in Polars.</p> <p>You can return a <code>Series</code> based on the validity bitmap for a column in a <code>DataFrame</code> or a <code>Series</code> with the <code>is_null</code> method:</p>  Python Rust <p> <code>is_null</code> <pre><code>is_null_series = df.select(\n    pl.col(\"value\").is_null(),\n)\nprint(is_null_series)\n</code></pre></p> <p> <code>is_null</code> <pre><code>let is_null_series = df\n    .clone()\n    .lazy()\n    .select([col(\"value\").is_null()])\n    .collect()?;\nprintln!(\"{}\", &amp;is_null_series);\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 bool  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 false \u2502\n\u2502 true  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The <code>is_null</code> method is a cheap operation that does not require scanning the full column for <code>null</code> values. This is because the validity bitmap already exists and can be returned as a Boolean array.</p>"},{"location":"user-guide/expressions/missing-data/#filling-missing-data","title":"Filling missing data","text":"<p>Missing data in a <code>Series</code> can be filled with the <code>fill_null</code> method. You have to specify how you want the <code>fill_null</code> method to fill the missing data. The main ways to do this are filling with:</p> <ul> <li>a literal such as 0 or \"0\"</li> <li>a strategy such as filling forwards</li> <li>an expression such as replacing with values from another column</li> <li>interpolation</li> </ul> <p>We illustrate each way to fill nulls by defining a simple <code>DataFrame</code> with a missing value in <code>col2</code>:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"col1\": [1, 2, 3],\n        \"col2\": [1, None, 3],\n    },\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = df!(\n        \"col1\" =&gt; &amp;[Some(1), Some(2), Some(3)],\n        \"col2\" =&gt; &amp;[Some(1), None, Some(3)],\n\n)?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 1    \u2502\n\u2502 2    \u2506 null \u2502\n\u2502 3    \u2506 3    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/missing-data/#fill-with-specified-literal-value","title":"Fill with specified literal value","text":"<p>We can fill the missing data with a specified literal value with <code>pl.lit</code>:</p>  Python Rust <p> <code>fill_null</code> <pre><code>fill_literal_df = df.with_columns(\n    pl.col(\"col2\").fill_null(pl.lit(2)),\n)\nprint(fill_literal_df)\n</code></pre></p> <p> <code>fill_null</code> <pre><code>let fill_literal_df = df\n    .clone()\n    .lazy()\n    .with_columns([col(\"col2\").fill_null(lit(2))])\n    .collect()?;\nprintln!(\"{}\", &amp;fill_literal_df);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 1    \u2502\n\u2502 2    \u2506 2    \u2502\n\u2502 3    \u2506 3    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/missing-data/#fill-with-a-strategy","title":"Fill with a strategy","text":"<p>We can fill the missing data with a strategy such as filling forward:</p>  Python Rust <p> <code>fill_null</code> <pre><code>fill_forward_df = df.with_columns(\n    pl.col(\"col2\").fill_null(strategy=\"forward\"),\n)\nprint(fill_forward_df)\n</code></pre></p> <p> <code>fill_null</code> <pre><code>let fill_forward_df = df\n    .clone()\n    .lazy()\n    .with_columns([col(\"col2\").forward_fill(None)])\n    .collect()?;\nprintln!(\"{}\", &amp;fill_forward_df);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 1    \u2502\n\u2502 2    \u2506 1    \u2502\n\u2502 3    \u2506 3    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can find other fill strategies in the API docs.</p>"},{"location":"user-guide/expressions/missing-data/#fill-with-an-expression","title":"Fill with an expression","text":"<p>For more flexibility we can fill the missing data with an expression. For example, to fill nulls with the median value from that column:</p>  Python Rust <p> <code>fill_null</code> <pre><code>fill_median_df = df.with_columns(\n    pl.col(\"col2\").fill_null(pl.median(\"col2\")),\n)\nprint(fill_median_df)\n</code></pre></p> <p> <code>fill_null</code> <pre><code>let fill_median_df = df\n    .clone()\n    .lazy()\n    .with_columns([col(\"col2\").fill_null(median(\"col2\"))])\n    .collect()?;\nprintln!(\"{}\", &amp;fill_median_df);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 f64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 1.0  \u2502\n\u2502 2    \u2506 2.0  \u2502\n\u2502 3    \u2506 3.0  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this case the column is cast from integer to float because the median is a float statistic.</p>"},{"location":"user-guide/expressions/missing-data/#fill-with-interpolation","title":"Fill with interpolation","text":"<p>In addition, we can fill nulls with interpolation (without using the <code>fill_null</code> function):</p>  Python Rust <p> <code>interpolate</code> <pre><code>fill_interpolation_df = df.with_columns(\n    pl.col(\"col2\").interpolate(),\n)\nprint(fill_interpolation_df)\n</code></pre></p> <p> <code>interpolate</code> <pre><code>let fill_interpolation_df = df\n    .clone()\n    .lazy()\n    .with_columns([col(\"col2\").interpolate(InterpolationMethod::Linear)])\n    .collect()?;\nprintln!(\"{}\", &amp;fill_interpolation_df);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 f64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 1.0  \u2502\n\u2502 2    \u2506 2.0  \u2502\n\u2502 3    \u2506 3.0  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/missing-data/#notanumber-or-nan-values","title":"<code>NotaNumber</code> or <code>NaN</code> values","text":"<p>Missing data in a <code>Series</code> has a <code>null</code> value. However, you can use <code>NotaNumber</code> or <code>NaN</code> values in columns with float datatypes. These <code>NaN</code> values can be created from Numpy's <code>np.nan</code> or the native python <code>float('nan')</code>:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>nan_df = pl.DataFrame(\n    {\n        \"value\": [1.0, np.nan, float(\"nan\"), 3.0],\n    },\n)\nprint(nan_df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let nan_df = df!(\n        \"value\" =&gt; [1.0, f64::NAN, f64::NAN, 3.0],\n)?;\nprintln!(\"{}\", &amp;nan_df);\n</code></pre></p> <pre><code>shape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.0   \u2502\n\u2502 NaN   \u2502\n\u2502 NaN   \u2502\n\u2502 3.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Info</p> <p>In pandas by default a <code>NaN</code> value in an integer column causes the column to be cast to float. This does not happen in Polars - instead an exception is raised.</p> <p><code>NaN</code> values are considered to be a type of floating point data and are not considered to be missing data in Polars. This means:</p> <ul> <li><code>NaN</code> values are not counted with the <code>null_count</code> method</li> <li><code>NaN</code> values are filled when you use <code>fill_nan</code> method but are not filled with the <code>fill_null</code> method</li> </ul> <p>Polars has <code>is_nan</code> and <code>fill_nan</code> methods which work in a similar way to the <code>is_null</code> and <code>fill_null</code> methods. The underlying Arrow arrays do not have a pre-computed validity bitmask for <code>NaN</code> values so this has to be computed for the <code>is_nan</code> method.</p> <p>One further difference between <code>null</code> and <code>NaN</code> values is that taking the <code>mean</code> of a column with <code>null</code> values excludes the <code>null</code> values from the calculation but with <code>NaN</code> values taking the mean results in a <code>NaN</code>. This behaviour can be avoided by replacing the <code>NaN</code> values with <code>null</code> values;</p>  Python Rust <p> <code>fill_nan</code> <pre><code>mean_nan_df = nan_df.with_columns(\n    pl.col(\"value\").fill_nan(None).alias(\"value\"),\n).mean()\nprint(mean_nan_df)\n</code></pre></p> <p> <code>fill_nan</code> <pre><code>let mean_nan_df = nan_df\n    .clone()\n    .lazy()\n    .with_columns([col(\"value\").fill_nan(lit(NULL)).alias(\"value\")])\n    .mean()\n    .collect()?;\nprintln!(\"{}\", &amp;mean_nan_df);\n</code></pre></p> <pre><code>shape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/numpy/","title":"Numpy","text":"<p>Polars expressions support NumPy ufuncs. See here for a list on all supported numpy functions.</p> <p>This means that if a function is not provided by Polars, we can use NumPy and we still have fast columnar operation through the NumPy API.</p>"},{"location":"user-guide/expressions/numpy/#example","title":"Example","text":"Python <p> <code>DataFrame</code> \u00b7 <code>log</code> \u00b7  Available on feature numpy <pre><code>import polars as pl\nimport numpy as np\n\ndf = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n\nout = df.select(np.log(pl.all()).name.suffix(\"_log\"))\nprint(out)\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a_log    \u2506 b_log    \u2502\n\u2502 ---      \u2506 ---      \u2502\n\u2502 f64      \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.0      \u2506 1.386294 \u2502\n\u2502 0.693147 \u2506 1.609438 \u2502\n\u2502 1.098612 \u2506 1.791759 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/numpy/#interoperability","title":"Interoperability","text":"<p>Polars <code>Series</code> have support for NumPy universal functions (ufuncs). Element-wise functions such as <code>np.exp()</code>, <code>np.cos()</code>, <code>np.div()</code>, etc. all work with almost zero overhead.</p> <p>However, as a Polars-specific remark: missing values are a separate bitmask and are not visible by NumPy. This can lead to a window function or a <code>np.convolve()</code> giving flawed or incomplete results.</p> <p>Convert a Polars <code>Series</code> to a NumPy array with the <code>.to_numpy()</code> method. Missing values will be replaced by <code>np.nan</code> during the conversion.</p>"},{"location":"user-guide/expressions/operators/","title":"Basic operators","text":"<p>This section describes how to use basic operators (e.g. addition, subtraction) in conjunction with Expressions. We will provide various examples using different themes in the context of the following dataframe.</p> <p>Note</p> <p>In Rust and Python it is possible to use the operators directly (as in <code>+ - * / &lt; &gt;</code>) as the language allows operator overloading. For instance, the operator <code>+</code> translates to the <code>.add()</code> method. You can choose the one you prefer.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use rand::{thread_rng, Rng};\n\nlet mut arr = [0f64; 5];\nthread_rng().fill(&amp;mut arr);\n\nlet df = df! (\n    \"nrs\" =&gt; &amp;[Some(1), Some(2), Some(3), None, Some(5)],\n    \"names\" =&gt; &amp;[Some(\"foo\"), Some(\"ham\"), Some(\"spam\"), Some(\"eggs\"), None],\n    \"random\" =&gt; &amp;arr,\n    \"groups\" =&gt; &amp;[\"A\", \"A\", \"B\", \"C\", \"B\"],\n)?;\n\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2506 A      \u2502\n\u2502 2    \u2506 ham   \u2506 0.74005  \u2506 A      \u2502\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2506 B      \u2502\n\u2502 null \u2506 egg   \u2506 0.533739 \u2506 C      \u2502\n\u2502 5    \u2506 null  \u2506 0.014575 \u2506 B      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/operators/#numerical","title":"Numerical","text":"Python Rust <p> <code>operators</code> <pre><code>df_numerical = df.select(\n    (pl.col(\"nrs\") + 5).alias(\"nrs + 5\"),\n    (pl.col(\"nrs\") - 5).alias(\"nrs - 5\"),\n    (pl.col(\"nrs\") * pl.col(\"random\")).alias(\"nrs * random\"),\n    (pl.col(\"nrs\") / pl.col(\"random\")).alias(\"nrs / random\"),\n)\nprint(df_numerical)\n</code></pre></p> <p> <code>operators</code> <pre><code>let df_numerical = df\n    .clone()\n    .lazy()\n    .select([\n        (col(\"nrs\") + lit(5)).alias(\"nrs + 5\"),\n        (col(\"nrs\") - lit(5)).alias(\"nrs - 5\"),\n        (col(\"nrs\") * col(\"random\")).alias(\"nrs * random\"),\n        (col(\"nrs\") / col(\"random\")).alias(\"nrs / random\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;df_numerical);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs + 5 \u2506 nrs - 5 \u2506 nrs * random \u2506 nrs / random \u2502\n\u2502 ---     \u2506 ---     \u2506 ---          \u2506 ---          \u2502\n\u2502 i64     \u2506 i64     \u2506 f64          \u2506 f64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6       \u2506 -4      \u2506 0.154163     \u2506 6.486647     \u2502\n\u2502 7       \u2506 -3      \u2506 1.480099     \u2506 2.702521     \u2502\n\u2502 8       \u2506 -2      \u2506 0.789945     \u2506 11.393198    \u2502\n\u2502 null    \u2506 null    \u2506 null         \u2506 null         \u2502\n\u2502 10      \u2506 0       \u2506 0.072875     \u2506 343.054056   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/operators/#logical","title":"Logical","text":"Python Rust <p> <code>operators</code> <pre><code>df_logical = df.select(\n    (pl.col(\"nrs\") &gt; 1).alias(\"nrs &gt; 1\"),\n    (pl.col(\"random\") &lt;= 0.5).alias(\"random &lt;= .5\"),\n    (pl.col(\"nrs\") != 1).alias(\"nrs != 1\"),\n    (pl.col(\"nrs\") == 1).alias(\"nrs == 1\"),\n    ((pl.col(\"random\") &lt;= 0.5) &amp; (pl.col(\"nrs\") &gt; 1)).alias(\"and_expr\"),  # and\n    ((pl.col(\"random\") &lt;= 0.5) | (pl.col(\"nrs\") &gt; 1)).alias(\"or_expr\"),  # or\n)\nprint(df_logical)\n</code></pre></p> <p> <code>operators</code> <pre><code>let df_logical = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"nrs\").gt(1).alias(\"nrs &gt; 1\"),\n        col(\"random\").lt_eq(0.5).alias(\"random &lt; .5\"),\n        col(\"nrs\").neq(1).alias(\"nrs != 1\"),\n        col(\"nrs\").eq(1).alias(\"nrs == 1\"),\n        (col(\"random\").lt_eq(0.5))\n            .and(col(\"nrs\").gt(1))\n            .alias(\"and_expr\"), // and\n        (col(\"random\").lt_eq(0.5))\n            .or(col(\"nrs\").gt(1))\n            .alias(\"or_expr\"), // or\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;df_logical);\n</code></pre></p> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs &gt; 1 \u2506 random &lt;= .5 \u2506 nrs != 1 \u2506 nrs == 1 \u2506 and_expr \u2506 or_expr \u2502\n\u2502 ---     \u2506 ---          \u2506 ---      \u2506 ---      \u2506 ---      \u2506 ---     \u2502\n\u2502 bool    \u2506 bool         \u2506 bool     \u2506 bool     \u2506 bool     \u2506 bool    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 false   \u2506 true         \u2506 false    \u2506 true     \u2506 false    \u2506 true    \u2502\n\u2502 true    \u2506 false        \u2506 true     \u2506 false    \u2506 false    \u2506 true    \u2502\n\u2502 true    \u2506 true         \u2506 true     \u2506 false    \u2506 true     \u2506 true    \u2502\n\u2502 null    \u2506 false        \u2506 null     \u2506 null     \u2506 false    \u2506 null    \u2502\n\u2502 true    \u2506 true         \u2506 true     \u2506 false    \u2506 true     \u2506 true    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/plugins/","title":"Expression plugins","text":"<p>Expression plugins are the preferred way to create user defined functions. They allow you to compile a Rust function and register that as an expression into the Polars library. The Polars engine will dynamically link your function at runtime and your expression will run almost as fast as native expressions. Note that this works without any interference of Python and thus no GIL contention.</p> <p>They will benefit from the same benefits default expressions have:</p> <ul> <li>Optimization</li> <li>Parallelism</li> <li>Rust native performance</li> </ul> <p>To get started we will see what is needed to create a custom expression.</p>"},{"location":"user-guide/expressions/plugins/#our-first-custom-expression-pig-latin","title":"Our first custom expression: Pig Latin","text":"<p>For our first expression we are going to create a pig latin converter. Pig latin is a silly language where in every word the first letter is removed, added to the back and finally \"ay\" is added. So the word \"pig\" would convert to \"igpay\".</p> <p>We could of course already do that with expressions, e.g. <code>col(\"name\").str.slice(1) + col(\"name\").str.slice(0, 1) + \"ay\"</code>, but a specialized function for this would perform better and allows us to learn about the plugins.</p>"},{"location":"user-guide/expressions/plugins/#setting-up","title":"Setting up","text":"<p>We start with a new library as the following <code>Cargo.toml</code> file</p> <pre><code>[package]\nname = \"expression_lib\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[lib]\nname = \"expression_lib\"\ncrate-type = [\"cdylib\"]\n\n[dependencies]\npolars = { version = \"*\" }\npyo3 = { version = \"*\", features = [\"extension-module\", \"abi-py38\"] }\npyo3-polars = { version = \"*\", features = [\"derive\"] }\nserde = { version = \"*\", features = [\"derive\"] }\n</code></pre>"},{"location":"user-guide/expressions/plugins/#writing-the-expression","title":"Writing the expression","text":"<p>In this library we create a helper function that converts a <code>&amp;str</code> to pig-latin, and we create the function that we will expose as an expression. To expose a function we must add the <code>#[polars_expr(output_type=DataType)]</code> attribute and the function must always accept <code>inputs: &amp;[Series]</code> as its first argument.</p> <pre><code>// src/expressions.rs\nuse polars::prelude::*;\nuse pyo3_polars::derive::polars_expr;\nuse std::fmt::Write;\n\nfn pig_latin_str(value: &amp;str, output: &amp;mut String) {\n    if let Some(first_char) = value.chars().next() {\n        write!(output, \"{}{}ay\", &amp;value[1..], first_char).unwrap()\n    }\n}\n\n#[polars_expr(output_type=String)]\nfn pig_latinnify(inputs: &amp;[Series]) -&gt; PolarsResult&lt;Series&gt; {\n    let ca = inputs[0].str()?;\n    let out: StringChunked = ca.apply_to_buffer(pig_latin_str);\n    Ok(out.into_series())\n}\n</code></pre> <p>This is all that is needed on the Rust side. On the Python side we must setup a folder with the same name as defined in the <code>Cargo.toml</code>, in this case \"expression_lib\". We will create a folder in the same directory as our Rust <code>src</code> folder named <code>expression_lib</code> and we create an <code>expression_lib/__init__.py</code>. The resulting file structure should look something like this:</p> <pre><code>\u251c\u2500\u2500 \ud83d\udcc1 expression_lib/  # name must match \"lib.name\" in Cargo.toml\n|   \u2514\u2500\u2500 __init__.py\n|\n\u251c\u2500\u2500 \ud83d\udcc1src/\n|   \u251c\u2500\u2500 lib.rs\n|   \u2514\u2500\u2500 expressions.rs\n|\n\u251c\u2500\u2500 Cargo.toml\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>Then we create a new class <code>Language</code> that will hold the expressions for our new <code>expr.language</code> namespace. The function name of our expression can be registered. Note that it is important that this name is correct, otherwise the main Polars package cannot resolve the function name. Furthermore we can set additional keyword arguments that explain to Polars how this expression behaves. In this case we tell Polars that this function is elementwise. This allows Polars to run this expression in batches. Whereas for other operations this would not be allowed, think for instance of a sort, or a slice.</p> <pre><code># expression_lib/__init__.py\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nimport polars as pl\nfrom polars.plugins import register_plugin_function\nfrom polars.type_aliases import IntoExpr\n\n\ndef pig_latinnify(expr: IntoExpr) -&gt; pl.Expr:\n    \"\"\"Pig-latinnify expression.\"\"\"\n    return register_plugin_function(\n        plugin_path=Path(__file__).parent,\n        function_name=\"pig_latinnify\",\n        args=expr,\n        is_elementwise=True,\n    )\n</code></pre> <p>We can then compile this library in our environment by installing <code>maturin</code> and running <code>maturin develop --release</code>.</p> <p>And that's it. Our expression is ready to use!</p> <pre><code>import polars as pl\nfrom expression_lib import pig_latinnify\n\ndf = pl.DataFrame(\n    {\n        \"convert\": [\"pig\", \"latin\", \"is\", \"silly\"],\n    }\n)\nout = df.with_columns(pig_latin=pig_latinnify(\"convert\"))\n</code></pre> <p>Alternatively, you can register a custom namespace, which enables you to write:</p> <pre><code>out = df.with_columns(\n    pig_latin=pl.col(\"convert\").language.pig_latinnify(),\n)\n</code></pre>"},{"location":"user-guide/expressions/plugins/#accepting-kwargs","title":"Accepting kwargs","text":"<p>If you want to accept <code>kwargs</code> (keyword arguments) in a polars expression, all you have to do is define a Rust <code>struct</code> and make sure that it derives <code>serde::Deserialize</code>.</p> <pre><code>/// Provide your own kwargs struct with the proper schema and accept that type\n/// in your plugin expression.\n#[derive(Deserialize)]\npub struct MyKwargs {\n    float_arg: f64,\n    integer_arg: i64,\n    string_arg: String,\n    boolean_arg: bool,\n}\n\n/// If you want to accept `kwargs`. You define a `kwargs` argument\n/// on the second position in you plugin. You can provide any custom struct that is deserializable\n/// with the pickle protocol (on the Rust side).\n#[polars_expr(output_type=String)]\nfn append_kwargs(input: &amp;[Series], kwargs: MyKwargs) -&gt; PolarsResult&lt;Series&gt; {\n    let input = &amp;input[0];\n    let input = input.cast(&amp;DataType::String)?;\n    let ca = input.str().unwrap();\n\n    Ok(ca\n        .apply_to_buffer(|val, buf| {\n            write!(\n                buf,\n                \"{}-{}-{}-{}-{}\",\n                val, kwargs.float_arg, kwargs.integer_arg, kwargs.string_arg, kwargs.boolean_arg\n            )\n                .unwrap()\n        })\n        .into_series())\n}\n</code></pre> <p>On the Python side the kwargs can be passed when we register the plugin.</p> <pre><code>def append_args(\n    expr: IntoExpr,\n    float_arg: float,\n    integer_arg: int,\n    string_arg: str,\n    boolean_arg: bool,\n) -&gt; pl.Expr:\n    \"\"\"\n    This example shows how arguments other than `Series` can be used.\n    \"\"\"\n    return register_plugin_function(\n        plugin_path=Path(__file__).parent,\n        function_name=\"append_kwargs\",\n        args=expr,\n        kwargs={\n            \"float_arg\": float_arg,\n            \"integer_arg\": integer_arg,\n            \"string_arg\": string_arg,\n            \"boolean_arg\": boolean_arg,\n        },\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"user-guide/expressions/plugins/#output-data-types","title":"Output data types","text":"<p>Output data types of course don't have to be fixed. They often depend on the input types of an expression. To accommodate this you can provide the <code>#[polars_expr()]</code> macro with an <code>output_type_func</code> argument that points to a function. This function can map input fields <code>&amp;[Field]</code> to an output <code>Field</code> (name and data type).</p> <p>In the snippet below is an example where we use the utility <code>FieldsMapper</code> to help with this mapping.</p> <pre><code>use polars_plan::dsl::FieldsMapper;\n\nfn haversine_output(input_fields: &amp;[Field]) -&gt; PolarsResult&lt;Field&gt; {\n    FieldsMapper::new(input_fields).map_to_float_dtype()\n}\n\n#[polars_expr(output_type_func=haversine_output)]\nfn haversine(inputs: &amp;[Series]) -&gt; PolarsResult&lt;Series&gt; {\n    let out = match inputs[0].dtype() {\n        DataType::Float32 =&gt; {\n            let start_lat = inputs[0].f32().unwrap();\n            let start_long = inputs[1].f32().unwrap();\n            let end_lat = inputs[2].f32().unwrap();\n            let end_long = inputs[3].f32().unwrap();\n            crate::distances::naive_haversine(start_lat, start_long, end_lat, end_long)?\n                .into_series()\n        }\n        DataType::Float64 =&gt; {\n            let start_lat = inputs[0].f64().unwrap();\n            let start_long = inputs[1].f64().unwrap();\n            let end_lat = inputs[2].f64().unwrap();\n            let end_long = inputs[3].f64().unwrap();\n            crate::distances::naive_haversine(start_lat, start_long, end_lat, end_long)?\n                .into_series()\n        }\n        _ =&gt; polars_bail!(InvalidOperation: \"only supported for float types\"),\n    };\n    Ok(out)\n}\n</code></pre> <p>That's all you need to know to get started. Take a look at this repo to see how this all fits together, and at this tutorial to gain a more thorough understanding.</p>"},{"location":"user-guide/expressions/plugins/#community-plugins","title":"Community plugins","text":"<p>Here is a curated (non-exhaustive) list of community-implemented plugins.</p> <ul> <li>polars-xdt Polars plugin with extra datetime-related functionality   which isn't quite in-scope for the main library</li> <li>polars-distance Polars plugin for pairwise distance functions</li> <li>polars-ds Polars extension aiming to simplify common numerical/string data analysis procedures</li> <li>polars-hash Stable non-cryptographic and cryptographic hashing functions for Polars</li> <li>polars-reverse-geocode Offline reverse geocoder for finding the closest city   to a given (latitude, longitude) pair</li> </ul>"},{"location":"user-guide/expressions/plugins/#other-material","title":"Other material","text":"<ul> <li>Ritchie Vink - Keynote on Polars Plugins</li> <li>Polars plugins tutorial Learn how to write a plugin by   going through some very simple and minimal examples</li> <li>cookiecutter-polars-plugin Project template for Polars Plugins</li> </ul>"},{"location":"user-guide/expressions/strings/","title":"Strings","text":"<p>The following section discusses operations performed on <code>String</code> data, which is a frequently used <code>DataType</code> when working with <code>DataFrames</code>. However, processing strings can often be inefficient due to their unpredictable memory size, causing the CPU to access many random memory locations. To address this issue, Polars utilizes Arrow as its backend, which stores all strings in a contiguous block of memory. As a result, string traversal is cache-optimal and predictable for the CPU.</p> <p>String processing functions are available in the <code>str</code> namespace.</p>"},{"location":"user-guide/expressions/strings/#accessing-the-string-namespace","title":"Accessing the string namespace","text":"<p>The <code>str</code> namespace can be accessed through the <code>.str</code> attribute of a column with <code>String</code> data type. In the following example, we create a column named <code>animal</code> and compute the length of each element in the column in terms of the number of bytes and the number of characters. If you are working with ASCII text, then the results of these two computations will be the same, and using <code>lengths</code> is recommended since it is faster.</p>  Python Rust <p> <code>str.len_bytes</code> \u00b7 <code>str.len_chars</code> <pre><code>df = pl.DataFrame({\"animal\": [\"Crab\", \"cat and dog\", \"rab$bit\", None]})\n\nout = df.select(\n    pl.col(\"animal\").str.len_bytes().alias(\"byte_count\"),\n    pl.col(\"animal\").str.len_chars().alias(\"letter_count\"),\n)\nprint(out)\n</code></pre></p> <p> <code>str.len_bytes</code> \u00b7 <code>str.len_chars</code> <pre><code>let df = df! (\n        \"animal\" =&gt; &amp;[Some(\"Crab\"), Some(\"cat and dog\"), Some(\"rab$bit\"), None],\n)?;\n\nlet out = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"animal\").str().len_bytes().alias(\"byte_count\"),\n        col(\"animal\").str().len_chars().alias(\"letter_count\"),\n    ])\n    .collect()?;\n\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 byte_count \u2506 letter_count \u2502\n\u2502 ---        \u2506 ---          \u2502\n\u2502 u32        \u2506 u32          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 4          \u2506 4            \u2502\n\u2502 11         \u2506 11           \u2502\n\u2502 7          \u2506 7            \u2502\n\u2502 null       \u2506 null         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#string-parsing","title":"String parsing","text":"<p>Polars offers multiple methods for checking and parsing elements of a string. Firstly, we can use the <code>contains</code> method to check whether a given pattern exists within a substring. Subsequently, we can extract these patterns and replace them using other methods, which will be demonstrated in upcoming examples.</p>"},{"location":"user-guide/expressions/strings/#check-for-existence-of-a-pattern","title":"Check for existence of a pattern","text":"<p>To check for the presence of a pattern within a string, we can use the contains method. The <code>contains</code> method accepts either a regular substring or a regex pattern, depending on the value of the <code>literal</code> parameter. If the pattern we're searching for is a simple substring located either at the beginning or end of the string, we can alternatively use the <code>starts_with</code> and <code>ends_with</code> functions.</p>  Python Rust <p> <code>str.contains</code> \u00b7 <code>str.starts_with</code> \u00b7 <code>str.ends_with</code> <pre><code>out = df.select(\n    pl.col(\"animal\"),\n    pl.col(\"animal\").str.contains(\"cat|bit\").alias(\"regex\"),\n    pl.col(\"animal\").str.contains(\"rab$\", literal=True).alias(\"literal\"),\n    pl.col(\"animal\").str.starts_with(\"rab\").alias(\"starts_with\"),\n    pl.col(\"animal\").str.ends_with(\"dog\").alias(\"ends_with\"),\n)\nprint(out)\n</code></pre></p> <p> <code>str.contains</code> \u00b7 <code>str.starts_with</code> \u00b7 <code>str.ends_with</code> \u00b7  Available on feature regex <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"animal\"),\n        col(\"animal\")\n            .str()\n            .contains(lit(\"cat|bit\"), false)\n            .alias(\"regex\"),\n        col(\"animal\")\n            .str()\n            .contains_literal(lit(\"rab$\"))\n            .alias(\"literal\"),\n        col(\"animal\")\n            .str()\n            .starts_with(lit(\"rab\"))\n            .alias(\"starts_with\"),\n        col(\"animal\").str().ends_with(lit(\"dog\")).alias(\"ends_with\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (4, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 animal      \u2506 regex \u2506 literal \u2506 starts_with \u2506 ends_with \u2502\n\u2502 ---         \u2506 ---   \u2506 ---     \u2506 ---         \u2506 ---       \u2502\n\u2502 str         \u2506 bool  \u2506 bool    \u2506 bool        \u2506 bool      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Crab        \u2506 false \u2506 false   \u2506 false       \u2506 false     \u2502\n\u2502 cat and dog \u2506 true  \u2506 false   \u2506 false       \u2506 true      \u2502\n\u2502 rab$bit     \u2506 true  \u2506 true    \u2506 true        \u2506 false     \u2502\n\u2502 null        \u2506 null  \u2506 null    \u2506 null        \u2506 null      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#extract-a-pattern","title":"Extract a pattern","text":"<p>The <code>extract</code> method allows us to extract a pattern from a specified string. This method takes a regex pattern containing one or more capture groups, which are defined by parentheses <code>()</code> in the pattern. The group index indicates which capture group to output.</p>  Python Rust <p> <code>str.extract</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": [\n            \"http://vote.com/ballon_dor?candidate=messi&amp;ref=polars\",\n            \"http://vote.com/ballon_dor?candidat=jorginho&amp;ref=polars\",\n            \"http://vote.com/ballon_dor?candidate=ronaldo&amp;ref=polars\",\n        ]\n    }\n)\nout = df.select(\n    pl.col(\"a\").str.extract(r\"candidate=(\\w+)\", group_index=1),\n)\nprint(out)\n</code></pre></p> <p> <code>str.extract</code> <pre><code>let df = df!(\n        \"a\" =&gt;  &amp;[\n            \"http://vote.com/ballon_dor?candidate=messi&amp;ref=polars\",\n            \"http://vote.com/ballon_dor?candidat=jorginho&amp;ref=polars\",\n            \"http://vote.com/ballon_dor?candidate=ronaldo&amp;ref=polars\",\n        ]\n)?;\nlet out = df\n    .clone()\n    .lazy()\n    .select([col(\"a\").str().extract(lit(r\"candidate=(\\w+)\"), 1)])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a       \u2502\n\u2502 ---     \u2502\n\u2502 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 messi   \u2502\n\u2502 null    \u2502\n\u2502 ronaldo \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To extract all occurrences of a pattern within a string, we can use the <code>extract_all</code> method. In the example below, we extract all numbers from a string using the regex pattern <code>(\\d+)</code>, which matches one or more digits. The resulting output of the <code>extract_all</code> method is a list containing all instances of the matched pattern within the string.</p>  Python Rust <p> <code>str.extract_all</code> <pre><code>df = pl.DataFrame({\"foo\": [\"123 bla 45 asd\", \"xyz 678 910t\"]})\nout = df.select(\n    pl.col(\"foo\").str.extract_all(r\"(\\d+)\").alias(\"extracted_nrs\"),\n)\nprint(out)\n</code></pre></p> <p> <code>str.extract_all</code> <pre><code>let df = df!(\"foo\"=&gt; &amp;[\"123 bla 45 asd\", \"xyz 678 910t\"])?;\nlet out = df\n    .clone()\n    .lazy()\n    .select([col(\"foo\")\n        .str()\n        .extract_all(lit(r\"(\\d+)\"))\n        .alias(\"extracted_nrs\")])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 extracted_nrs  \u2502\n\u2502 ---            \u2502\n\u2502 list[str]      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [\"123\", \"45\"]  \u2502\n\u2502 [\"678\", \"910\"] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#replace-a-pattern","title":"Replace a pattern","text":"<p>We have discussed two methods for pattern matching and extraction thus far, and now we will explore how to replace a pattern within a string. Similar to <code>extract</code> and <code>extract_all</code>, Polars provides the <code>replace</code> and <code>replace_all</code> methods for this purpose. In the example below we replace one match of <code>abc</code> at the end of a word (<code>\\b</code>) by <code>ABC</code> and we replace all occurrence of <code>a</code> with <code>-</code>.</p>  Python Rust <p> <code>str.replace</code> \u00b7 <code>str.replace_all</code> <pre><code>df = pl.DataFrame({\"id\": [1, 2], \"text\": [\"123abc\", \"abc456\"]})\nout = df.with_columns(\n    pl.col(\"text\").str.replace(r\"abc\\b\", \"ABC\"),\n    pl.col(\"text\").str.replace_all(\"a\", \"-\", literal=True).alias(\"text_replace_all\"),\n)\nprint(out)\n</code></pre></p> <p> <code>str.replace</code> \u00b7 <code>str.replace_all</code> \u00b7  Available on feature regex <pre><code>let df = df!(\"id\"=&gt; &amp;[1, 2], \"text\"=&gt; &amp;[\"123abc\", \"abc456\"])?;\nlet out = df\n    .clone()\n    .lazy()\n    .with_columns([\n        col(\"text\").str().replace(lit(r\"abc\\b\"), lit(\"ABC\"), false),\n        col(\"text\")\n            .str()\n            .replace_all(lit(\"a\"), lit(\"-\"), false)\n            .alias(\"text_replace_all\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 text   \u2506 text_replace_all \u2502\n\u2502 --- \u2506 ---    \u2506 ---              \u2502\n\u2502 i64 \u2506 str    \u2506 str              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 123ABC \u2506 123-bc           \u2502\n\u2502 2   \u2506 abc456 \u2506 -bc456           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#api-documentation","title":"API documentation","text":"<p>In addition to the examples covered above, Polars offers various other string manipulation methods for tasks such as formatting, stripping, splitting, and more. To explore these additional methods, you can go to the API documentation of your chosen programming language for Polars.</p>"},{"location":"user-guide/expressions/structs/","title":"The Struct datatype","text":"<p>Polars <code>Struct</code>s are the idiomatic way of working with multiple columns. It is also a free operation i.e. moving columns into <code>Struct</code>s does not copy any data!</p> <p>For this section, let's start with a <code>DataFrame</code> that captures the average rating of a few movies across some states in the U.S.:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>ratings = pl.DataFrame(\n    {\n        \"Movie\": [\"Cars\", \"IT\", \"ET\", \"Cars\", \"Up\", \"IT\", \"Cars\", \"ET\", \"Up\", \"ET\"],\n        \"Theatre\": [\"NE\", \"ME\", \"IL\", \"ND\", \"NE\", \"SD\", \"NE\", \"IL\", \"IL\", \"SD\"],\n        \"Avg_Rating\": [4.5, 4.4, 4.6, 4.3, 4.8, 4.7, 4.7, 4.9, 4.7, 4.6],\n        \"Count\": [30, 27, 26, 29, 31, 28, 28, 26, 33, 26],\n    }\n)\nprint(ratings)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let ratings = df!(\n        \"Movie\"=&gt; &amp;[\"Cars\", \"IT\", \"ET\", \"Cars\", \"Up\", \"IT\", \"Cars\", \"ET\", \"Up\", \"ET\"],\n        \"Theatre\"=&gt; &amp;[\"NE\", \"ME\", \"IL\", \"ND\", \"NE\", \"SD\", \"NE\", \"IL\", \"IL\", \"SD\"],\n        \"Avg_Rating\"=&gt; &amp;[4.5, 4.4, 4.6, 4.3, 4.8, 4.7, 4.7, 4.9, 4.7, 4.6],\n        \"Count\"=&gt; &amp;[30, 27, 26, 29, 31, 28, 28, 26, 33, 26],\n\n)?;\nprintln!(\"{}\", &amp;ratings);\n</code></pre></p> <pre><code>shape: (10, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Movie \u2506 Theatre \u2506 Avg_Rating \u2506 Count \u2502\n\u2502 ---   \u2506 ---     \u2506 ---        \u2506 ---   \u2502\n\u2502 str   \u2506 str     \u2506 f64        \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Cars  \u2506 NE      \u2506 4.5        \u2506 30    \u2502\n\u2502 IT    \u2506 ME      \u2506 4.4        \u2506 27    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.6        \u2506 26    \u2502\n\u2502 Cars  \u2506 ND      \u2506 4.3        \u2506 29    \u2502\n\u2502 Up    \u2506 NE      \u2506 4.8        \u2506 31    \u2502\n\u2502 IT    \u2506 SD      \u2506 4.7        \u2506 28    \u2502\n\u2502 Cars  \u2506 NE      \u2506 4.7        \u2506 28    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.9        \u2506 26    \u2502\n\u2502 Up    \u2506 IL      \u2506 4.7        \u2506 33    \u2502\n\u2502 ET    \u2506 SD      \u2506 4.6        \u2506 26    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/structs/#encountering-the-struct-type","title":"Encountering the <code>Struct</code> type","text":"<p>A common operation that will lead to a <code>Struct</code> column is the ever so popular <code>value_counts</code> function that is commonly used in exploratory data analysis. Checking the number of times a state appears the data will be done as so:</p>  Python Rust <p> <code>value_counts</code> <pre><code>out = ratings.select(pl.col(\"Theatre\").value_counts(sort=True))\nprint(out)\n</code></pre></p> <p> <code>value_counts</code> \u00b7  Available on feature dtype-struct <pre><code>let out = ratings\n    .clone()\n    .lazy()\n    .select([col(\"Theatre\").value_counts(true, true)])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Theatre   \u2502\n\u2502 ---       \u2502\n\u2502 struct[2] \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 {\"NE\",3}  \u2502\n\u2502 {\"IL\",3}  \u2502\n\u2502 {\"SD\",2}  \u2502\n\u2502 {\"ME\",1}  \u2502\n\u2502 {\"ND\",1}  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Quite unexpected an output, especially if coming from tools that do not have such a data type. We're not in peril though, to get back to a more familiar output, all we need to do is <code>unnest</code> the <code>Struct</code> column into its constituent columns:</p>  Python Rust <p> <code>unnest</code> <pre><code>out = ratings.select(pl.col(\"Theatre\").value_counts(sort=True)).unnest(\"Theatre\")\nprint(out)\n</code></pre></p> <p> <code>unnest</code> <pre><code>let out = ratings\n    .clone()\n    .lazy()\n    .select([col(\"Theatre\").value_counts(true, true)])\n    .unnest([\"Theatre\"])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Theatre \u2506 count \u2502\n\u2502 ---     \u2506 ---   \u2502\n\u2502 str     \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 NE      \u2506 3     \u2502\n\u2502 IL      \u2506 3     \u2502\n\u2502 SD      \u2506 2     \u2502\n\u2502 ME      \u2506 1     \u2502\n\u2502 ND      \u2506 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why <code>value_counts</code> returns a <code>Struct</code></p> <p>Polars expressions always have a <code>Fn(Series) -&gt; Series</code> signature and <code>Struct</code> is thus the data type that allows us to provide multiple columns as input/output of an expression. In other words, all expressions have to return a <code>Series</code> object, and <code>Struct</code> allows us to stay consistent with that requirement.</p>"},{"location":"user-guide/expressions/structs/#structs-as-dicts","title":"Structs as <code>dict</code>s","text":"<p>Polars will interpret a <code>dict</code> sent to the <code>Series</code> constructor as a <code>Struct</code>:</p>  Python Rust <p> <code>Series</code> <pre><code>rating_series = pl.Series(\n    \"ratings\",\n    [\n        {\"Movie\": \"Cars\", \"Theatre\": \"NE\", \"Avg_Rating\": 4.5},\n        {\"Movie\": \"Toy Story\", \"Theatre\": \"ME\", \"Avg_Rating\": 4.9},\n    ],\n)\nprint(rating_series)\n</code></pre></p> <p> <code>Series</code> <pre><code>// Don't think we can make it the same way in rust, but this works\nlet rating_series = df!(\n    \"Movie\" =&gt; &amp;[\"Cars\", \"Toy Story\"],\n    \"Theatre\" =&gt; &amp;[\"NE\", \"ME\"],\n    \"Avg_Rating\" =&gt; &amp;[4.5, 4.9],\n)?\n.into_struct(\"ratings\")\n.into_series();\nprintln!(\"{}\", &amp;rating_series);\n</code></pre></p> <pre><code>shape: (2,)\nSeries: 'ratings' [struct[3]]\n[\n    {\"Cars\",\"NE\",4.5}\n    {\"Toy Story\",\"ME\",4.9}\n]\n</code></pre> <p>Constructing <code>Series</code> objects</p> <p>Note that <code>Series</code> here was constructed with the <code>name</code> of the series in the beginning, followed by the <code>values</code>. Providing the latter first is considered an anti-pattern in Polars, and must be avoided.</p>"},{"location":"user-guide/expressions/structs/#extracting-individual-values-of-a-struct","title":"Extracting individual values of a <code>Struct</code>","text":"<p>Let's say that we needed to obtain just the <code>movie</code> value in the <code>Series</code> that we created above. We can use the <code>field</code> method to do so:</p>  Python Rust <p> <code>struct.field</code> <pre><code>out = rating_series.struct.field(\"Movie\")\nprint(out)\n</code></pre></p> <p> <code>struct.field_by_name</code> <pre><code>let out = rating_series.struct_()?.field_by_name(\"Movie\")?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (2,)\nSeries: 'Movie' [str]\n[\n    \"Cars\"\n    \"Toy Story\"\n]\n</code></pre>"},{"location":"user-guide/expressions/structs/#renaming-individual-keys-of-a-struct","title":"Renaming individual keys of a <code>Struct</code>","text":"<p>What if we need to rename individual <code>field</code>s of a <code>Struct</code> column? We first convert the <code>rating_series</code> object to a <code>DataFrame</code> so that we can view the changes easily, and then use the <code>rename_fields</code> method:</p>  Python Rust <p> <code>struct.rename_fields</code> <pre><code>out = (\n    rating_series.to_frame()\n    .select(pl.col(\"ratings\").struct.rename_fields([\"Film\", \"State\", \"Value\"]))\n    .unnest(\"ratings\")\n)\nprint(out)\n</code></pre></p> <p> <code>struct.rename_fields</code> <pre><code>let out = DataFrame::new([rating_series].into())?\n    .lazy()\n    .select([col(\"ratings\")\n        .struct_()\n        .rename_fields([\"Film\".into(), \"State\".into(), \"Value\".into()].to_vec())])\n    .unnest([\"ratings\"])\n    .collect()?;\n\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Film      \u2506 State \u2506 Value \u2502\n\u2502 ---       \u2506 ---   \u2506 ---   \u2502\n\u2502 str       \u2506 str   \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Cars      \u2506 NE    \u2506 4.5   \u2502\n\u2502 Toy Story \u2506 ME    \u2506 4.9   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/structs/#practical-use-cases-of-struct-columns","title":"Practical use-cases of <code>Struct</code> columns","text":""},{"location":"user-guide/expressions/structs/#identifying-duplicate-rows","title":"Identifying duplicate rows","text":"<p>Let's get back to the <code>ratings</code> data. We want to identify cases where there are duplicates at a <code>Movie</code> and <code>Theatre</code> level. This is where the <code>Struct</code> datatype shines:</p>  Python Rust <p> <code>is_duplicated</code> \u00b7 <code>struct</code> <pre><code>out = ratings.filter(pl.struct(\"Movie\", \"Theatre\").is_duplicated())\nprint(out)\n</code></pre></p> <p> <code>is_duplicated</code> \u00b7 <code>Struct</code> \u00b7  Available on feature dtype-struct <pre><code>let out = ratings\n    .clone()\n    .lazy()\n    // .filter(as_struct(&amp;[col(\"Movie\"), col(\"Theatre\")]).is_duplicated())\n    // Error: .is_duplicated() not available if you try that\n    // https://github.com/pola-rs/polars/issues/3803\n    .filter(len().over([col(\"Movie\"), col(\"Theatre\")]).gt(lit(1)))\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Movie \u2506 Theatre \u2506 Avg_Rating \u2506 Count \u2502\n\u2502 ---   \u2506 ---     \u2506 ---        \u2506 ---   \u2502\n\u2502 str   \u2506 str     \u2506 f64        \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Cars  \u2506 NE      \u2506 4.5        \u2506 30    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.6        \u2506 26    \u2502\n\u2502 Cars  \u2506 NE      \u2506 4.7        \u2506 28    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.9        \u2506 26    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can identify the unique cases at this level also with <code>is_unique</code>!</p>"},{"location":"user-guide/expressions/structs/#multi-column-ranking","title":"Multi-column ranking","text":"<p>Suppose, given that we know there are duplicates, we want to choose which rank gets a higher priority. We define <code>Count</code> of ratings to be more important than the actual <code>Avg_Rating</code> themselves, and only use it to break a tie. We can then do:</p>  Python Rust <p> <code>is_duplicated</code> \u00b7 <code>struct</code> <pre><code>out = ratings.with_columns(\n    pl.struct(\"Count\", \"Avg_Rating\")\n    .rank(\"dense\", descending=True)\n    .over(\"Movie\", \"Theatre\")\n    .alias(\"Rank\")\n).filter(pl.struct(\"Movie\", \"Theatre\").is_duplicated())\nprint(out)\n</code></pre></p> <p> <code>is_duplicated</code> \u00b7 <code>Struct</code> \u00b7  Available on feature dtype-struct <pre><code>let out = ratings\n    .clone()\n    .lazy()\n    .with_columns([as_struct(vec![col(\"Count\"), col(\"Avg_Rating\")])\n        .rank(\n            RankOptions {\n                method: RankMethod::Dense,\n                descending: false,\n            },\n            None,\n        )\n        .over([col(\"Movie\"), col(\"Theatre\")])\n        .alias(\"Rank\")])\n    // .filter(as_struct(&amp;[col(\"Movie\"), col(\"Theatre\")]).is_duplicated())\n    // Error: .is_duplicated() not available if you try that\n    // https://github.com/pola-rs/polars/issues/3803\n    .filter(len().over([col(\"Movie\"), col(\"Theatre\")]).gt(lit(1)))\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (4, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Movie \u2506 Theatre \u2506 Avg_Rating \u2506 Count \u2506 Rank \u2502\n\u2502 ---   \u2506 ---     \u2506 ---        \u2506 ---   \u2506 ---  \u2502\n\u2502 str   \u2506 str     \u2506 f64        \u2506 i64   \u2506 u32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Cars  \u2506 NE      \u2506 4.5        \u2506 30    \u2506 1    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.6        \u2506 26    \u2506 2    \u2502\n\u2502 Cars  \u2506 NE      \u2506 4.7        \u2506 28    \u2506 2    \u2502\n\u2502 ET    \u2506 IL      \u2506 4.9        \u2506 26    \u2506 1    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>That's a pretty complex set of requirements done very elegantly in Polars!</p>"},{"location":"user-guide/expressions/structs/#using-multi-column-apply","title":"Using multi-column apply","text":"<p>This was discussed in the previous section on User Defined Functions.</p>"},{"location":"user-guide/expressions/user-defined-functions/","title":"User-defined functions (Python)","text":"<p>You should be convinced by now that Polars expressions are so powerful and flexible that there is much less need for custom Python functions than in other libraries.</p> <p>Still, you need to have the power to be able to pass an expression's state to a third party library or apply your black box function over data in Polars.</p> <p>For this we provide the following expressions:</p> <ul> <li><code>map_batches</code></li> <li><code>map_elements</code></li> </ul>"},{"location":"user-guide/expressions/user-defined-functions/#to-map_batches-or-to-map_elements","title":"To <code>map_batches</code> or to <code>map_elements</code>.","text":"<p>These functions have an important distinction in how they operate and consequently what data they will pass to the user.</p> <p>A <code>map_batches</code> passes the <code>Series</code> backed by the <code>expression</code> as is.</p> <p><code>map_batches</code> follows the same rules in both the <code>select</code> and the <code>group_by</code> context, this will mean that the <code>Series</code> represents a column in a <code>DataFrame</code>. Note that in the <code>group_by</code> context, that column is not yet aggregated!</p> <p>Use cases for <code>map_batches</code> are for instance passing the <code>Series</code> in an expression to a third party library. Below we show how we could use <code>map_batches</code> to pass an expression column to a neural network model.</p>  Python <p> <code>map_batches</code></p> <pre><code>df.with_columns([\n    pl.col(\"features\").map_batches(lambda s: MyNeuralNetwork.forward(s.to_numpy())).alias(\"activations\")\n])\n</code></pre>  Rust <pre><code>df.with_columns([\n    col(\"features\").map(|s| Ok(my_nn.forward(s))).alias(\"activations\")\n])\n</code></pre> <p>Use cases for <code>map_batches</code> in the <code>group_by</code> context are slim. They are only used for performance reasons, but can quite easily lead to incorrect results. Let me explain why.</p>  Python Rust <pre><code>df = pl.DataFrame(\n    {\n        \"keys\": [\"a\", \"a\", \"b\"],\n        \"values\": [10, 7, 1],\n    }\n)\nprint(df)\n</code></pre> <pre><code>let df = df!(\n    \"keys\" =&gt; &amp;[\"a\", \"a\", \"b\"],\n    \"values\" =&gt; &amp;[10, 7, 1],\n)?;\nprintln!(\"{}\", df);\n</code></pre> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 keys \u2506 values \u2502\n\u2502 ---  \u2506 ---    \u2502\n\u2502 str  \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 10     \u2502\n\u2502 a    \u2506 7      \u2502\n\u2502 b    \u2506 1      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In the snippet above we group by the <code>\"keys\"</code> column. That means we have the following groups:</p> <pre><code>\"a\" -&gt; [10, 7]\n\"b\" -&gt; [1]\n</code></pre> <p>If we would then apply a <code>shift</code> operation to the right, we'd expect:</p> <pre><code>\"a\" -&gt; [null, 10]\n\"b\" -&gt; [null]\n</code></pre> <p>Let's try that out and see what we get:</p>  Python Rust <pre><code>out = df.group_by(\"keys\", maintain_order=True).agg(\n    pl.col(\"values\")\n    .map_batches(lambda s: s.shift(), is_elementwise=True)\n    .alias(\"shift_map_batches\"),\n    pl.col(\"values\").shift().alias(\"shift_expression\"),\n)\nprint(out)\n</code></pre> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .group_by([\"keys\"])\n    .agg([\n        col(\"values\")\n            .map(|s| Ok(Some(s.shift(1))), GetOutput::default())\n            // note: the `'shift_map_batches'` alias is just there to show how you\n            // get the same output as in the Python API example.\n            .alias(\"shift_map_batches\"),\n        col(\"values\").shift(lit(1)).alias(\"shift_expression\"),\n    ])\n    .collect()?;\n\nprintln!(\"{}\", out);\n</code></pre> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 keys \u2506 shift_map_batches \u2506 shift_expression \u2502\n\u2502 ---  \u2506 ---               \u2506 ---              \u2502\n\u2502 str  \u2506 list[i64]         \u2506 list[i64]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 [null, 10]        \u2506 [null, 10]       \u2502\n\u2502 b    \u2506 [7]               \u2506 [null]           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Ouch.. we clearly get the wrong results here. Group <code>\"b\"</code> even got a value from group <code>\"a\"</code> \ud83d\ude35.</p> <p>This went horribly wrong because <code>map_batches</code> applied the function before aggregation, due to the <code>is_elementwise=True</code> parameter being provided. So that means the whole column <code>[10, 7, 1]</code> got shifted to <code>[null, 10, 7]</code> and was then aggregated.</p> <p>So my advice is to never use <code>map_batches</code> in the <code>group_by</code> context unless you know you need it and know what you are doing.</p>"},{"location":"user-guide/expressions/user-defined-functions/#to-map_elements","title":"To <code>map_elements</code>","text":"<p>Luckily we can fix previous example with <code>map_elements</code>. <code>map_elements</code> works on the smallest logical elements for that operation.</p> <p>That is:</p> <ul> <li><code>select context</code> -&gt; single elements</li> <li><code>group by context</code> -&gt; single groups</li> </ul> <p>So with <code>map_elements</code> we should be able to fix our example:</p>  Python <p> <code>map_elements</code></p>  Python Rust <pre><code>out = df.group_by(\"keys\", maintain_order=True).agg(\n    pl.col(\"values\")\n    .map_elements(lambda s: s.shift(), return_dtype=pl.List(int))\n    .alias(\"shift_map_elements\"),\n    pl.col(\"values\").shift().alias(\"shift_expression\"),\n)\nprint(out)\n</code></pre> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .group_by([col(\"keys\")])\n    .agg([\n        col(\"values\")\n            .apply(|s| Ok(Some(s.shift(1))), GetOutput::default())\n            // note: the `'shift_map_elements'` alias is just there to show how you\n            // get the same output as in the Python API example.\n            .alias(\"shift_map_elements\"),\n        col(\"values\").shift(lit(1)).alias(\"shift_expression\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 keys \u2506 shift_map_elements \u2506 shift_expression \u2502\n\u2502 ---  \u2506 ---                \u2506 ---              \u2502\n\u2502 str  \u2506 list[i64]          \u2506 list[i64]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 [null, 10]         \u2506 [null, 10]       \u2502\n\u2502 b    \u2506 [null]             \u2506 [null]           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>And observe, a valid result! \ud83c\udf89</p>"},{"location":"user-guide/expressions/user-defined-functions/#map_elements-in-the-select-context","title":"<code>map_elements</code> in the <code>select</code> context","text":"<p>In the <code>select</code> context, the <code>map_elements</code> expression passes elements of the column to the Python function.</p> <p>Note that you are now running Python, this will be slow.</p> <p>Let's go through some examples to see what to expect. We will continue with the <code>DataFrame</code> we defined at the start of this section and show an example with the <code>map_elements</code> function and a counter example where we use the expression API to achieve the same goals.</p>"},{"location":"user-guide/expressions/user-defined-functions/#adding-a-counter","title":"Adding a counter","text":"<p>In this example we create a global <code>counter</code> and then add the integer <code>1</code> to the global state at every element processed. Every iteration the result of the increment will be added to the element value.</p> <p>Note, this example isn't provided in Rust. The reason is that the global <code>counter</code> value would lead to data races when this <code>apply</code> is evaluated in parallel. It would be possible to wrap it in a <code>Mutex</code> to protect the variable, but that would be obscuring the point of the example. This is a case where the Python Global Interpreter Lock's performance tradeoff provides some safety guarantees.</p>  Python Rust <pre><code>counter = 0\n\n\ndef add_counter(val: int) -&gt; int:\n    global counter\n    counter += 1\n    return counter + val\n\n\nout = df.select(\n    pl.col(\"values\").map_elements(add_counter).alias(\"solution_map_elements\"),\n    (pl.col(\"values\") + pl.int_range(1, pl.len() + 1)).alias(\"solution_expr\"),\n)\nprint(out)\n</code></pre> <pre><code>\n</code></pre> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 solution_map_elements \u2506 solution_expr \u2502\n\u2502 ---                   \u2506 ---           \u2502\n\u2502 i64                   \u2506 i64           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11                    \u2506 11            \u2502\n\u2502 9                     \u2506 9             \u2502\n\u2502 4                     \u2506 4             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/user-defined-functions/#combining-multiple-column-values","title":"Combining multiple column values","text":"<p>If we want to have access to values of different columns in a single <code>map_elements</code> function call, we can create <code>struct</code> data type. This data type collects those columns as fields in the <code>struct</code>. So if we'd create a struct from the columns <code>\"keys\"</code> and <code>\"values\"</code>, we would get the following struct elements:</p> <pre><code>[\n    {\"keys\": \"a\", \"values\": 10},\n    {\"keys\": \"a\", \"values\": 7},\n    {\"keys\": \"b\", \"values\": 1},\n]\n</code></pre> <p>In Python, those would be passed as <code>dict</code> to the calling Python function and can thus be indexed by <code>field: str</code>. In Rust, you'll get a <code>Series</code> with the <code>Struct</code> type. The fields of the struct can then be indexed and downcast.</p>  Python Rust <pre><code>out = df.select(\n    pl.struct([\"keys\", \"values\"])\n    .map_elements(lambda x: len(x[\"keys\"]) + x[\"values\"])\n    .alias(\"solution_map_elements\"),\n    (pl.col(\"keys\").str.len_bytes() + pl.col(\"values\")).alias(\"solution_expr\"),\n)\nprint(out)\n</code></pre> <pre><code>let out = df\n    .lazy()\n    .select([\n        // pack to struct to get access to multiple fields in a custom `apply/map`\n        as_struct(vec![col(\"keys\"), col(\"values\")])\n            // we will compute the len(a) + b\n            .apply(\n                |s| {\n                    // downcast to struct\n                    let ca = s.struct_()?;\n\n                    // get the fields as Series\n                    let s_a = &amp;ca.fields()[0];\n                    let s_b = &amp;ca.fields()[1];\n\n                    // downcast the `Series` to their known type\n                    let ca_a = s_a.str()?;\n                    let ca_b = s_b.i32()?;\n\n                    // iterate both `ChunkedArrays`\n                    let out: Int32Chunked = ca_a\n                        .into_iter()\n                        .zip(ca_b)\n                        .map(|(opt_a, opt_b)| match (opt_a, opt_b) {\n                            (Some(a), Some(b)) =&gt; Some(a.len() as i32 + b),\n                            _ =&gt; None,\n                        })\n                        .collect();\n\n                    Ok(Some(out.into_series()))\n                },\n                GetOutput::from_type(DataType::Int32),\n            )\n            // note: the `'solution_map_elements'` alias is just there to show how you\n            // get the same output as in the Python API example.\n            .alias(\"solution_map_elements\"),\n        (col(\"keys\").str().count_matches(lit(\".\"), true) + col(\"values\"))\n            .alias(\"solution_expr\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 solution_map_elements \u2506 solution_expr \u2502\n\u2502 ---                   \u2506 ---           \u2502\n\u2502 i64                   \u2506 i64           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11                    \u2506 11            \u2502\n\u2502 8                     \u2506 8             \u2502\n\u2502 2                     \u2506 2             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p><code>Structs</code> are covered in detail in the next section.</p>"},{"location":"user-guide/expressions/user-defined-functions/#return-types","title":"Return types?","text":"<p>Custom Python functions are black boxes for Polars. We really don't know what kind of black arts you are doing, so we have to infer and try our best to understand what you meant.</p> <p>As a user it helps to understand what we do to better utilize custom functions.</p> <p>The data type is automatically inferred. We do that by waiting for the first non-null value. That value will then be used to determine the type of the <code>Series</code>.</p> <p>The mapping of Python types to Polars data types is as follows:</p> <ul> <li><code>int</code> -&gt; <code>Int64</code></li> <li><code>float</code> -&gt; <code>Float64</code></li> <li><code>bool</code> -&gt; <code>Boolean</code></li> <li><code>str</code> -&gt; <code>String</code></li> <li><code>list[tp]</code> -&gt; <code>List[tp]</code> (where the inner type is inferred with the same rules)</li> <li><code>dict[str, [tp]]</code> -&gt; <code>struct</code></li> <li><code>Any</code> -&gt; <code>object</code> (Prevent this at all times)</li> </ul> <p>Rust types map as follows:</p> <ul> <li><code>i32</code> or <code>i64</code> -&gt; <code>Int64</code></li> <li><code>f32</code> or <code>f64</code> -&gt; <code>Float64</code></li> <li><code>bool</code> -&gt; <code>Boolean</code></li> <li><code>String</code> or <code>str</code> -&gt; <code>String</code></li> <li><code>Vec&lt;tp&gt;</code> -&gt; <code>List[tp]</code> (where the inner type is inferred with the same rules)</li> </ul>"},{"location":"user-guide/expressions/window/","title":"Window functions","text":"<p>Window functions are expressions with superpowers. They allow you to perform aggregations on groups in the <code>select</code> context. Let's get a feel for what that means. First we create a dataset. The dataset loaded in the snippet below contains information about pokemon:</p>  Python Rust <p> <code>read_csv</code> <pre><code>import polars as pl\n\n# then let's load some csv data with information about pokemon\ndf = pl.read_csv(\n    \"https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv\"\n)\nprint(df.head())\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>use polars::prelude::*;\nuse reqwest::blocking::Client;\n\nlet data: Vec&lt;u8&gt; = Client::new()\n    .get(\"https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv\")\n    .send()?\n    .text()?\n    .bytes()\n    .collect();\n\nlet df = CsvReader::new(std::io::Cursor::new(data))\n    .has_header(true)\n    .finish()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 13)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 #   \u2506 Name                  \u2506 Type 1 \u2506 Type 2 \u2506 \u2026 \u2506 Sp. Def \u2506 Speed \u2506 Generation \u2506 Legendary \u2502\n\u2502 --- \u2506 ---                   \u2506 ---    \u2506 ---    \u2506   \u2506 ---     \u2506 ---   \u2506 ---        \u2506 ---       \u2502\n\u2502 i64 \u2506 str                   \u2506 str    \u2506 str    \u2506   \u2506 i64     \u2506 i64   \u2506 i64        \u2506 bool      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Bulbasaur             \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 65      \u2506 45    \u2506 1          \u2506 false     \u2502\n\u2502 2   \u2506 Ivysaur               \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 80      \u2506 60    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 Venusaur              \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 100     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 VenusaurMega Venusaur \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 120     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 4   \u2506 Charmander            \u2506 Fire   \u2506 null   \u2506 \u2026 \u2506 50      \u2506 65    \u2506 1          \u2506 false     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/window/#group-by-aggregations-in-selection","title":"Group by aggregations in selection","text":"<p>Below we show how to use window functions to group over different columns and perform an aggregation on them. Doing so allows us to use multiple group by operations in parallel, using a single query. The results of the aggregation are projected back to the original rows. Therefore, a window function will almost always lead to a <code>DataFrame</code> with the same size as the original.</p> <p>We will discuss later the cases where a window function can change the numbers of rows in a <code>DataFrame</code>.</p> <p>Note how we call <code>.over(\"Type 1\")</code> and <code>.over([\"Type 1\", \"Type 2\"])</code>. Using window functions we can aggregate over different groups in a single <code>select</code> call! Note that, in Rust, the type of the argument to <code>over()</code> must be a collection, so even when you're only using one column, you must provide it in an array.</p> <p>The best part is, this won't cost you anything. The computed groups are cached and shared between different <code>window</code> expressions.</p>  Python Rust <p> <code>over</code> <pre><code>out = df.select(\n    \"Type 1\",\n    \"Type 2\",\n    pl.col(\"Attack\").mean().over(\"Type 1\").alias(\"avg_attack_by_type\"),\n    pl.col(\"Defense\")\n    .mean()\n    .over([\"Type 1\", \"Type 2\"])\n    .alias(\"avg_defense_by_type_combination\"),\n    pl.col(\"Attack\").mean().alias(\"avg_attack\"),\n)\nprint(out)\n</code></pre></p> <p> <code>over</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"Type 1\"),\n        col(\"Type 2\"),\n        col(\"Attack\")\n            .mean()\n            .over([\"Type 1\"])\n            .alias(\"avg_attack_by_type\"),\n        col(\"Defense\")\n            .mean()\n            .over([\"Type 1\", \"Type 2\"])\n            .alias(\"avg_defense_by_type_combination\"),\n        col(\"Attack\").mean().alias(\"avg_attack\"),\n    ])\n    .collect()?;\n\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (163, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Type 1  \u2506 Type 2 \u2506 avg_attack_by_type \u2506 avg_defense_by_type_combinatio\u2026 \u2506 avg_attack \u2502\n\u2502 ---     \u2506 ---    \u2506 ---                \u2506 ---                             \u2506 ---        \u2502\n\u2502 str     \u2506 str    \u2506 f64                \u2506 f64                             \u2506 f64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Grass   \u2506 Poison \u2506 72.923077          \u2506 67.8                            \u2506 75.349693  \u2502\n\u2502 Grass   \u2506 Poison \u2506 72.923077          \u2506 67.8                            \u2506 75.349693  \u2502\n\u2502 Grass   \u2506 Poison \u2506 72.923077          \u2506 67.8                            \u2506 75.349693  \u2502\n\u2502 Grass   \u2506 Poison \u2506 72.923077          \u2506 67.8                            \u2506 75.349693  \u2502\n\u2502 Fire    \u2506 null   \u2506 88.642857          \u2506 58.3                            \u2506 75.349693  \u2502\n\u2502 \u2026       \u2506 \u2026      \u2506 \u2026                  \u2506 \u2026                               \u2506 \u2026          \u2502\n\u2502 Fire    \u2506 Flying \u2506 88.642857          \u2506 82.0                            \u2506 75.349693  \u2502\n\u2502 Dragon  \u2506 null   \u2506 94.0               \u2506 55.0                            \u2506 75.349693  \u2502\n\u2502 Dragon  \u2506 null   \u2506 94.0               \u2506 55.0                            \u2506 75.349693  \u2502\n\u2502 Dragon  \u2506 Flying \u2506 94.0               \u2506 95.0                            \u2506 75.349693  \u2502\n\u2502 Psychic \u2506 null   \u2506 53.875             \u2506 51.428571                       \u2506 75.349693  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/window/#operations-per-group","title":"Operations per group","text":"<p>Window functions can do more than aggregation. They can also be viewed as an operation within a group. If, for instance, you want to <code>sort</code> the values within a <code>group</code>, you can write <code>col(\"value\").sort().over(\"group\")</code> and voil\u00e0! We sorted by group!</p> <p>Let's filter out some rows to make this more clear.</p>  Python Rust <p> <code>filter</code> <pre><code>filtered = df.filter(pl.col(\"Type 2\") == \"Psychic\").select(\n    \"Name\",\n    \"Type 1\",\n    \"Speed\",\n)\nprint(filtered)\n</code></pre></p> <p> <code>filter</code> <pre><code>let filtered = df\n    .clone()\n    .lazy()\n    .filter(col(\"Type 2\").eq(lit(\"Psychic\")))\n    .select([col(\"Name\"), col(\"Type 1\"), col(\"Speed\")])\n    .collect()?;\n\nprintln!(\"{}\", filtered);\n</code></pre></p> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name                \u2506 Type 1 \u2506 Speed \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---   \u2502\n\u2502 str                 \u2506 str    \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Slowpoke            \u2506 Water  \u2506 15    \u2502\n\u2502 Slowbro             \u2506 Water  \u2506 30    \u2502\n\u2502 SlowbroMega Slowbro \u2506 Water  \u2506 30    \u2502\n\u2502 Exeggcute           \u2506 Grass  \u2506 40    \u2502\n\u2502 Exeggutor           \u2506 Grass  \u2506 55    \u2502\n\u2502 Starmie             \u2506 Water  \u2506 115   \u2502\n\u2502 Jynx                \u2506 Ice    \u2506 95    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Observe that the group <code>Water</code> of column <code>Type 1</code> is not contiguous. There are two rows of <code>Grass</code> in between. Also note that each pokemon within a group are sorted by <code>Speed</code> in <code>ascending</code> order. Unfortunately, for this example we want them sorted in <code>descending</code> speed order. Luckily with window functions this is easy to accomplish.</p>  Python Rust <p> <code>over</code> <pre><code>out = filtered.with_columns(\n    pl.col([\"Name\", \"Speed\"]).sort_by(\"Speed\", descending=True).over(\"Type 1\"),\n)\nprint(out)\n</code></pre></p> <p> <code>over</code> <pre><code>let out = filtered\n    .lazy()\n    .with_columns([cols([\"Name\", \"Speed\"])\n        .sort_by(\n            [\"Speed\"],\n            SortMultipleOptions::default().with_order_descending(true),\n        )\n        .over([\"Type 1\"])])\n    .collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name                \u2506 Type 1 \u2506 Speed \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---   \u2502\n\u2502 str                 \u2506 str    \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Starmie             \u2506 Water  \u2506 115   \u2502\n\u2502 Slowbro             \u2506 Water  \u2506 30    \u2502\n\u2502 SlowbroMega Slowbro \u2506 Water  \u2506 30    \u2502\n\u2502 Exeggutor           \u2506 Grass  \u2506 55    \u2502\n\u2502 Exeggcute           \u2506 Grass  \u2506 40    \u2502\n\u2502 Slowpoke            \u2506 Water  \u2506 15    \u2502\n\u2502 Jynx                \u2506 Ice    \u2506 95    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Polars keeps track of each group's location and maps the expressions to the proper row locations. This will also work over different groups in a single <code>select</code>.</p> <p>The power of window expressions is that you often don't need a <code>group_by -&gt; explode</code> combination, but you can put the logic in a single expression. It also makes the API cleaner. If properly used a:</p> <ul> <li><code>group_by</code> -&gt; marks that groups are aggregated and we expect a <code>DataFrame</code> of size <code>n_groups</code></li> <li><code>over</code> -&gt; marks that we want to compute something within a group, and doesn't modify the original size of the <code>DataFrame</code> except in specific cases</li> </ul>"},{"location":"user-guide/expressions/window/#map-the-expression-result-to-the-dataframe-rows","title":"Map the expression result to the DataFrame rows","text":"<p>In cases where the expression results in multiple values per group, the Window function has 3 strategies for linking the values back to the <code>DataFrame</code> rows:</p> <ul> <li> <p><code>mapping_strategy = 'group_to_rows'</code> -&gt; each value is assigned back to one row. The number of values returned should match the number of rows.</p> </li> <li> <p><code>mapping_strategy = 'join'</code> -&gt; the values are imploded in a list, and the list is repeated on all rows. This can be memory intensive.</p> </li> <li> <p><code>mapping_strategy = 'explode'</code> -&gt; the values are exploded to new rows. This operation changes the number of rows.</p> </li> </ul>"},{"location":"user-guide/expressions/window/#window-expression-rules","title":"Window expression rules","text":"<p>The evaluations of window expressions are as follows (assuming we apply it to a <code>pl.Int32</code> column):</p>  Python Rust <p> <code>over</code> <pre><code># aggregate and broadcast within a group\n# output type: -&gt; Int32\npl.sum(\"foo\").over(\"groups\")\n\n# sum within a group and multiply with group elements\n# output type: -&gt; Int32\n(pl.col(\"x\").sum() * pl.col(\"y\")).over(\"groups\")\n\n# sum within a group and multiply with group elements\n# and aggregate the group to a list\n# output type: -&gt; List(Int32)\n(pl.col(\"x\").sum() * pl.col(\"y\")).over(\"groups\", mapping_strategy=\"join\")\n\n# sum within a group and multiply with group elements\n# and aggregate the group to a list\n# then explode the list to multiple rows\n\n# This is the fastest method to do things over groups when the groups are sorted\n(pl.col(\"x\").sum() * pl.col(\"y\")).over(\"groups\", mapping_strategy=\"explode\")\n</code></pre></p> <p> <code>over</code> <pre><code>// aggregate and broadcast within a group\n// output type: -&gt; i32\nlet _ = sum(\"foo\").over([col(\"groups\")]);\n// sum within a group and multiply with group elements\n// output type: -&gt; i32\nlet _ = (col(\"x\").sum() * col(\"y\"))\n    .over([col(\"groups\")])\n    .alias(\"x1\");\n// sum within a group and multiply with group elements\n// and aggregate the group to a list\n// output type: -&gt; ChunkedArray&lt;i32&gt;\nlet _ = (col(\"x\").sum() * col(\"y\"))\n    .over([col(\"groups\")])\n    .alias(\"x2\");\n// note that it will require an explicit `list()` call\n// sum within a group and multiply with group elements\n// and aggregate the group to a list\n// the flatten call explodes that list\n\n// This is the fastest method to do things over groups when the groups are sorted\nlet _ = (col(\"x\").sum() * col(\"y\"))\n    .over([col(\"groups\")])\n    .flatten()\n    .alias(\"x3\");\n</code></pre></p>"},{"location":"user-guide/expressions/window/#more-examples","title":"More examples","text":"<p>For more exercise, below are some window functions for us to compute:</p> <ul> <li>sort all pokemon by type</li> <li>select the first <code>3</code> pokemon per type as <code>\"Type 1\"</code></li> <li>sort the pokemon within a type by speed in descending order and select the first <code>3</code> as <code>\"fastest/group\"</code></li> <li>sort the pokemon within a type by attack in descending order and select the first <code>3</code> as <code>\"strongest/group\"</code></li> <li>sort the pokemon within a type by name and select the first <code>3</code> as <code>\"sorted_by_alphabet\"</code></li> </ul>  Python Rust <p> <code>over</code> \u00b7 <code>implode</code> <pre><code>out = df.sort(\"Type 1\").select(\n    pl.col(\"Type 1\").head(3).over(\"Type 1\", mapping_strategy=\"explode\"),\n    pl.col(\"Name\")\n    .sort_by(pl.col(\"Speed\"), descending=True)\n    .head(3)\n    .over(\"Type 1\", mapping_strategy=\"explode\")\n    .alias(\"fastest/group\"),\n    pl.col(\"Name\")\n    .sort_by(pl.col(\"Attack\"), descending=True)\n    .head(3)\n    .over(\"Type 1\", mapping_strategy=\"explode\")\n    .alias(\"strongest/group\"),\n    pl.col(\"Name\")\n    .sort()\n    .head(3)\n    .over(\"Type 1\", mapping_strategy=\"explode\")\n    .alias(\"sorted_by_alphabet\"),\n)\nprint(out)\n</code></pre></p> <p> <code>over</code> \u00b7 <code>implode</code> <pre><code>let out = df\n    .clone()\n    .lazy()\n    .select([\n        col(\"Type 1\").head(Some(3)).over([\"Type 1\"]).flatten(),\n        col(\"Name\")\n            .sort_by(\n                [\"Speed\"],\n                SortMultipleOptions::default().with_order_descending(true),\n            )\n            .head(Some(3))\n            .over([\"Type 1\"])\n            .flatten()\n            .alias(\"fastest/group\"),\n        col(\"Name\")\n            .sort_by(\n                [\"Attack\"],\n                SortMultipleOptions::default().with_order_descending(true),\n            )\n            .head(Some(3))\n            .over([\"Type 1\"])\n            .flatten()\n            .alias(\"strongest/group\"),\n        col(\"Name\")\n            .sort(Default::default())\n            .head(Some(3))\n            .over([\"Type 1\"])\n            .flatten()\n            .alias(\"sorted_by_alphabet\"),\n    ])\n    .collect()?;\nprintln!(\"{:?}\", out);\n</code></pre></p> <pre><code>shape: (43, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Type 1 \u2506 fastest/group         \u2506 strongest/group       \u2506 sorted_by_alphabet        \u2502\n\u2502 ---    \u2506 ---                   \u2506 ---                   \u2506 ---                       \u2502\n\u2502 str    \u2506 str                   \u2506 str                   \u2506 str                       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Bug    \u2506 BeedrillMega Beedrill \u2506 PinsirMega Pinsir     \u2506 Beedrill                  \u2502\n\u2502 Bug    \u2506 Scyther               \u2506 BeedrillMega Beedrill \u2506 BeedrillMega Beedrill     \u2502\n\u2502 Bug    \u2506 PinsirMega Pinsir     \u2506 Pinsir                \u2506 Butterfree                \u2502\n\u2502 Dragon \u2506 Dragonite             \u2506 Dragonite             \u2506 Dragonair                 \u2502\n\u2502 Dragon \u2506 Dragonair             \u2506 Dragonair             \u2506 Dragonite                 \u2502\n\u2502 \u2026      \u2506 \u2026                     \u2506 \u2026                     \u2506 \u2026                         \u2502\n\u2502 Rock   \u2506 Aerodactyl            \u2506 Golem                 \u2506 AerodactylMega Aerodactyl \u2502\n\u2502 Rock   \u2506 Kabutops              \u2506 Kabutops              \u2506 Geodude                   \u2502\n\u2502 Water  \u2506 Starmie               \u2506 GyaradosMega Gyarados \u2506 Blastoise                 \u2502\n\u2502 Water  \u2506 Tentacruel            \u2506 Kingler               \u2506 BlastoiseMega Blastoise   \u2502\n\u2502 Water  \u2506 Poliwag               \u2506 Gyarados              \u2506 Cloyster                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/io/","title":"IO","text":"<p>Reading and writing your data is crucial for a DataFrame library. In this chapter you will learn more on how to read and write to different file formats that are supported by Polars.</p> <ul> <li>CSV</li> <li>Excel</li> <li>Parquet</li> <li>Json</li> <li>Multiple</li> <li>Database</li> <li>Cloud storage</li> <li>Google Big Query</li> </ul>"},{"location":"user-guide/io/bigquery/","title":"Google BigQuery","text":"<p>To read or write from GBQ, additional dependencies are needed:</p>  Python <pre><code>$ pip install google-cloud-bigquery\n</code></pre>"},{"location":"user-guide/io/bigquery/#read","title":"Read","text":"<p>We can load a query into a <code>DataFrame</code> like this:</p>  Python <p> <code>from_arrow</code> \u00b7  Available on feature pyarrow \u00b7  Available on feature fsspec <pre><code>import polars as pl\nfrom google.cloud import bigquery\n\nclient = bigquery.Client()\n\n# Perform a query.\nQUERY = (\n    'SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` '\n    'WHERE state = \"TX\" '\n    'LIMIT 100')\nquery_job = client.query(QUERY)  # API request\nrows = query_job.result()  # Waits for query to finish\n\ndf = pl.from_arrow(rows.to_arrow())\n</code></pre></p>"},{"location":"user-guide/io/bigquery/#write","title":"Write","text":"Python <pre><code>from google.cloud import bigquery\n\nclient = bigquery.Client()\n\n# Write DataFrame to stream as parquet file; does not hit disk\nwith io.BytesIO() as stream:\n    df.write_parquet(stream)\n    stream.seek(0)\n    job = client.load_table_from_file(\n        stream,\n        destination='tablename',\n        project='projectname',\n        job_config=bigquery.LoadJobConfig(\n            source_format=bigquery.SourceFormat.PARQUET,\n        ),\n    )\njob.result()  # Waits for the job to complete\n</code></pre>"},{"location":"user-guide/io/cloud-storage/","title":"Cloud storage","text":"<p>Polars can read and write to AWS S3, Azure Blob Storage and Google Cloud Storage. The API is the same for all three storage providers.</p> <p>To read from cloud storage, additional dependencies may be needed depending on the use case and cloud storage provider:</p>  Python Rust <pre><code>$ pip install fsspec s3fs adlfs gcsfs\n</code></pre> <pre><code>$ cargo add aws_sdk_s3 aws_config tokio --features tokio/full\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#reading-from-cloud-storage","title":"Reading from cloud storage","text":"<p>Polars can read a CSV, IPC or Parquet file in eager mode from cloud storage.</p>  Python Rust <p> <code>read_parquet</code> \u00b7 <code>read_csv</code> \u00b7 <code>read_ipc</code> <pre><code>import polars as pl\n\nsource = \"s3://bucket/*.parquet\"\n\ndf = pl.read_parquet(source)\n</code></pre></p> <p> <code>ParquetReader</code> \u00b7 <code>CsvReader</code> \u00b7 <code>IpcReader</code> \u00b7  Available on feature ipc \u00b7  Available on feature parquet \u00b7  Available on feature csv <pre><code>use aws_config::BehaviorVersion;\nuse polars::prelude::*;\n\n#[tokio::main]\nasync fn main() {\n    let bucket = \"&lt;YOUR_BUCKET&gt;\";\n    let path = \"&lt;YOUR_PATH&gt;\";\n\n    let config = aws_config::load_defaults(BehaviorVersion::latest()).await;\n    let client = aws_sdk_s3::Client::new(&amp;config);\n\n    let object = client\n        .get_object()\n        .bucket(bucket)\n        .key(path)\n        .send()\n        .await\n        .unwrap();\n\n    let bytes = object.body.collect().await.unwrap().into_bytes();\n\n    let cursor = std::io::Cursor::new(bytes);\n    let df = CsvReader::new(cursor).finish().unwrap();\n\n    println!(\"{:?}\", df);\n}\n</code></pre></p> <p>This eager query downloads the file to a buffer in memory and creates a <code>DataFrame</code> from there. Polars uses <code>fsspec</code> to manage this download internally for all cloud storage providers.</p>"},{"location":"user-guide/io/cloud-storage/#scanning-from-cloud-storage-with-query-optimisation","title":"Scanning from cloud storage with query optimisation","text":"<p>Polars can scan a Parquet file in lazy mode from cloud storage. We may need to provide further details beyond the source url such as authentication details or storage region. Polars looks for these as environment variables but we can also do this manually by passing a <code>dict</code> as the <code>storage_options</code> argument.</p>  Python Rust <p> <code>scan_parquet</code> <pre><code>import polars as pl\n\nsource = \"s3://bucket/*.parquet\"\n\nstorage_options = {\n    \"aws_access_key_id\": \"&lt;secret&gt;\",\n    \"aws_secret_access_key\": \"&lt;secret&gt;\",\n    \"aws_region\": \"us-east-1\",\n}\ndf = pl.scan_parquet(source, storage_options=storage_options)\n</code></pre></p> <p> <code>scan_parquet</code> \u00b7  Available on feature parquet <pre><code>\n</code></pre></p> <p>This query creates a <code>LazyFrame</code> without downloading the file. In the <code>LazyFrame</code> we have access to file metadata such as the schema. Polars uses the <code>object_store.rs</code> library internally to manage the interface with the cloud storage providers and so no extra dependencies are required in Python to scan a cloud Parquet file.</p> <p>If we create a lazy query with predicate and projection pushdowns, the query optimizer will apply them before the file is downloaded. This can significantly reduce the amount of data that needs to be downloaded. The query evaluation is triggered by calling <code>collect</code>.</p>  Python Rust <pre><code>import polars as pl\n\nsource = \"s3://bucket/*.parquet\"\n\n\ndf = pl.scan_parquet(source).filter(pl.col(\"id\") &lt; 100).select(\"id\",\"value\").collect()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/io/cloud-storage/#scanning-with-pyarrow","title":"Scanning with PyArrow","text":"<p>We can also scan from cloud storage using PyArrow. This is particularly useful for partitioned datasets such as Hive partitioning.</p> <p>We first create a PyArrow dataset and then create a <code>LazyFrame</code> from the dataset.</p>  Python Rust <p> <code>scan_pyarrow_dataset</code> <pre><code>import polars as pl\nimport pyarrow.dataset as ds\n\ndset = ds.dataset(\"s3://my-partitioned-folder/\", format=\"parquet\")\n(\n    pl.scan_pyarrow_dataset(dset)\n    .filter(pl.col(\"foo\") == \"a\")\n    .select([\"foo\", \"bar\"])\n    .collect()\n)\n</code></pre></p> <p> <code>scan_pyarrow_dataset</code> <pre><code>\n</code></pre></p>"},{"location":"user-guide/io/cloud-storage/#writing-to-cloud-storage","title":"Writing to cloud storage","text":"<p>We can write a <code>DataFrame</code> to cloud storage in Python using s3fs for S3, adlfs for Azure Blob Storage and gcsfs for Google Cloud Storage. In this example, we write a Parquet file to S3.</p>  Python Rust <p> <code>write_parquet</code> <pre><code>import polars as pl\nimport s3fs\n\ndf = pl.DataFrame({\n    \"foo\": [\"a\", \"b\", \"c\", \"d\", \"d\"],\n    \"bar\": [1, 2, 3, 4, 5],\n})\n\nfs = s3fs.S3FileSystem()\ndestination = \"s3://bucket/my_file.parquet\"\n\n# write parquet\nwith fs.open(destination, mode='wb') as f:\n    df.write_parquet(f)\n</code></pre></p> <p> <code>ParquetWriter</code> \u00b7  Available on feature parquet <pre><code>\n</code></pre></p>"},{"location":"user-guide/io/csv/","title":"CSV","text":""},{"location":"user-guide/io/csv/#read-write","title":"Read &amp; write","text":"<p>Reading a CSV file should look familiar:</p>  Python Rust <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"docs/data/path.csv\")\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>use polars::prelude::*;\n\nlet df = CsvReader::from_path(\"docs/data/path.csv\")\n    .unwrap()\n    .finish()\n    .unwrap();\n</code></pre></p> <p>Writing a CSV file is similar with the <code>write_csv</code> function:</p>  Python Rust <p> <code>write_csv</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_csv(\"docs/data/path.csv\")\n</code></pre></p> <p> <code>CsvWriter</code> \u00b7  Available on feature csv <pre><code>let mut df = df!(\n    \"foo\" =&gt; &amp;[1, 2, 3],\n    \"bar\" =&gt; &amp;[None, Some(\"bak\"), Some(\"baz\")],\n)\n.unwrap();\n\nlet mut file = std::fs::File::create(\"docs/data/path.csv\").unwrap();\nCsvWriter::new(&amp;mut file).finish(&amp;mut df).unwrap();\n</code></pre></p>"},{"location":"user-guide/io/csv/#scan","title":"Scan","text":"<p>Polars allows you to scan a CSV input. Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>  Python Rust <p> <code>scan_csv</code> <pre><code>df = pl.scan_csv(\"docs/data/path.csv\")\n</code></pre></p> <p> <code>LazyCsvReader</code> \u00b7  Available on feature csv <pre><code>let lf = LazyCsvReader::new(\"./test.csv\").finish().unwrap();\n</code></pre></p> <p>If you want to know why this is desirable, you can read more about these Polars optimizations here.</p>"},{"location":"user-guide/io/database/","title":"Databases","text":""},{"location":"user-guide/io/database/#read-from-a-database","title":"Read from a database","text":"<p>Polars can read from a database using the <code>pl.read_database_uri</code> and <code>pl.read_database</code> functions.</p>"},{"location":"user-guide/io/database/#difference-between-read_database_uri-and-read_database","title":"Difference between <code>read_database_uri</code> and <code>read_database</code>","text":"<p>Use <code>pl.read_database_uri</code> if you want to specify the database connection with a connection string called a <code>uri</code>. For example, the following snippet shows a query to read all columns from the <code>foo</code> table in a Postgres database where we use the <code>uri</code> to connect:</p>  Python <p> <code>read_database_uri</code> <pre><code>import polars as pl\n\nuri = \"postgresql://username:password@server:port/database\"\nquery = \"SELECT * FROM foo\"\n\npl.read_database_uri(query=query, uri=uri)\n</code></pre></p> <p>On the other hand, use <code>pl.read_database</code> if you want to connect via a connection engine created with a library like SQLAlchemy.</p>  Python <p> <code>read_database</code> <pre><code>import polars as pl\nfrom sqlalchemy import create_engine\n\nconn = create_engine(f\"sqlite:///test.db\")\n\nquery = \"SELECT * FROM foo\"\n\npl.read_database(query=query, connection=conn.connect())\n</code></pre></p> <p>Note that <code>pl.read_database_uri</code> is likely to be faster than <code>pl.read_database</code> if you are using a SQLAlchemy or DBAPI2 connection as these connections may load the data row-wise into Python before copying the data again to the column-wise Apache Arrow format.</p>"},{"location":"user-guide/io/database/#engines","title":"Engines","text":"<p>Polars doesn't manage connections and data transfer from databases by itself. Instead, external libraries (known as engines) handle this.</p> <p>When using <code>pl.read_database</code>, you specify the engine when you create the connection object. When using <code>pl.read_database_uri</code>, you can specify one of two engines to read from the database:</p> <ul> <li>ConnectorX and</li> <li>ADBC</li> </ul> <p>Both engines have native support for Apache Arrow and so can read data directly into a Polars <code>DataFrame</code> without copying the data.</p>"},{"location":"user-guide/io/database/#connectorx","title":"ConnectorX","text":"<p>ConnectorX is the default engine and supports numerous databases including Postgres, Mysql, SQL Server and Redshift. ConnectorX is written in Rust and stores data in Arrow format to allow for zero-copy to Polars.</p> <p>To read from one of the supported databases with <code>ConnectorX</code> you need to activate the additional dependency <code>ConnectorX</code> when installing Polars or install it manually with</p> <pre><code>$ pip install connectorx\n</code></pre>"},{"location":"user-guide/io/database/#adbc","title":"ADBC","text":"<p>ADBC (Arrow Database Connectivity) is an engine supported by the Apache Arrow project. ADBC aims to be both an API standard for connecting to databases and libraries implementing this standard in a range of languages.</p> <p>It is still early days for ADBC so support for different databases is limited. At present, drivers for ADBC are only available for Postgres, SQLite and Snowflake. To install ADBC, you need to install the driver for your database. For example, to install the driver for SQLite, you run:</p> <pre><code>$ pip install adbc-driver-sqlite\n</code></pre> <p>As ADBC is not the default engine, you must specify the engine as an argument to <code>pl.read_database_uri</code>.</p>  Python <p> <code>read_database_uri</code> <pre><code>uri = \"postgresql://username:password@server:port/database\"\nquery = \"SELECT * FROM foo\"\n\npl.read_database_uri(query=query, uri=uri, engine=\"adbc\")\n</code></pre></p>"},{"location":"user-guide/io/database/#write-to-a-database","title":"Write to a database","text":"<p>We can write to a database with Polars using the <code>pl.write_database</code> function.</p>"},{"location":"user-guide/io/database/#engines_1","title":"Engines","text":"<p>As with reading from a database above, Polars uses an engine to write to a database. The currently supported engines are:</p> <ul> <li>SQLAlchemy and</li> <li>Arrow Database Connectivity (ADBC)</li> </ul>"},{"location":"user-guide/io/database/#sqlalchemy","title":"SQLAlchemy","text":"<p>With the default engine SQLAlchemy you can write to any database supported by SQLAlchemy. To use this engine you need to install SQLAlchemy and Pandas</p> <pre><code>$ pip install SQLAlchemy pandas\n</code></pre> <p>In this example, we write the <code>DataFrame</code> to a table called <code>records</code> in the database</p>  Python <p> <code>write_database</code> <pre><code>uri = \"postgresql://username:password@server:port/database\"\ndf = pl.DataFrame({\"foo\": [1, 2, 3]})\n\ndf.write_database(table_name=\"records\",  connection=uri)\n</code></pre></p> <p>In the SQLAlchemy approach, Polars converts the <code>DataFrame</code> to a Pandas <code>DataFrame</code> backed by PyArrow and then uses SQLAlchemy methods on a Pandas <code>DataFrame</code> to write to the database.</p>"},{"location":"user-guide/io/database/#adbc_1","title":"ADBC","text":"<p>ADBC can also be used to write to a database. Writing is supported for the same databases that support reading with ADBC. As shown above, you need to install the appropriate ADBC driver for your database.</p>  Python <p> <code>write_database</code> <pre><code>uri = \"postgresql://username:password@server:port/database\"\ndf = pl.DataFrame({\"foo\": [1, 2, 3]})\n\ndf.write_database(table_name=\"records\", connection=uri, engine=\"adbc\")\n</code></pre></p>"},{"location":"user-guide/io/excel/","title":"Excel","text":"<p>Polars can read and write to Excel files from Python. From a performance perspective, we recommend using other formats if possible, such as Parquet or CSV files.</p>"},{"location":"user-guide/io/excel/#read","title":"Read","text":"<p>Polars does not have a native Excel reader. Instead, it uses external libraries to parse Excel files into objects that Polars can parse. The available engines are:</p> <ul> <li>xlsx2csv: This is the current default.</li> <li>openpyxl: Typically slower than xls2csv, but can provide more flexibility for files that are difficult to parse.</li> <li>pyxlsb: For reading binary Excel files (xlsb).</li> <li>fastexcel: This reader is based on calamine and is typically the fastest reader but has fewer features than xls2csv.</li> </ul> <p>Although fastexcel is not the default at this point, we recommend trying fastexcel first and using xlsx2csv or openpyxl if you encounter issues.</p> <p>To use one of these engines, the appropriate Python package must be installed as an additional dependency.</p>  Python <pre><code>$ pip install xlsx2csv openpyxl pyxlsb fastexcel\n</code></pre> <p>The default Excel reader is xlsx2csv. It is a Python library which parses the Excel file into a CSV file which Polars then reads with the native CSV reader. We read an Excel file with <code>read_excel</code>:</p>  Python <p> <code>read_excel</code> <pre><code>df = pl.read_excel(\"docs/data/path.xlsx\")\n</code></pre></p> <p>We can specify the sheet name to read with the <code>sheet_name</code> argument. If we do not specify a sheet name, the first sheet will be read.</p>  Python <p> <code>read_excel</code> <pre><code>df = pl.read_excel(\"docs/data/path.xlsx\", sheet_name=\"Sales\")\n</code></pre></p>"},{"location":"user-guide/io/excel/#write","title":"Write","text":"<p>We need the xlswriter library installed as an additional dependency to write to Excel files.</p>  Python <pre><code>$ pip install xlsxwriter\n</code></pre> <p>Writing to Excel files is not currently available in Rust Polars, though it is possible to use this crate to write to Excel files from Rust.</p> <p>Writing a <code>DataFrame</code> to an Excel file is done with the <code>write_excel</code> method:</p>  Python <p> <code>write_excel</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_excel(\"docs/data/path.xlsx\")\n</code></pre></p> <p>The name of the worksheet can be specified with the <code>worksheet</code> argument.</p>  Python <p> <code>write_excel</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_excel(\"docs/data/path.xlsx\", worksheet=\"Sales\")\n</code></pre></p> <p>Polars can create rich Excel files with multiple sheets and formatting. For more details, see the API docs for <code>write_excel</code>.</p>"},{"location":"user-guide/io/json/","title":"JSON files","text":"<p>Polars can read and write both standard JSON and newline-delimited JSON (NDJSON).</p>"},{"location":"user-guide/io/json/#read","title":"Read","text":""},{"location":"user-guide/io/json/#json","title":"JSON","text":"<p>Reading a JSON file should look familiar:</p>  Python Rust <p> <code>read_json</code> <pre><code>df = pl.read_json(\"docs/data/path.json\")\n</code></pre></p> <p> <code>JsonReader</code> \u00b7  Available on feature json <pre><code>use polars::prelude::*;\n\nlet mut file = std::fs::File::open(\"docs/data/path.json\").unwrap();\nlet df = JsonReader::new(&amp;mut file).finish().unwrap();\n</code></pre></p>"},{"location":"user-guide/io/json/#newline-delimited-json","title":"Newline Delimited JSON","text":"<p>JSON objects that are delimited by newlines can be read into Polars in a much more performant way than standard json.</p> <p>Polars can read an NDJSON file into a <code>DataFrame</code> using the <code>read_ndjson</code> function:</p>  Python Rust <p> <code>read_ndjson</code> <pre><code>df = pl.read_ndjson(\"docs/data/path.json\")\n</code></pre></p> <p> <code>JsonLineReader</code> \u00b7  Available on feature json <pre><code>let mut file = std::fs::File::open(\"docs/data/path.json\").unwrap();\nlet df = JsonLineReader::new(&amp;mut file).finish().unwrap();\n</code></pre></p>"},{"location":"user-guide/io/json/#write","title":"Write","text":"Python Rust <p> <code>write_json</code> \u00b7 <code>write_ndjson</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_json(\"docs/data/path.json\")\n</code></pre></p> <p> <code>JsonWriter</code> \u00b7 <code>JsonWriter</code> \u00b7  Available on feature json <pre><code>let mut df = df!(\n    \"foo\" =&gt; &amp;[1, 2, 3],\n    \"bar\" =&gt; &amp;[None, Some(\"bak\"), Some(\"baz\")],\n)\n.unwrap();\n\nlet mut file = std::fs::File::create(\"docs/data/path.json\").unwrap();\n\n// json\nJsonWriter::new(&amp;mut file)\n    .with_json_format(JsonFormat::Json)\n    .finish(&amp;mut df)\n    .unwrap();\n\n// ndjson\nJsonWriter::new(&amp;mut file)\n    .with_json_format(JsonFormat::JsonLines)\n    .finish(&amp;mut df)\n    .unwrap();\n</code></pre></p>"},{"location":"user-guide/io/json/#scan","title":"Scan","text":"<p>Polars allows you to scan a JSON input only for newline delimited json. Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>  Python Rust <p> <code>scan_ndjson</code> <pre><code>df = pl.scan_ndjson(\"docs/data/path.json\")\n</code></pre></p> <p> <code>LazyJsonLineReader</code> \u00b7  Available on feature json <pre><code>let lf = LazyJsonLineReader::new(\"docs/data/path.json\")\n    .finish()\n    .unwrap();\n</code></pre></p>"},{"location":"user-guide/io/multiple/","title":"Multiple","text":""},{"location":"user-guide/io/multiple/#dealing-with-multiple-files","title":"Dealing with multiple files.","text":"<p>Polars can deal with multiple files differently depending on your needs and memory strain.</p> <p>Let's create some files to give us some context:</p>  Python <p> <code>write_csv</code> <pre><code>import polars as pl\n\ndf = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"ham\", \"spam\"]})\n\nfor i in range(5):\n    df.write_csv(f\"docs/data/my_many_files_{i}.csv\")\n</code></pre></p>"},{"location":"user-guide/io/multiple/#reading-into-a-single-dataframe","title":"Reading into a single <code>DataFrame</code>","text":"<p>To read multiple files into a single <code>DataFrame</code>, we can use globbing patterns:</p>  Python <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"docs/data/my_many_files_*.csv\")\nprint(df)\n</code></pre></p> <pre><code>shape: (15, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar  \u2502\n\u2502 --- \u2506 ---  \u2502\n\u2502 i64 \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 null \u2502\n\u2502 2   \u2506 ham  \u2502\n\u2502 3   \u2506 spam \u2502\n\u2502 1   \u2506 null \u2502\n\u2502 2   \u2506 ham  \u2502\n\u2502 \u2026   \u2506 \u2026    \u2502\n\u2502 2   \u2506 ham  \u2502\n\u2502 3   \u2506 spam \u2502\n\u2502 1   \u2506 null \u2502\n\u2502 2   \u2506 ham  \u2502\n\u2502 3   \u2506 spam \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To see how this works we can take a look at the query plan. Below we see that all files are read separately and concatenated into a single <code>DataFrame</code>. Polars will try to parallelize the reading.</p>  Python <p> <code>show_graph</code> <pre><code>pl.scan_csv(\"docs/data/my_many_files_*.csv\").show_graph()\n</code></pre></p> <p></p>"},{"location":"user-guide/io/multiple/#reading-and-processing-in-parallel","title":"Reading and processing in parallel","text":"<p>If your files don't have to be in a single table you can also build a query plan for each file and execute them in parallel on the Polars thread pool.</p> <p>All query plan execution is embarrassingly parallel and doesn't require any communication.</p>  Python <p> <code>scan_csv</code> <pre><code>import glob\n\nimport polars as pl\n\nqueries = []\nfor file in glob.glob(\"docs/data/my_many_files_*.csv\"):\n    q = pl.scan_csv(file).group_by(\"bar\").agg(pl.len(), pl.sum(\"foo\"))\n    queries.append(q)\n\ndataframes = pl.collect_all(queries)\nprint(dataframes)\n</code></pre></p> <pre><code>[shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 len \u2506 foo \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 str  \u2506 u32 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null \u2506 1   \u2506 1   \u2502\n\u2502 ham  \u2506 1   \u2506 2   \u2502\n\u2502 spam \u2506 1   \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 len \u2506 foo \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 str  \u2506 u32 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 spam \u2506 1   \u2506 3   \u2502\n\u2502 null \u2506 1   \u2506 1   \u2502\n\u2502 ham  \u2506 1   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 len \u2506 foo \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 str  \u2506 u32 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 ham  \u2506 1   \u2506 2   \u2502\n\u2502 spam \u2506 1   \u2506 3   \u2502\n\u2502 null \u2506 1   \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 len \u2506 foo \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 str  \u2506 u32 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 ham  \u2506 1   \u2506 2   \u2502\n\u2502 spam \u2506 1   \u2506 3   \u2502\n\u2502 null \u2506 1   \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 len \u2506 foo \u2502\n\u2502 ---  \u2506 --- \u2506 --- \u2502\n\u2502 str  \u2506 u32 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 spam \u2506 1   \u2506 3   \u2502\n\u2502 ham  \u2506 1   \u2506 2   \u2502\n\u2502 null \u2506 1   \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518]\n</code></pre>"},{"location":"user-guide/io/parquet/","title":"Parquet","text":"<p>Loading or writing <code>Parquet</code> files is lightning fast as the layout of data in a Polars <code>DataFrame</code> in memory mirrors the layout of a Parquet file on disk in many respects.</p> <p>Unlike CSV, Parquet is a columnar format. This means that the data is stored in columns rather than rows. This is a more efficient way of storing data as it allows for better compression and faster access to data.</p>"},{"location":"user-guide/io/parquet/#read","title":"Read","text":"<p>We can read a <code>Parquet</code> file into a <code>DataFrame</code> using the <code>read_parquet</code> function:</p>  Python Rust <p> <code>read_parquet</code> <pre><code>df = pl.read_parquet(\"docs/data/path.parquet\")\n</code></pre></p> <p> <code>ParquetReader</code> \u00b7  Available on feature parquet <pre><code>let mut file = std::fs::File::open(\"docs/data/path.parquet\").unwrap();\n\nlet df = ParquetReader::new(&amp;mut file).finish().unwrap();\n</code></pre></p>"},{"location":"user-guide/io/parquet/#write","title":"Write","text":"Python Rust <p> <code>write_parquet</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_parquet(\"docs/data/path.parquet\")\n</code></pre></p> <p> <code>ParquetWriter</code> \u00b7  Available on feature parquet <pre><code>let mut df = df!(\n    \"foo\" =&gt; &amp;[1, 2, 3],\n    \"bar\" =&gt; &amp;[None, Some(\"bak\"), Some(\"baz\")],\n)\n.unwrap();\n\nlet mut file = std::fs::File::create(\"docs/data/path.parquet\").unwrap();\nParquetWriter::new(&amp;mut file).finish(&amp;mut df).unwrap();\n</code></pre></p>"},{"location":"user-guide/io/parquet/#scan","title":"Scan","text":"<p>Polars allows you to scan a <code>Parquet</code> input. Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>  Python Rust <p> <code>scan_parquet</code> <pre><code>df = pl.scan_parquet(\"docs/data/path.parquet\")\n</code></pre></p> <p> <code>scan_parquet</code> \u00b7  Available on feature parquet <pre><code>let args = ScanArgsParquet::default();\nlet lf = LazyFrame::scan_parquet(\"./file.parquet\", args).unwrap();\n</code></pre></p> <p>If you want to know why this is desirable, you can read more about those Polars optimizations here.</p> <p>When we scan a <code>Parquet</code> file stored in the cloud, we can also apply predicate and projection pushdowns. This can significantly reduce the amount of data that needs to be downloaded. For scanning a Parquet file in the cloud, see Cloud storage.</p>"},{"location":"user-guide/lazy/","title":"Lazy","text":"<p>The Lazy chapter is a guide for working with <code>LazyFrames</code>. It covers the functionalities like how to use it and how to optimise it. You can also find more information about the query plan or gain more insight in the streaming capabilities.</p> <ul> <li>Using lazy API</li> <li>Optimisations</li> <li>Schemas</li> <li>Query plan</li> <li>Execution</li> <li>Streaming</li> </ul>"},{"location":"user-guide/lazy/execution/","title":"Query execution","text":"<p>Our example query on the Reddit dataset is:</p>  Python <p> <code>scan_csv</code> <pre><code>q1 = (\n    pl.scan_csv(\"docs/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n)\n</code></pre></p> <p>If we were to run the code above on the Reddit CSV the query would not be evaluated. Instead Polars takes each line of code, adds it to the internal query graph and optimizes the query graph.</p> <p>When we execute the code Polars executes the optimized query graph by default.</p>"},{"location":"user-guide/lazy/execution/#execution-on-the-full-dataset","title":"Execution on the full dataset","text":"<p>We can execute our query on the full dataset by calling the <code>.collect</code> method on the query.</p>  Python <p> <code>scan_csv</code> \u00b7 <code>collect</code> <pre><code>q4 = (\n    pl.scan_csv(f\"docs/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n    .collect()\n)\n</code></pre></p> <pre><code>shape: (14_029, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id      \u2506 name                      \u2506 created_utc \u2506 updated_on \u2506 comment_karma \u2506 link_karma \u2502\n\u2502 ---     \u2506 ---                       \u2506 ---         \u2506 ---        \u2506 ---           \u2506 ---        \u2502\n\u2502 i64     \u2506 str                       \u2506 i64         \u2506 i64        \u2506 i64           \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6       \u2506 TAOJIANLONG_JASONBROKEN   \u2506 1397113510  \u2506 1536527864 \u2506 4             \u2506 0          \u2502\n\u2502 17      \u2506 SSAIG_JASONBROKEN         \u2506 1397113544  \u2506 1536527864 \u2506 1             \u2506 0          \u2502\n\u2502 19      \u2506 FDBVFDSSDGFDS_JASONBROKEN \u2506 1397113552  \u2506 1536527864 \u2506 3             \u2506 0          \u2502\n\u2502 37      \u2506 IHATEWHOWEARE_JASONBROKEN \u2506 1397113636  \u2506 1536527864 \u2506 61            \u2506 0          \u2502\n\u2502 \u2026       \u2506 \u2026                         \u2506 \u2026           \u2506 \u2026          \u2506 \u2026             \u2506 \u2026          \u2502\n\u2502 1229384 \u2506 DSFOX                     \u2506 1163177415  \u2506 1536497412 \u2506 44411         \u2506 7917       \u2502\n\u2502 1229459 \u2506 NEOCARTY                  \u2506 1163177859  \u2506 1536533090 \u2506 40            \u2506 0          \u2502\n\u2502 1229587 \u2506 TEHSMA                    \u2506 1163178847  \u2506 1536497412 \u2506 14794         \u2506 5707       \u2502\n\u2502 1229621 \u2506 JEREMYLOW                 \u2506 1163179075  \u2506 1536497412 \u2506 411           \u2506 1063       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Above we see that from the 10 million rows there are 14,029 rows that match our predicate.</p> <p>With the default <code>collect</code> method Polars processes all of your data as one batch. This means that all the data has to fit into your available memory at the point of peak memory usage in your query.</p> <p>Reusing <code>LazyFrame</code> objects</p> <p>Remember that <code>LazyFrame</code>s are query plans i.e. a promise on computation and is not guaranteed to cache common subplans. This means that every time you reuse it in separate downstream queries after it is defined, it is computed all over again. If you define an operation on a <code>LazyFrame</code> that doesn't maintain row order (such as a <code>group_by</code>), then the order will also change every time it is run. To avoid this, use <code>maintain_order=True</code> arguments for such operations.</p>"},{"location":"user-guide/lazy/execution/#execution-on-larger-than-memory-data","title":"Execution on larger-than-memory data","text":"<p>If your data requires more memory than you have available Polars may be able to process the data in batches using streaming mode. To use streaming mode you simply pass the <code>streaming=True</code> argument to <code>collect</code></p>  Python <p> <code>scan_csv</code> \u00b7 <code>collect</code> <pre><code>q5 = (\n    pl.scan_csv(f\"docs/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n    .collect(streaming=True)\n)\n</code></pre></p> <p>We look at streaming in more detail here.</p>"},{"location":"user-guide/lazy/execution/#execution-on-a-partial-dataset","title":"Execution on a partial dataset","text":"<p>While you're writing, optimizing or checking your query on a large dataset, querying all available data may lead to a slow development process.</p> <p>You can instead execute the query with the <code>.fetch</code> method. The <code>.fetch</code> method takes a parameter <code>n_rows</code> and tries to 'fetch' that number of rows at the data source. The number of rows cannot be guaranteed, however, as the lazy API does not count how many rows there are at each stage of the query.</p> <p>Here we \"fetch\" 100 rows from the source file and apply the predicates.</p>  Python <p> <code>scan_csv</code> \u00b7 <code>collect</code> \u00b7 <code>fetch</code> <pre><code>q9 = (\n    pl.scan_csv(f\"docs/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n    .fetch(n_rows=int(100))\n)\n</code></pre></p> <pre><code>shape: (27, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id    \u2506 name                      \u2506 created_utc \u2506 updated_on \u2506 comment_karma \u2506 link_karma \u2502\n\u2502 ---   \u2506 ---                       \u2506 ---         \u2506 ---        \u2506 ---           \u2506 ---        \u2502\n\u2502 i64   \u2506 str                       \u2506 i64         \u2506 i64        \u2506 i64           \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6     \u2506 TAOJIANLONG_JASONBROKEN   \u2506 1397113510  \u2506 1536527864 \u2506 4             \u2506 0          \u2502\n\u2502 17    \u2506 SSAIG_JASONBROKEN         \u2506 1397113544  \u2506 1536527864 \u2506 1             \u2506 0          \u2502\n\u2502 19    \u2506 FDBVFDSSDGFDS_JASONBROKEN \u2506 1397113552  \u2506 1536527864 \u2506 3             \u2506 0          \u2502\n\u2502 37    \u2506 IHATEWHOWEARE_JASONBROKEN \u2506 1397113636  \u2506 1536527864 \u2506 61            \u2506 0          \u2502\n\u2502 \u2026     \u2506 \u2026                         \u2506 \u2026           \u2506 \u2026          \u2506 \u2026             \u2506 \u2026          \u2502\n\u2502 77763 \u2506 LUNCHY                    \u2506 1137599510  \u2506 1536528275 \u2506 65            \u2506 0          \u2502\n\u2502 77765 \u2506 COMPOSTELLAS              \u2506 1137474000  \u2506 1536528276 \u2506 6             \u2506 0          \u2502\n\u2502 77766 \u2506 GENERICBOB                \u2506 1137474000  \u2506 1536528276 \u2506 291           \u2506 14         \u2502\n\u2502 77768 \u2506 TINHEADNED                \u2506 1139665457  \u2506 1536497404 \u2506 4434          \u2506 103        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/lazy/optimizations/","title":"Optimizations","text":"<p>If you use Polars' lazy API, Polars will run several optimizations on your query. Some of them are executed up front, others are determined just in time as the materialized data comes in.</p> <p>Here is a non-complete overview of optimizations done by polars, what they do and how often they run.</p> Optimization Explanation runs Predicate pushdown Applies filters as early as possible/ at scan level. 1 time Projection pushdown Select only the columns that are needed at the scan level. 1 time Slice pushdown Only load the required slice from the scan level. Don't materialize sliced outputs (e.g. join.head(10)). 1 time Common subplan elimination Cache subtrees/file scans that are used by multiple subtrees in the query plan. 1 time Simplify expressions Various optimizations, such as constant folding and replacing expensive operations with faster alternatives. until fixed point Join ordering Estimates the branches of joins that should be executed first in order to reduce memory pressure. 1 time Type coercion Coerce types such that operations succeed and run on minimal required memory. until fixed point Cardinality estimation Estimates cardinality in order to determine optimal group by strategy. 0/n times; dependent on query"},{"location":"user-guide/lazy/query-plan/","title":"Query plan","text":"<p>For any lazy query Polars has both:</p> <ul> <li>a non-optimized plan with the set of steps code as we provided it and</li> <li>an optimized plan with changes made by the query optimizer</li> </ul> <p>We can understand both the non-optimized and optimized query plans with visualization and by printing them as text.</p> <p>Below we consider the following query:</p>  Python <pre><code>q1 = (\n    pl.scan_csv(\"docs/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n)\n</code></pre> <p></p>"},{"location":"user-guide/lazy/query-plan/#non-optimized-query-plan","title":"Non-optimized query plan","text":""},{"location":"user-guide/lazy/query-plan/#graphviz-visualization","title":"Graphviz visualization","text":"<p>To create visualizations of the query plan, Graphviz should be installed and added to your PATH.</p> <p>First we visualize the non-optimized plan by setting <code>optimized=False</code>.</p>  Python <p> <code>show_graph</code> <pre><code>q1.show_graph(optimized=False)\n</code></pre></p> <p></p> <p>The query plan visualization should be read from bottom to top. In the visualization:</p> <ul> <li>each box corresponds to a stage in the query plan</li> <li>the <code>sigma</code> stands for <code>SELECTION</code> and indicates any filter conditions</li> <li>the <code>pi</code> stands for <code>PROJECTION</code> and indicates choosing a subset of columns</li> </ul>"},{"location":"user-guide/lazy/query-plan/#printed-query-plan","title":"Printed query plan","text":"<p>We can also print the non-optimized plan with <code>explain(optimized=False)</code></p>  Python <p> <code>explain</code> <pre><code>q1.explain(optimized=False)\n</code></pre></p> <p></p> <pre><code>FILTER [(col(\"comment_karma\")) &gt; (0)] FROM WITH_COLUMNS:\n [col(\"name\").str.uppercase()]\n\n    CSV SCAN data/reddit.csv\n    PROJECT */6 COLUMNS\n</code></pre> <p>The printed plan should also be read from bottom to top. This non-optimized plan is roughly equal to:</p> <ul> <li>read from the <code>data/reddit.csv</code> file</li> <li>read all 6 columns (where the * wildcard in PROJECT */6 COLUMNS means take all columns)</li> <li>transform the <code>name</code> column to uppercase</li> <li>apply a filter on the <code>comment_karma</code> column</li> </ul>"},{"location":"user-guide/lazy/query-plan/#optimized-query-plan","title":"Optimized query plan","text":"<p>Now we visualize the optimized plan with <code>show_graph</code>.</p>  Python <p> <code>show_graph</code> <pre><code>q1.show_graph()\n</code></pre></p> <p></p> <p>We can also print the optimized plan with <code>explain</code></p>  Python <p> <code>explain</code> <pre><code>q1.explain()\n</code></pre></p> <pre><code> WITH_COLUMNS:\n [col(\"name\").str.uppercase()]\n\n    CSV SCAN data/reddit.csv\n    PROJECT */6 COLUMNS\n    SELECTION: [(col(\"comment_karma\")) &gt; (0)]\n</code></pre> <p>The optimized plan is to:</p> <ul> <li>read the data from the Reddit CSV</li> <li>apply the filter on the <code>comment_karma</code> column while the CSV is being read line-by-line</li> <li>transform the <code>name</code> column to uppercase</li> </ul> <p>In this case the query optimizer has identified that the <code>filter</code> can be applied while the CSV is read from disk rather than reading the whole file into memory and then applying the filter. This optimization is called Predicate Pushdown.</p>"},{"location":"user-guide/lazy/schemas/","title":"Schema","text":"<p>The schema of a Polars <code>DataFrame</code> or <code>LazyFrame</code> sets out the names of the columns and their datatypes. You can see the schema with the <code>.schema</code> method on a <code>DataFrame</code> or <code>LazyFrame</code></p>  Python <p> <code>DataFrame</code> \u00b7 <code>lazy</code> <pre><code>q3 = pl.DataFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [0, 1, 2]}).lazy()\n\nprint(q3.schema)\n</code></pre></p> <pre><code>OrderedDict({'foo': String, 'bar': Int64})\n</code></pre> <p>The schema plays an important role in the lazy API.</p>"},{"location":"user-guide/lazy/schemas/#type-checking-in-the-lazy-api","title":"Type checking in the lazy API","text":"<p>One advantage of the lazy API is that Polars will check the schema before any data is processed. This check happens when you execute your lazy query.</p> <p>We see how this works in the following simple example where we call the <code>.round</code> expression on the integer <code>bar</code> column.</p>  Python <p> <code>lazy</code> \u00b7 <code>with_columns</code> <pre><code>q4 = (\n    pl.DataFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [0, 1, 2]})\n    .lazy()\n    .with_columns(pl.col(\"bar\").round(0))\n)\n</code></pre></p> <p>The <code>.round</code> expression is only valid for columns with a floating point dtype. Calling <code>.round</code> on an integer column means the operation will raise an <code>InvalidOperationError</code> when we evaluate the query with <code>collect</code>. This schema check happens before the data is processed when we call <code>collect</code>.</p>  Python <pre><code>try:\n    print(q4.collect())\nexcept Exception as e:\n    print(e)\n</code></pre> <pre><code>`round` operation not supported for dtype `i64`\n</code></pre> <p>If we executed this query in eager mode the error would only be found once the data had been processed in all earlier steps.</p> <p>When we execute a lazy query Polars checks for any potential <code>InvalidOperationError</code> before the time-consuming step of actually processing the data in the pipeline.</p>"},{"location":"user-guide/lazy/schemas/#the-lazy-api-must-know-the-schema","title":"The lazy API must know the schema","text":"<p>In the lazy API the Polars query optimizer must be able to infer the schema at every step of a query plan. This means that operations where the schema is not knowable in advance cannot be used with the lazy API.</p> <p>The classic example of an operation where the schema is not knowable in advance is a <code>.pivot</code> operation. In a <code>.pivot</code> the new column names come from data in one of the columns. As these column names cannot be known in advance a <code>.pivot</code> is not available in the lazy API.</p>"},{"location":"user-guide/lazy/schemas/#dealing-with-operations-not-available-in-the-lazy-api","title":"Dealing with operations not available in the lazy API","text":"<p>If your pipeline includes an operation that is not available in the lazy API it is normally best to:</p> <ul> <li>run the pipeline in lazy mode up until that point</li> <li>execute the pipeline with <code>.collect</code> to materialize a <code>DataFrame</code></li> <li>do the non-lazy operation on the <code>DataFrame</code></li> <li>convert the output back to a <code>LazyFrame</code> with <code>.lazy</code> and continue in lazy mode</li> </ul> <p>We show how to deal with a non-lazy operation in this example where we:</p> <ul> <li>create a simple <code>DataFrame</code></li> <li>convert it to a <code>LazyFrame</code> with <code>.lazy</code></li> <li>do a transformation using <code>.with_columns</code></li> <li>execute the query before the pivot with <code>.collect</code> to get a <code>DataFrame</code></li> <li>do the <code>.pivot</code> on the <code>DataFrame</code></li> <li>convert back in lazy mode</li> <li>do a <code>.filter</code></li> <li>finish by executing the query with <code>.collect</code> to get a <code>DataFrame</code></li> </ul>  Python <p> <code>collect</code> \u00b7 <code>pivot</code> \u00b7 <code>filter</code> <pre><code>lazy_eager_query = (\n    pl.DataFrame(\n        {\n            \"id\": [\"a\", \"b\", \"c\"],\n            \"month\": [\"jan\", \"feb\", \"mar\"],\n            \"values\": [0, 1, 2],\n        }\n    )\n    .lazy()\n    .with_columns((2 * pl.col(\"values\")).alias(\"double_values\"))\n    .collect()\n    .pivot(\n        index=\"id\", columns=\"month\", values=\"double_values\", aggregate_function=\"first\"\n    )\n    .lazy()\n    .filter(pl.col(\"mar\").is_null())\n    .collect()\n)\nprint(lazy_eager_query)\n</code></pre></p> <pre><code>shape: (2, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 jan  \u2506 feb  \u2506 mar  \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 0    \u2506 null \u2506 null \u2502\n\u2502 b   \u2506 null \u2506 2    \u2506 null \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/lazy/streaming/","title":"Streaming","text":"<p> Under Construction </p> <p>This section is still under development. Want to help out? Consider contributing and making a pull request to our repository. Please read our contributing guide on how to proceed.</p>"},{"location":"user-guide/lazy/using/","title":"Usage","text":"<p>With the lazy API, Polars doesn't run each query line-by-line but instead processes the full query end-to-end. To get the most out of Polars it is important that you use the lazy API because:</p> <ul> <li>the lazy API allows Polars to apply automatic query optimization with the query optimizer</li> <li>the lazy API allows you to work with larger than memory datasets using streaming</li> <li>the lazy API can catch schema errors before processing the data</li> </ul> <p>Here we see how to use the lazy API starting from either a file or an existing <code>DataFrame</code>.</p>"},{"location":"user-guide/lazy/using/#using-the-lazy-api-from-a-file","title":"Using the lazy API from a file","text":"<p>In the ideal case we would use the lazy API right from a file as the query optimizer may help us to reduce the amount of data we read from the file.</p> <p>We create a lazy query from the Reddit CSV data and apply some transformations.</p> <p>By starting the query with <code>pl.scan_csv</code> we are using the lazy API.</p>  Python <p> <code>scan_csv</code> \u00b7 <code>with_columns</code> \u00b7 <code>filter</code> \u00b7 <code>col</code> <pre><code>q1 = (\n    pl.scan_csv(f\"docs/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n)\n</code></pre></p> <p>A <code>pl.scan_</code> function is available for a number of file types including CSV, IPC, Parquet and JSON.</p> <p>In this query we tell Polars that we want to:</p> <ul> <li>load data from the Reddit CSV file</li> <li>convert the <code>name</code> column to uppercase</li> <li>apply a filter to the <code>comment_karma</code> column</li> </ul> <p>The lazy query will not be executed at this point. See this page on executing lazy queries for more on running lazy queries.</p>"},{"location":"user-guide/lazy/using/#using-the-lazy-api-from-a-dataframe","title":"Using the lazy API from a <code>DataFrame</code>","text":"<p>An alternative way to access the lazy API is to call <code>.lazy</code> on a <code>DataFrame</code> that has already been created in memory.</p>  Python <p> <code>lazy</code> <pre><code>q3 = pl.DataFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [0, 1, 2]}).lazy()\n</code></pre></p> <p>By calling <code>.lazy</code> we convert the <code>DataFrame</code> to a <code>LazyFrame</code>.</p>"},{"location":"user-guide/migration/pandas/","title":"Coming from Pandas","text":"<p>Here we set out the key points that anyone who has experience with pandas and wants to try Polars should know. We include both differences in the concepts the libraries are built on and differences in how you should write Polars code compared to pandas code.</p>"},{"location":"user-guide/migration/pandas/#differences-in-concepts-between-polars-and-pandas","title":"Differences in concepts between Polars and pandas","text":""},{"location":"user-guide/migration/pandas/#polars-does-not-have-a-multi-indexindex","title":"Polars does not have a multi-index/index","text":"<p>pandas gives a label to each row with an index. Polars does not use an index and each row is indexed by its integer position in the table.</p> <p>Polars aims to have predictable results and readable queries, as such we think an index does not help us reach that objective. We believe the semantics of a query should not change by the state of an index or a <code>reset_index</code> call.</p> <p>In Polars a DataFrame will always be a 2D table with heterogeneous data-types. The data-types may have nesting, but the table itself will not. Operations like resampling will be done by specialized functions or methods that act like 'verbs' on a table explicitly stating the columns that that 'verb' operates on. As such, it is our conviction that not having indices make things simpler, more explicit, more readable and less error-prone.</p> <p>Note that an 'index' data structure as known in databases will be used by Polars as an optimization technique.</p>"},{"location":"user-guide/migration/pandas/#polars-uses-apache-arrow-arrays-to-represent-data-in-memory-while-pandas-uses-numpy-arrays","title":"Polars uses Apache Arrow arrays to represent data in memory while pandas uses NumPy arrays","text":"<p>Polars represents data in memory with Arrow arrays while pandas represents data in memory with NumPy arrays. Apache Arrow is an emerging standard for in-memory columnar analytics that can accelerate data load times, reduce memory usage and accelerate calculations.</p> <p>Polars can convert data to NumPy format with the <code>to_numpy</code> method.</p>"},{"location":"user-guide/migration/pandas/#polars-has-more-support-for-parallel-operations-than-pandas","title":"Polars has more support for parallel operations than pandas","text":"<p>Polars exploits the strong support for concurrency in Rust to run many operations in parallel. While some operations in pandas are multi-threaded the core of the library is single-threaded and an additional library such as <code>Dask</code> must be used to parallelize operations.</p>"},{"location":"user-guide/migration/pandas/#polars-can-lazily-evaluate-queries-and-apply-query-optimization","title":"Polars can lazily evaluate queries and apply query optimization","text":"<p>Eager evaluation is when code is evaluated as soon as you run the code. Lazy evaluation is when running a line of code means that the underlying logic is added to a query plan rather than being evaluated.</p> <p>Polars supports eager evaluation and lazy evaluation whereas pandas only supports eager evaluation. The lazy evaluation mode is powerful because Polars carries out automatic query optimization when it examines the query plan and looks for ways to accelerate the query or reduce memory usage.</p> <p><code>Dask</code> also supports lazy evaluation when it generates a query plan. However, <code>Dask</code> does not carry out query optimization on the query plan.</p>"},{"location":"user-guide/migration/pandas/#key-syntax-differences","title":"Key syntax differences","text":"<p>Users coming from pandas generally need to know one thing...</p> <pre><code>polars != pandas\n</code></pre> <p>If your Polars code looks like it could be pandas code, it might run, but it likely runs slower than it should.</p> <p>Let's go through some typical pandas code and see how we might rewrite it in Polars.</p>"},{"location":"user-guide/migration/pandas/#selecting-data","title":"Selecting data","text":"<p>As there is no index in Polars there is no <code>.loc</code> or <code>iloc</code> method in Polars - and there is also no <code>SettingWithCopyWarning</code> in Polars.</p> <p>However, the best way to select data in Polars is to use the expression API. For example, if you want to select a column in pandas, you can do one of the following:</p> <pre><code>df['a']\ndf.loc[:,'a']\n</code></pre> <p>but in Polars you would use the <code>.select</code> method:</p> <pre><code>df.select('a')\n</code></pre> <p>If you want to select rows based on the values then in Polars you use the <code>.filter</code> method:</p> <pre><code>df.filter(pl.col('a') &lt; 10)\n</code></pre> <p>As noted in the section on expressions below, Polars can run operations in <code>.select</code> and <code>filter</code> in parallel and Polars can carry out query optimization on the full set of data selection criteria.</p>"},{"location":"user-guide/migration/pandas/#be-lazy","title":"Be lazy","text":"<p>Working in lazy evaluation mode is straightforward and should be your default in Polars as the lazy mode allows Polars to do query optimization.</p> <p>We can run in lazy mode by either using an implicitly lazy function (such as <code>scan_csv</code>) or explicitly using the <code>lazy</code> method.</p> <p>Take the following simple example where we read a CSV file from disk and do a group by. The CSV file has numerous columns but we just want to do a group by on one of the id columns (<code>id1</code>) and then sum by a value column (<code>v1</code>). In pandas this would be:</p> <pre><code>df = pd.read_csv(csv_file, usecols=['id1','v1'])\ngrouped_df = df.loc[:,['id1','v1']].groupby('id1').sum('v1')\n</code></pre> <p>In Polars you can build this query in lazy mode with query optimization and evaluate it by replacing the eager pandas function <code>read_csv</code> with the implicitly lazy Polars function <code>scan_csv</code>:</p> <pre><code>df = pl.scan_csv(csv_file)\ngrouped_df = df.group_by('id1').agg(pl.col('v1').sum()).collect()\n</code></pre> <p>Polars optimizes this query by identifying that only the <code>id1</code> and <code>v1</code> columns are relevant and so will only read these columns from the CSV. By calling the <code>.collect</code> method at the end of the second line we instruct Polars to eagerly evaluate the query.</p> <p>If you do want to run this query in eager mode you can just replace <code>scan_csv</code> with <code>read_csv</code> in the Polars code.</p> <p>Read more about working with lazy evaluation in the lazy API section.</p>"},{"location":"user-guide/migration/pandas/#express-yourself","title":"Express yourself","text":"<p>A typical pandas script consists of multiple data transformations that are executed sequentially. However, in Polars these transformations can be executed in parallel using expressions.</p>"},{"location":"user-guide/migration/pandas/#column-assignment","title":"Column assignment","text":"<p>We have a dataframe <code>df</code> with a column called <code>value</code>. We want to add two new columns, a column called <code>tenXValue</code> where the <code>value</code> column is multiplied by 10 and a column called <code>hundredXValue</code> where the <code>value</code> column is multiplied by 100.</p> <p>In pandas this would be:</p> <pre><code>df.assign(\n    tenXValue=lambda df_: df_.value * 10,\n    hundredXValue=lambda df_: df_.value * 100\n)\n</code></pre> <p>These column assignments are executed sequentially.</p> <p>In Polars we add columns to <code>df</code> using the <code>.with_columns</code> method:</p> <pre><code>df.with_columns(\n    tenXValue=pl.col(\"value\") * 10,\n    hundredXValue=pl.col(\"value\") * 100,\n)\n</code></pre> <p>These column assignments are executed in parallel.</p>"},{"location":"user-guide/migration/pandas/#column-assignment-based-on-predicate","title":"Column assignment based on predicate","text":"<p>In this case we have a dataframe <code>df</code> with columns <code>a</code>,<code>b</code> and <code>c</code>. We want to re-assign the values in column <code>a</code> based on a condition. When the value in column <code>c</code> is equal to 2 then we replace the value in <code>a</code> with the value in <code>b</code>.</p> <p>In pandas this would be:</p> <pre><code>df.assign(a=lambda df_: df_.a.where(df_.c != 2, df_.b))\n</code></pre> <p>while in Polars this would be:</p> <pre><code>df.with_columns(\n    pl.when(pl.col(\"c\") == 2)\n    .then(pl.col(\"b\"))\n    .otherwise(pl.col(\"a\")).alias(\"a\")\n)\n</code></pre> <p>Polars can compute every branch of an <code>if -&gt; then -&gt; otherwise</code> in parallel. This is valuable, when the branches get more expensive to compute.</p>"},{"location":"user-guide/migration/pandas/#filtering","title":"Filtering","text":"<p>We want to filter the dataframe <code>df</code> with housing data based on some criteria.</p> <p>In pandas you filter the dataframe by passing Boolean expressions to the <code>query</code> method:</p> <pre><code>df.query(\"m2_living &gt; 2500 and price &lt; 300000\")\n</code></pre> <p>or by directly evaluating a mask:</p> <pre><code>df[(df[\"m2_living\"] &gt; 2500) &amp; (df[\"price\"] &lt; 300000)]\n</code></pre> <p>while in Polars you call the <code>filter</code> method:</p> <pre><code>df.filter(\n    (pl.col(\"m2_living\") &gt; 2500) &amp; (pl.col(\"price\") &lt; 300000)\n)\n</code></pre> <p>The query optimizer in Polars can also detect if you write multiple filters separately and combine them into a single filter in the optimized plan.</p>"},{"location":"user-guide/migration/pandas/#pandas-transform","title":"pandas transform","text":"<p>The pandas documentation demonstrates an operation on a group by called <code>transform</code>. In this case we have a dataframe <code>df</code> and we want a new column showing the number of rows in each group.</p> <p>In pandas we have:</p> <pre><code>df = pd.DataFrame({\n    \"c\": [1, 1, 1, 2, 2, 2, 2],\n    \"type\": [\"m\", \"n\", \"o\", \"m\", \"m\", \"n\", \"n\"],\n})\n\ndf[\"size\"] = df.groupby(\"c\")[\"type\"].transform(len)\n</code></pre> <p>Here pandas does a group by on <code>\"c\"</code>, takes column <code>\"type\"</code>, computes the group length and then joins the result back to the original <code>DataFrame</code> producing:</p> <pre><code>   c type size\n0  1    m    3\n1  1    n    3\n2  1    o    3\n3  2    m    4\n4  2    m    4\n5  2    n    4\n6  2    n    4\n</code></pre> <p>In Polars the same can be achieved with <code>window</code> functions:</p> <pre><code>df.with_columns(\n    pl.col(\"type\").count().over(\"c\").alias(\"size\")\n)\n</code></pre> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 c   \u2506 type \u2506 size \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 i64 \u2506 str  \u2506 u32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 m    \u2506 3    \u2502\n\u2502 1   \u2506 n    \u2506 3    \u2502\n\u2502 1   \u2506 o    \u2506 3    \u2502\n\u2502 2   \u2506 m    \u2506 4    \u2502\n\u2502 2   \u2506 m    \u2506 4    \u2502\n\u2502 2   \u2506 n    \u2506 4    \u2502\n\u2502 2   \u2506 n    \u2506 4    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Because we can store the whole operation in a single expression, we can combine several <code>window</code> functions and even combine different groups!</p> <p>Polars will cache window expressions that are applied over the same group, so storing them in a single <code>with_columns</code> is both convenient and optimal. In the following example we look at a case where we are calculating group statistics over <code>\"c\"</code> twice:</p> <pre><code>df.with_columns(\n    pl.col(\"c\").count().over(\"c\").alias(\"size\"),\n    pl.col(\"c\").sum().over(\"type\").alias(\"sum\"),\n    pl.col(\"type\").reverse().over(\"c\").alias(\"reverse_type\")\n)\n</code></pre> <pre><code>shape: (7, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 c   \u2506 type \u2506 size \u2506 sum \u2506 reverse_type \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 --- \u2506 ---          \u2502\n\u2502 i64 \u2506 str  \u2506 u32  \u2506 i64 \u2506 str          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 m    \u2506 3    \u2506 5   \u2506 o            \u2502\n\u2502 1   \u2506 n    \u2506 3    \u2506 5   \u2506 n            \u2502\n\u2502 1   \u2506 o    \u2506 3    \u2506 1   \u2506 m            \u2502\n\u2502 2   \u2506 m    \u2506 4    \u2506 5   \u2506 n            \u2502\n\u2502 2   \u2506 m    \u2506 4    \u2506 5   \u2506 n            \u2502\n\u2502 2   \u2506 n    \u2506 4    \u2506 5   \u2506 m            \u2502\n\u2502 2   \u2506 n    \u2506 4    \u2506 5   \u2506 m            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/migration/pandas/#missing-data","title":"Missing data","text":"<p>pandas uses <code>NaN</code> and/or <code>None</code> values to indicate missing values depending on the dtype of the column. In addition the behaviour in pandas varies depending on whether the default dtypes or optional nullable arrays are used. In Polars missing data corresponds to a <code>null</code> value for all data types.</p> <p>For float columns Polars permits the use of <code>NaN</code> values. These <code>NaN</code> values are not considered to be missing data but instead a special floating point value.</p> <p>In pandas an integer column with missing values is cast to be a float column with <code>NaN</code> values for the missing values (unless using optional nullable integer dtypes). In Polars any missing values in an integer column are simply <code>null</code> values and the column remains an integer column.</p> <p>See the missing data section for more details.</p>"},{"location":"user-guide/migration/pandas/#pipe-littering","title":"Pipe littering","text":"<p>A common usage in pandas is utilizing <code>pipe</code> to apply some function to a <code>DataFrame</code>. Copying this coding style to Polars is unidiomatic and leads to suboptimal query plans.</p> <p>The snippet below shows a common pattern in pandas.</p> <pre><code>def add_foo(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df[\"foo\"] = ...\n    return df\n\ndef add_bar(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df[\"bar\"] = ...\n    return df\n\n\ndef add_ham(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df[\"ham\"] = ...\n    return df\n\n(df\n .pipe(add_foo)\n .pipe(add_bar)\n .pipe(add_ham)\n)\n</code></pre> <p>If we do this in polars, we would create 3 <code>with_columns</code> contexts, that forces Polars to run the 3 pipes sequentially, utilizing zero parallelism.</p> <p>The way to get similar abstractions in polars is creating functions that create expressions. The snippet below creates 3 expressions that run on a single context and thus are allowed to run in parallel.</p> <pre><code>def get_foo(input_column: str) -&gt; pl.Expr:\n    return pl.col(input_column).some_computation().alias(\"foo\")\n\ndef get_bar(input_column: str) -&gt; pl.Expr:\n    return pl.col(input_column).some_computation().alias(\"bar\")\n\ndef get_ham(input_column: str) -&gt; pl.Expr:\n    return pl.col(input_column).some_computation().alias(\"ham\")\n\n# This single context will run all 3 expressions in parallel\ndf.with_columns(\n    get_ham(\"col_a\"),\n    get_bar(\"col_b\"),\n    get_foo(\"col_c\"),\n)\n</code></pre> <p>If you need the schema in the functions that generate the expressions, you can utilize a single <code>pipe</code>:</p> <pre><code>from collections import OrderedDict\n\ndef get_foo(input_column: str, schema: OrderedDict) -&gt; pl.Expr:\n    if \"some_col\" in schema:\n        # branch_a\n        ...\n    else:\n        # branch b\n        ...\n\ndef get_bar(input_column: str, schema: OrderedDict) -&gt; pl.Expr:\n    if \"some_col\" in schema:\n        # branch_a\n        ...\n    else:\n        # branch b\n        ...\n\ndef get_ham(input_column: str) -&gt; pl.Expr:\n    return pl.col(input_column).some_computation().alias(\"ham\")\n\n# Use pipe (just once) to get hold of the schema of the LazyFrame.\nlf.pipe(lambda lf: lf.with_columns(\n    get_ham(\"col_a\"),\n    get_bar(\"col_b\", lf.schema),\n    get_foo(\"col_c\", lf.schema),\n)\n</code></pre> <p>Another benefit of writing functions that return expressions, is that these functions are composable as expressions can be chained and partially applied, leading to much more flexibility in the design.</p>"},{"location":"user-guide/migration/spark/","title":"Coming from Apache Spark","text":""},{"location":"user-guide/migration/spark/#column-based-api-vs-row-based-api","title":"Column-based API vs. Row-based API","text":"<p>Whereas the <code>Spark</code> <code>DataFrame</code> is analogous to a collection of rows, a Polars <code>DataFrame</code> is closer to a collection of columns. This means that you can combine columns in Polars in ways that are not possible in <code>Spark</code>, because <code>Spark</code> preserves the relationship of the data in each row.</p> <p>Consider this sample dataset:</p> <pre><code>import polars as pl\n\ndf = pl.DataFrame({\n    \"foo\": [\"a\", \"b\", \"c\", \"d\", \"d\"],\n    \"bar\": [1, 2, 3, 4, 5],\n})\n\ndfs = spark.createDataFrame(\n    [\n        (\"a\", 1),\n        (\"b\", 2),\n        (\"c\", 3),\n        (\"d\", 4),\n        (\"d\", 5),\n    ],\n    schema=[\"foo\", \"bar\"],\n)\n</code></pre>"},{"location":"user-guide/migration/spark/#example-1-combining-head-and-sum","title":"Example 1: Combining <code>head</code> and <code>sum</code>","text":"<p>In Polars you can write something like this:</p> <pre><code>df.select(\n    pl.col(\"foo\").sort().head(2),\n    pl.col(\"bar\").filter(pl.col(\"foo\") == \"d\").sum()\n)\n</code></pre> <p>Output:</p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 9   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 b   \u2506 9   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The expressions on columns <code>foo</code> and <code>bar</code> are completely independent. Since the expression on <code>bar</code> returns a single value, that value is repeated for each value output by the expression on <code>foo</code>. But <code>a</code> and <code>b</code> have no relation to the data that produced the sum of <code>9</code>.</p> <p>To do something similar in <code>Spark</code>, you'd need to compute the sum separately and provide it as a literal:</p> <pre><code>from pyspark.sql.functions import col, sum, lit\n\nbar_sum = (\n    dfs\n    .where(col(\"foo\") == \"d\")\n    .groupBy()\n    .agg(sum(col(\"bar\")))\n    .take(1)[0][0]\n)\n\n(\n    dfs\n    .orderBy(\"foo\")\n    .limit(2)\n    .withColumn(\"bar\", lit(bar_sum))\n    .show()\n)\n</code></pre> <p>Output:</p> <pre><code>+---+---+\n|foo|bar|\n+---+---+\n|  a|  9|\n|  b|  9|\n+---+---+\n</code></pre>"},{"location":"user-guide/migration/spark/#example-2-combining-two-heads","title":"Example 2: Combining Two <code>head</code>s","text":"<p>In Polars you can combine two different <code>head</code> expressions on the same DataFrame, provided that they return the same number of values.</p> <pre><code>df.select(\n    pl.col(\"foo\").sort().head(2),\n    pl.col(\"bar\").sort(descending=True).head(2),\n)\n</code></pre> <p>Output:</p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 5   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 b   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Again, the two <code>head</code> expressions here are completely independent, and the pairing of <code>a</code> to <code>5</code> and <code>b</code> to <code>4</code> results purely from the juxtaposition of the two columns output by the expressions.</p> <p>To accomplish something similar in <code>Spark</code>, you would need to generate an artificial key that enables you to join the values in this way.</p> <pre><code>from pyspark.sql import Window\nfrom pyspark.sql.functions import row_number\n\nfoo_dfs = (\n    dfs\n    .withColumn(\n        \"rownum\",\n        row_number().over(Window.orderBy(\"foo\"))\n    )\n)\n\nbar_dfs = (\n    dfs\n    .withColumn(\n        \"rownum\",\n        row_number().over(Window.orderBy(col(\"bar\").desc()))\n    )\n)\n\n(\n    foo_dfs.alias(\"foo\")\n    .join(bar_dfs.alias(\"bar\"), on=\"rownum\")\n    .select(\"foo.foo\", \"bar.bar\")\n    .limit(2)\n    .show()\n)\n</code></pre> <p>Output:</p> <pre><code>+---+---+\n|foo|bar|\n+---+---+\n|  a|  5|\n|  b|  4|\n+---+---+\n</code></pre>"},{"location":"user-guide/misc/comparison/","title":"Comparison with other tools","text":"<p>These are several libraries and tools that share similar functionalities with Polars. This often leads to questions from data experts about what the differences are. Below is a short comparison between some of the more popular data processing tools and Polars, to help data experts make a deliberate decision on which tool to use.</p> <p>You can find performance benchmarks (h2oai benchmark) of these tools here: Polars blog post or a more recent benchmark done by DuckDB</p>"},{"location":"user-guide/misc/comparison/#pandas","title":"Pandas","text":"<p>Pandas stands as a widely-adopted and comprehensive tool in Python data analysis, renowned for its rich feature set and strong community support. However, due to its single threaded nature, it can struggle with performance and memory usage on medium and large datasets.</p> <p>In contrast, Polars is optimised for high-performance multithreaded computing on single nodes, providing significant improvements in speed and memory efficiency, particularly for medium to large data operations. Its more composable and stricter API results in greater expressiveness and fewer schema-related bugs.</p>"},{"location":"user-guide/misc/comparison/#dask","title":"Dask","text":"<p>Dask extends Pandas' capabilities to large, distributed datasets. Dask mimics Pandas' API, offering a familiar environment for Pandas users, but with the added benefit of parallel and distributed computing.</p> <p>While Dask excels at scaling Pandas workflows across clusters, it only supports a subset of the Pandas API and therefore cannot be used for all use cases. Polars offers a more versatile API that delivers strong performance within the constraints of a single node.</p> <p>The choice between Dask and Polars often comes down to familiarity with the Pandas API and the need for distributed processing for extremely large datasets versus the need for efficiency and speed in a vertically scaled environment for a wide range of use cases.</p>"},{"location":"user-guide/misc/comparison/#modin","title":"Modin","text":"<p>Similar to Dask. In 2023, Snowflake acquired Ponder, the organisation that maintains Modin.</p>"},{"location":"user-guide/misc/comparison/#spark","title":"Spark","text":"<p>Spark (specifically PySpark) represents a different approach to large-scale data processing. While Polars has an optimised performance for single-node environments, Spark is designed for distributed data processing across clusters, making it suitable for extremely large datasets.</p> <p>However, Spark's distributed nature can introduce complexity and overhead, especially for small datasets and tasks that can run on a single machine. Another consideration is collaboration between data scientists and engineers. As they typically work with different tools (Pandas and Pyspark), refactoring is often required by engineers to deploy data scientists' data processing pipelines. Polars offers a single syntax that, due to vertical scaling, works in local environments and on a single machine in the cloud.</p> <p>The choice between Polars and Spark often depends on the scale of data and the specific requirements of the processing task. If you need to process TBs of data, Spark is a better choice.</p>"},{"location":"user-guide/misc/comparison/#duckdb","title":"DuckDB","text":"<p>Polars and DuckDB have many similarities. However, DuckDB is focused on providing an in-process SQL OLAP database management system, while Polars is focused on providing a scalable <code>DataFrame</code> interface to many languages. The different front-ends lead to different optimisation strategies and different algorithm prioritisation. The interoperability between both is zero-copy. DuckDB offers a guide on how to integrate with Polars.</p>"},{"location":"user-guide/misc/multiprocessing/","title":"Multiprocessing","text":"<p>TLDR: if you find that using Python's built-in <code>multiprocessing</code> module together with Polars results in a Polars error about multiprocessing methods, you should make sure you are using <code>spawn</code>, not <code>fork</code>, as the starting method:</p>  Python <pre><code>from multiprocessing import get_context\n\n\ndef my_fun(s):\n    print(s)\n\n\nwith get_context(\"spawn\").Pool() as pool:\n    pool.map(my_fun, [\"input1\", \"input2\", ...])\n</code></pre>"},{"location":"user-guide/misc/multiprocessing/#when-not-to-use-multiprocessing","title":"When not to use multiprocessing","text":"<p>Before we dive into the details, it is important to emphasize that Polars has been built from the start to use all your CPU cores. It does this by executing computations which can be done in parallel in separate threads. For example, requesting two expressions in a <code>select</code> statement can be done in parallel, with the results only being combined at the end. Another example is aggregating a value within groups using <code>group_by().agg(&lt;expr&gt;)</code>, each group can be evaluated separately. It is very unlikely that the <code>multiprocessing</code> module can improve your code performance in these cases.</p> <p>See the optimizations section for more optimizations.</p>"},{"location":"user-guide/misc/multiprocessing/#when-to-use-multiprocessing","title":"When to use multiprocessing","text":"<p>Although Polars is multithreaded, other libraries may be single-threaded. When the other library is the bottleneck, and the problem at hand is parallelizable, it makes sense to use multiprocessing to gain a speed up.</p>"},{"location":"user-guide/misc/multiprocessing/#the-problem-with-the-default-multiprocessing-config","title":"The problem with the default multiprocessing config","text":""},{"location":"user-guide/misc/multiprocessing/#summary","title":"Summary","text":"<p>The Python multiprocessing documentation lists the three methods to create a process pool:</p> <ol> <li>spawn</li> <li>fork</li> <li>forkserver</li> </ol> <p>The description of fork is (as of 2022-10-15):</p> <p>The parent process uses os.fork() to fork the Python interpreter. The child process, when it begins, is effectively identical to the parent process. All resources of the parent are inherited by the child process. Note that safely forking a multithreaded process is problematic.</p> <p>Available on Unix only. The default on Unix.</p> <p>The short summary is: Polars is multithreaded as to provide strong performance out-of-the-box. Thus, it cannot be combined with <code>fork</code>. If you are on Unix (Linux, BSD, etc), you are using <code>fork</code>, unless you explicitly override it.</p> <p>The reason you may not have encountered this before is that pure Python code, and most Python libraries, are (mostly) single threaded. Alternatively, you are on Windows or MacOS, on which <code>fork</code> is not even available as a method (for MacOS it was up to Python 3.7).</p> <p>Thus one should use <code>spawn</code>, or <code>forkserver</code>, instead. <code>spawn</code> is available on all platforms and the safest choice, and hence the recommended method.</p>"},{"location":"user-guide/misc/multiprocessing/#example","title":"Example","text":"<p>The problem with <code>fork</code> is in the copying of the parent's process. Consider the example below, which is a slightly modified example posted on the Polars issue tracker:</p>  Python <pre><code>import multiprocessing\nimport polars as pl\n\n\ndef test_sub_process(df: pl.DataFrame, job_id):\n    df_filtered = df.filter(pl.col(\"a\") &gt; 0)\n    print(f\"Filtered (job_id: {job_id})\", df_filtered, sep=\"\\n\")\n\n\ndef create_dataset():\n    return pl.DataFrame({\"a\": [0, 2, 3, 4, 5], \"b\": [0, 4, 5, 56, 4]})\n\n\ndef setup():\n    # some setup work\n    df = create_dataset()\n    df.write_parquet(\"/tmp/test.parquet\")\n\n\ndef main():\n    test_df = pl.read_parquet(\"/tmp/test.parquet\")\n\n    for i in range(0, 5):\n        proc = multiprocessing.get_context(\"spawn\").Process(\n            target=test_sub_process, args=(test_df, i)\n        )\n        proc.start()\n        proc.join()\n\n        print(f\"Executed sub process {i}\")\n\n\nif __name__ == \"__main__\":\n    setup()\n    main()\n</code></pre> <p>Using <code>fork</code> as the method, instead of <code>spawn</code>, will cause a dead lock. Please note: Polars will not even start and raise the error on multiprocessing method being set wrong, but if the check had not been there, the deadlock would exist.</p> <p>The fork method is equivalent to calling <code>os.fork()</code>, which is a system call as defined in the POSIX standard:</p> <p>A process shall be created with a single thread. If a multi-threaded process calls fork(), the new process shall contain a replica of the calling thread and its entire address space, possibly including the states of mutexes and other resources. Consequently, to avoid errors, the child process may only execute async-signal-safe operations until such time as one of the exec functions is called.</p> <p>In contrast, <code>spawn</code> will create a completely new fresh Python interpreter, and not inherit the state of mutexes.</p> <p>So what happens in the code example? For reading the file with <code>pl.read_parquet</code> the file has to be locked. Then <code>os.fork()</code> is called, copying the state of the parent process, including mutexes. Thus all child processes will copy the file lock in an acquired state, leaving them hanging indefinitely waiting for the file lock to be released, which never happens.</p> <p>What makes debugging these issues tricky is that <code>fork</code> can work. Change the example to not having the call to <code>pl.read_parquet</code>:</p>  Python <pre><code>import multiprocessing\nimport polars as pl\n\n\ndef test_sub_process(df: pl.DataFrame, job_id):\n    df_filtered = df.filter(pl.col(\"a\") &gt; 0)\n    print(f\"Filtered (job_id: {job_id})\", df_filtered, sep=\"\\n\")\n\n\ndef create_dataset():\n    return pl.DataFrame({\"a\": [0, 2, 3, 4, 5], \"b\": [0, 4, 5, 56, 4]})\n\n\ndef main():\n    test_df = create_dataset()\n\n    for i in range(0, 5):\n        proc = multiprocessing.get_context(\"fork\").Process(\n            target=test_sub_process, args=(test_df, i)\n        )\n        proc.start()\n        proc.join()\n\n        print(f\"Executed sub process {i}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>This works fine. Therefore debugging these issues in larger code bases, i.e. not the small toy examples here, can be a real pain, as a seemingly unrelated change can break your multiprocessing code. In general, one should therefore never use the <code>fork</code> start method with multithreaded libraries unless there are very specific requirements that cannot be met otherwise.</p>"},{"location":"user-guide/misc/multiprocessing/#pros-and-cons-of-fork","title":"Pro's and cons of fork","text":"<p>Based on the example, you may think, why is <code>fork</code> available in Python to start with?</p> <p>First, probably because of historical reasons: <code>spawn</code> was added to Python in version 3.4, whilst <code>fork</code> has been part of Python from the 2.x series.</p> <p>Second, there are several limitations for <code>spawn</code> and <code>forkserver</code> that do not apply to <code>fork</code>, in particular all arguments should be pickable. See the Python multiprocessing docs for more information.</p> <p>Third, because it is faster to create new processes compared to <code>spawn</code>, as <code>spawn</code> is effectively <code>fork</code> + creating a brand new Python process without the locks by calling execv. Hence the warning in the Python docs that it is slower: there is more overhead to <code>spawn</code>. However, in almost all cases, one would like to use multiple processes to speed up computations that take multiple minutes or even hours, meaning the overhead is negligible in the grand scheme of things. And more importantly, it actually works in combination with multithreaded libraries.</p> <p>Fourth, <code>spawn</code> starts a new process, and therefore it requires code to be importable, in contrast to <code>fork</code>. In particular, this means that when using <code>spawn</code> the relevant code should not be in the global scope, such as in Jupyter notebooks or in plain scripts. Hence in the examples above, we define functions where we spawn within, and run those functions from a <code>__main__</code> clause. This is not an issue for typical projects, but during quick experimentation in notebooks it could fail.</p>"},{"location":"user-guide/misc/multiprocessing/#references","title":"References","text":"<ol> <li> <p>https://docs.python.org/3/library/multiprocessing.html</p> </li> <li> <p>https://pythonspeed.com/articles/python-multiprocessing/</p> </li> <li> <p>https://pubs.opengroup.org/onlinepubs/9699919799/functions/fork.html</p> </li> <li> <p>https://bnikolic.co.uk/blog/python/parallelism/2019/11/13/python-forkserver-preload.html</p> </li> </ol>"},{"location":"user-guide/misc/visualization/","title":"Visualization","text":"<p>Data in a Polars <code>DataFrame</code> can be visualized using common visualization libraries.</p> <p>We illustrate plotting capabilities using the Iris dataset. We scan a CSV and then do a group-by on the <code>species</code> column and get the mean of the <code>petal_length</code>.</p>  Python <pre><code>import polars as pl\n\npath = \"docs/data/iris.csv\"\n\ndf = pl.scan_csv(path).group_by(\"species\").agg(pl.col(\"petal_length\").mean()).collect()\nprint(df)\n</code></pre> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 species    \u2506 petal_length \u2502\n\u2502 ---        \u2506 ---          \u2502\n\u2502 str        \u2506 f64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Versicolor \u2506 4.26         \u2502\n\u2502 Virginica  \u2506 5.552        \u2502\n\u2502 Setosa     \u2506 1.462        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/misc/visualization/#built-in-plotting-with-hvplot","title":"Built-in plotting with hvPlot","text":"<p>Polars has a <code>plot</code> method to create interactive plots using hvPlot.</p>  Python <pre><code>df.plot.bar(\n    x=\"species\",\n    y=\"petal_length\",\n    width=650,\n)\n</code></pre> hvplot_bar"},{"location":"user-guide/misc/visualization/#matplotlib","title":"Matplotlib","text":"<p>To create a bar chart we can pass columns of a <code>DataFrame</code> directly to Matplotlib as a <code>Series</code> for each column. Matplotlib does not have explicit support for Polars objects but Matplotlib can accept a Polars <code>Series</code> because it can convert each Series to a numpy array, which is zero-copy for numeric data without null values.</p>  Python <pre><code>import matplotlib.pyplot as plt\n\nplt.bar(x=df[\"species\"], height=df[\"petal_length\"])\n</code></pre> <p></p>"},{"location":"user-guide/misc/visualization/#seaborn-plotly-altair","title":"Seaborn, Plotly &amp; Altair","text":"<p>Seaborn, Plotly &amp; Altair can accept a Polars <code>DataFrame</code> by leveraging the dataframe interchange protocol, which offers zero-copy conversion where possible.</p>"},{"location":"user-guide/misc/visualization/#seaborn","title":"Seaborn","text":"Python <pre><code>import seaborn as sns\nsns.barplot(\n    df,\n    x=\"species\",\n    y=\"petal_length\",\n)\n</code></pre>"},{"location":"user-guide/misc/visualization/#plotly","title":"Plotly","text":"Python <pre><code>import plotly.express as px\n\npx.bar(\n    df,\n    x=\"species\",\n    y=\"petal_length\",\n    width=400,\n)\n</code></pre>"},{"location":"user-guide/misc/visualization/#altair","title":"Altair","text":"Python <pre><code>import altair as alt\n\nalt.Chart(df, width=700).mark_bar().encode(x=\"species:N\", y=\"petal_length:Q\")\n</code></pre>"},{"location":"user-guide/sql/create/","title":"CREATE","text":"<p>In Polars, the <code>SQLContext</code> provides a way to execute SQL statements against <code>LazyFrames</code> and <code>DataFrames</code> using SQL syntax. One of the SQL statements that can be executed using <code>SQLContext</code> is the <code>CREATE TABLE</code> statement, which is used to create a new table.</p> <p>The syntax for the <code>CREATE TABLE</code> statement in Polars is as follows:</p> <pre><code>CREATE TABLE table_name\nAS\nSELECT ...\n</code></pre> <p>In this syntax, <code>table_name</code> is the name of the new table that will be created, and <code>SELECT ...</code> is a SELECT statement that defines the data that will be inserted into the table.</p> <p>Here's an example of how to use the <code>CREATE TABLE</code> statement in Polars:</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code>data = {\"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"], \"age\": [25, 30, 35, 40]}\ndf = pl.LazyFrame(data)\n\nctx = pl.SQLContext(my_table=df, eager_execution=True)\n\nresult = ctx.execute(\n    \"\"\"\n    CREATE TABLE older_people\n    AS\n    SELECT * FROM my_table WHERE age &gt; 30\n\"\"\"\n)\n\nprint(ctx.execute(\"SELECT * FROM older_people\"))\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name    \u2506 age \u2502\n\u2502 ---     \u2506 --- \u2502\n\u2502 str     \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Charlie \u2506 35  \u2502\n\u2502 David   \u2506 40  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example, we use the <code>execute()</code> method of the <code>SQLContext</code> to execute a <code>CREATE TABLE</code> statement that creates a new table called <code>older_people</code> based on a SELECT statement that selects all rows from the <code>my_table</code> DataFrame where the <code>age</code> column is greater than 30.</p> <p>Note</p> <p>Note that the result of a <code>CREATE TABLE</code> statement is not the table itself. The table is registered in the <code>SQLContext</code>. In case you want to turn the table back to a <code>DataFrame</code> you can use a <code>SELECT * FROM ...</code> statement</p>"},{"location":"user-guide/sql/cte/","title":"Common Table Expressions","text":"<p>Common Table Expressions (CTEs) are a feature of SQL that allow you to define a temporary named result set that can be referenced within a SQL statement. CTEs provide a way to break down complex SQL queries into smaller, more manageable pieces, making them easier to read, write, and maintain.</p> <p>A CTE is defined using the <code>WITH</code> keyword followed by a comma-separated list of subqueries, each of which defines a named result set that can be used in subsequent queries. The syntax for a CTE is as follows:</p> <pre><code>WITH cte_name AS (\n    subquery\n)\nSELECT ...\n</code></pre> <p>In this syntax, <code>cte_name</code> is the name of the CTE, and <code>subquery</code> is the subquery that defines the result set. The CTE can then be referenced in subsequent queries as if it were a table or view.</p> <p>CTEs are particularly useful when working with complex queries that involve multiple levels of subqueries, as they allow you to break down the query into smaller, more manageable pieces that are easier to understand and debug. Additionally, CTEs can help improve query performance by allowing the database to optimize and cache the results of subqueries, reducing the number of times they need to be executed.</p> <p>Polars supports Common Table Expressions (CTEs) using the WITH clause in SQL syntax. Below is an example</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code>ctx = pl.SQLContext()\ndf = pl.LazyFrame(\n    {\"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"], \"age\": [25, 30, 35, 40]}\n)\nctx.register(\"my_table\", df)\n\nresult = ctx.execute(\n    \"\"\"\n    WITH older_people AS (\n        SELECT * FROM my_table WHERE age &gt; 30\n    )\n    SELECT * FROM older_people WHERE STARTS_WITH(name,'C')\n\"\"\",\n    eager=True,\n)\n\nprint(result)\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name    \u2506 age \u2502\n\u2502 ---     \u2506 --- \u2502\n\u2502 str     \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Charlie \u2506 35  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example, we use the <code>execute()</code> method of the <code>SQLContext</code> to execute a SQL query that includes a CTE. The CTE selects all rows from the <code>my_table</code> LazyFrame where the <code>age</code> column is greater than 30 and gives it the alias <code>older_people</code>. We then execute a second SQL query that selects all rows from the <code>older_people</code> CTE where the <code>name</code> column starts with the letter 'C'.</p>"},{"location":"user-guide/sql/intro/","title":"Introduction","text":"<p>While Polars supports interaction with SQL, it's recommended that users familiarize themselves with the expression syntax to produce more readable and expressive code. As the DataFrame interface is primary, new features are typically added to the expression API first. However, if you already have an existing SQL codebase or prefer the use of SQL, Polars does offers support for this.</p> <p>Note</p> <p>There is no separate SQL engine because Polars translates SQL queries into expressions, which are then executed using its own engine. This approach ensures that Polars maintains its performance and scalability advantages as a native DataFrame library, while still providing users with the ability to work with SQL.</p>"},{"location":"user-guide/sql/intro/#context","title":"Context","text":"<p>Polars uses the <code>SQLContext</code> object to manage SQL queries. The context contains a mapping of <code>DataFrame</code> and <code>LazyFrame</code> identifier names to their corresponding datasets<sup>1</sup>. The example below starts a <code>SQLContext</code>:</p>  Python <p> <code>SQLContext</code> <pre><code>ctx = pl.SQLContext()\n</code></pre></p> <p></p>"},{"location":"user-guide/sql/intro/#register-dataframes","title":"Register Dataframes","text":"<p>There are several ways to register DataFrames during <code>SQLContext</code> initialization.</p> <ul> <li>register all <code>LazyFrame</code> and <code>DataFrame</code> objects in the global namespace.</li> <li>register explicitly via a dictionary mapping, or kwargs.</li> </ul>  Python <p> <code>SQLContext</code> <pre><code>df = pl.DataFrame({\"a\": [1, 2, 3]})\nlf = pl.LazyFrame({\"b\": [4, 5, 6]})\n\n# Register all dataframes in the global namespace: registers both \"df\" and \"lf\"\nctx = pl.SQLContext(register_globals=True)\n\n# Register an explicit mapping of identifier name to frame\nctx = pl.SQLContext(frames={\"table_one\": df, \"table_two\": lf})\n\n# Register frames using kwargs; dataframe df as \"df\" and lazyframe lf as \"lf\"\nctx = pl.SQLContext(df=df, lf=lf)\n</code></pre></p> <p></p> <p>We can also register Pandas DataFrames by converting them to Polars first.</p>  Python <p> <code>SQLContext</code> <pre><code>import pandas as pd\n\ndf_pandas = pd.DataFrame({\"c\": [7, 8, 9]})\nctx = pl.SQLContext(df_pandas=pl.from_pandas(df_pandas))\n</code></pre></p> <p></p> <p>Note</p> <p>Converting a Pandas DataFrame backed by Numpy will trigger a potentially expensive conversion; however, if the Pandas DataFrame is already backed by Arrow then the conversion will be significantly cheaper (and in some cases close to free).</p> <p>Once the <code>SQLContext</code> is initialized, we can register additional Dataframes or unregister existing Dataframes with:</p> <ul> <li><code>register</code></li> <li><code>register_globals</code></li> <li><code>register_many</code></li> <li><code>unregister</code></li> </ul>"},{"location":"user-guide/sql/intro/#execute-queries-and-collect-results","title":"Execute queries and collect results","text":"<p>SQL queries are always executed in lazy mode to take advantage of the full set of query planning optimizations, so we have two options to collect the result:</p> <ul> <li>Set the parameter <code>eager_execution</code> to True in <code>SQLContext</code>; this ensures that Polars automatically collects the   LazyFrame results from <code>execute</code> calls.</li> <li>Set the parameter <code>eager</code> to True when executing a query with <code>execute</code>, or explicitly collect the result   using <code>collect</code>.</li> </ul> <p>We execute SQL queries by calling <code>execute</code> on a <code>SQLContext</code>.</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code># For local files use scan_csv instead\npokemon = pl.read_csv(\n    \"https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv\"\n)\nwith pl.SQLContext(register_globals=True, eager_execution=True) as ctx:\n    df_small = ctx.execute(\"SELECT * from pokemon LIMIT 5\")\n    print(df_small)\n</code></pre></p> <pre><code>shape: (5, 13)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 #   \u2506 Name                  \u2506 Type 1 \u2506 Type 2 \u2506 \u2026 \u2506 Sp. Def \u2506 Speed \u2506 Generation \u2506 Legendary \u2502\n\u2502 --- \u2506 ---                   \u2506 ---    \u2506 ---    \u2506   \u2506 ---     \u2506 ---   \u2506 ---        \u2506 ---       \u2502\n\u2502 i64 \u2506 str                   \u2506 str    \u2506 str    \u2506   \u2506 i64     \u2506 i64   \u2506 i64        \u2506 bool      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Bulbasaur             \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 65      \u2506 45    \u2506 1          \u2506 false     \u2502\n\u2502 2   \u2506 Ivysaur               \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 80      \u2506 60    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 Venusaur              \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 100     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 VenusaurMega Venusaur \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 120     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 4   \u2506 Charmander            \u2506 Fire   \u2506 null   \u2506 \u2026 \u2506 50      \u2506 65    \u2506 1          \u2506 false     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/intro/#execute-queries-from-multiple-sources","title":"Execute queries from multiple sources","text":"<p>SQL queries can be executed just as easily from multiple sources. In the example below, we register:</p> <ul> <li>a CSV file (loaded lazily)</li> <li>a NDJSON file (loaded lazily)</li> <li>a Pandas DataFrame</li> </ul> <p>And join them together using SQL. Lazy reading allows to only load the necessary rows and columns from the files.</p> <p>In the same way, it's possible to register cloud datalakes (S3, Azure Data Lake). A PyArrow dataset can point to the datalake, then Polars can read it with <code>scan_pyarrow_dataset</code>.</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code># Input data:\n# products_masterdata.csv with schema {'product_id': Int64, 'product_name': String}\n# products_categories.json with schema {'product_id': Int64, 'category': String}\n# sales_data is a Pandas DataFrame with schema {'product_id': Int64, 'sales': Int64}\n\nwith pl.SQLContext(\n    products_masterdata=pl.scan_csv(\"docs/data/products_masterdata.csv\"),\n    products_categories=pl.scan_ndjson(\"docs/data/products_categories.json\"),\n    sales_data=pl.from_pandas(sales_data),\n    eager_execution=True,\n) as ctx:\n    query = \"\"\"\n    SELECT\n        product_id,\n        product_name,\n        category,\n        sales\n    FROM\n        products_masterdata\n    LEFT JOIN products_categories USING (product_id)\n    LEFT JOIN sales_data USING (product_id)\n    \"\"\"\n    print(ctx.execute(query))\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 product_id \u2506 product_name \u2506 category   \u2506 sales \u2502\n\u2502 ---        \u2506 ---          \u2506 ---        \u2506 ---   \u2502\n\u2502 i64        \u2506 str          \u2506 str        \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1          \u2506 Product A    \u2506 Category 1 \u2506 100   \u2502\n\u2502 2          \u2506 Product B    \u2506 Category 1 \u2506 200   \u2502\n\u2502 3          \u2506 Product C    \u2506 Category 2 \u2506 150   \u2502\n\u2502 4          \u2506 Product D    \u2506 Category 2 \u2506 250   \u2502\n\u2502 5          \u2506 Product E    \u2506 Category 3 \u2506 300   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/intro/#compatibility","title":"Compatibility","text":"<p>Polars does not support the complete SQL specification, but it does support a subset of the most common statement types.</p> <p>Note</p> <p>Where possible, Polars aims to follow PostgreSQL syntax definitions and function behaviour.</p> <p>For example, here is a non-exhaustive list of some of the supported functionality:</p> <ul> <li>Write a <code>CREATE</code> statements: <code>CREATE TABLE xxx AS ...</code></li> <li>Write a <code>SELECT</code> statements containing:<code>WHERE</code>,<code>ORDER</code>,<code>LIMIT</code>,<code>GROUP BY</code>,<code>UNION</code> and <code>JOIN</code> clauses ...</li> <li>Write Common Table Expressions (CTE's) such as: <code>WITH tablename AS</code></li> <li>Explain a query: <code>EXPLAIN SELECT ...</code></li> <li>List registered tables: <code>SHOW TABLES</code></li> <li>Drop a table: <code>DROP TABLE tablename</code></li> <li>Truncate a table: <code>TRUNCATE TABLE tablename</code></li> </ul> <p>The following are some features that are not yet supported:</p> <ul> <li><code>INSERT</code>, <code>UPDATE</code> or <code>DELETE</code> statements</li> <li>Meta queries such as <code>ANALYZE</code></li> </ul> <p>In the upcoming sections we will cover each of the statements in more detail.</p> <ol> <li> <p>Additionally it also tracks the common table expressions as well.\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/sql/select/","title":"SELECT","text":"<p>In Polars SQL, the <code>SELECT</code> statement is used to retrieve data from a table into a <code>DataFrame</code>. The basic syntax of a <code>SELECT</code> statement in Polars SQL is as follows:</p> <pre><code>SELECT column1, column2, ...\nFROM table_name;\n</code></pre> <p>Here, <code>column1</code>, <code>column2</code>, etc. are the columns that you want to select from the table. You can also use the wildcard <code>*</code> to select all columns. <code>table_name</code> is the name of the table or that you want to retrieve data from. In the sections below we will cover some of the more common SELECT variants</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code>df = pl.DataFrame(\n    {\n        \"city\": [\n            \"New York\",\n            \"Los Angeles\",\n            \"Chicago\",\n            \"Houston\",\n            \"Phoenix\",\n            \"Amsterdam\",\n        ],\n        \"country\": [\"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"Netherlands\"],\n        \"population\": [8399000, 3997000, 2705000, 2320000, 1680000, 900000],\n    }\n)\n\nctx = pl.SQLContext(population=df, eager_execution=True)\n\nprint(ctx.execute(\"SELECT * FROM population\"))\n</code></pre></p> <pre><code>shape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 city        \u2506 country     \u2506 population \u2502\n\u2502 ---         \u2506 ---         \u2506 ---        \u2502\n\u2502 str         \u2506 str         \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 New York    \u2506 USA         \u2506 8399000    \u2502\n\u2502 Los Angeles \u2506 USA         \u2506 3997000    \u2502\n\u2502 Chicago     \u2506 USA         \u2506 2705000    \u2502\n\u2502 Houston     \u2506 USA         \u2506 2320000    \u2502\n\u2502 Phoenix     \u2506 USA         \u2506 1680000    \u2502\n\u2502 Amsterdam   \u2506 Netherlands \u2506 900000     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#group-by","title":"GROUP BY","text":"<p>The <code>GROUP BY</code> statement is used to group rows in a table by one or more columns and compute aggregate functions on each group.</p>  Python <p> <code>execute</code> <pre><code>result = ctx.execute(\n    \"\"\"\n        SELECT country, AVG(population) as avg_population\n        FROM population\n        GROUP BY country\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 country     \u2506 avg_population \u2502\n\u2502 ---         \u2506 ---            \u2502\n\u2502 str         \u2506 f64            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Netherlands \u2506 900000.0       \u2502\n\u2502 USA         \u2506 3.8202e6       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#order-by","title":"ORDER BY","text":"<p>The <code>ORDER BY</code> statement is used to sort the result set of a query by one or more columns in ascending or descending order.</p>  Python <p> <code>execute</code> <pre><code>result = ctx.execute(\n    \"\"\"\n        SELECT city, population\n        FROM population\n        ORDER BY population\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (6, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 city        \u2506 population \u2502\n\u2502 ---         \u2506 ---        \u2502\n\u2502 str         \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Amsterdam   \u2506 900000     \u2502\n\u2502 Phoenix     \u2506 1680000    \u2502\n\u2502 Houston     \u2506 2320000    \u2502\n\u2502 Chicago     \u2506 2705000    \u2502\n\u2502 Los Angeles \u2506 3997000    \u2502\n\u2502 New York    \u2506 8399000    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#join","title":"JOIN","text":"Python <p> <code>register_many</code> \u00b7 <code>execute</code> <pre><code>income = pl.DataFrame(\n    {\n        \"city\": [\n            \"New York\",\n            \"Los Angeles\",\n            \"Chicago\",\n            \"Houston\",\n            \"Amsterdam\",\n            \"Rotterdam\",\n            \"Utrecht\",\n        ],\n        \"country\": [\n            \"USA\",\n            \"USA\",\n            \"USA\",\n            \"USA\",\n            \"Netherlands\",\n            \"Netherlands\",\n            \"Netherlands\",\n        ],\n        \"income\": [55000, 62000, 48000, 52000, 42000, 38000, 41000],\n    }\n)\nctx.register_many(income=income)\nresult = ctx.execute(\n    \"\"\"\n        SELECT country, city, income, population\n        FROM population\n        LEFT JOIN income on population.city = income.city\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (6, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 country     \u2506 city        \u2506 income \u2506 population \u2502\n\u2502 ---         \u2506 ---         \u2506 ---    \u2506 ---        \u2502\n\u2502 str         \u2506 str         \u2506 i64    \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 USA         \u2506 New York    \u2506 55000  \u2506 8399000    \u2502\n\u2502 USA         \u2506 Los Angeles \u2506 62000  \u2506 3997000    \u2502\n\u2502 USA         \u2506 Chicago     \u2506 48000  \u2506 2705000    \u2502\n\u2502 USA         \u2506 Houston     \u2506 52000  \u2506 2320000    \u2502\n\u2502 USA         \u2506 Phoenix     \u2506 null   \u2506 1680000    \u2502\n\u2502 Netherlands \u2506 Amsterdam   \u2506 42000  \u2506 900000     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#functions","title":"Functions","text":"<p>Polars provides a wide range of SQL functions, including:</p> <ul> <li>Mathematical functions: <code>ABS</code>, <code>EXP</code>, <code>LOG</code>, <code>ASIN</code>, <code>ACOS</code>, <code>ATAN</code>, etc.</li> <li>String functions: <code>LOWER</code>, <code>UPPER</code>, <code>LTRIM</code>, <code>RTRIM</code>, <code>STARTS_WITH</code>,<code>ENDS_WITH</code>.</li> <li>Aggregation functions: <code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code>, <code>COUNT</code>, <code>STDDEV</code>, <code>FIRST</code> etc.</li> <li>Array functions: <code>EXPLODE</code>, <code>UNNEST</code>,<code>ARRAY_SUM</code>,<code>ARRAY_REVERSE</code>, etc.</li> </ul> <p>For a full list of supported functions go the API documentation. The example below demonstrates how to use a function in a query</p>  Python <p> <code>query</code> <pre><code>result = ctx.execute(\n    \"\"\"\n        SELECT city, population\n        FROM population\n        WHERE STARTS_WITH(country,'U')\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 city        \u2506 population \u2502\n\u2502 ---         \u2506 ---        \u2502\n\u2502 str         \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 New York    \u2506 8399000    \u2502\n\u2502 Los Angeles \u2506 3997000    \u2502\n\u2502 Chicago     \u2506 2705000    \u2502\n\u2502 Houston     \u2506 2320000    \u2502\n\u2502 Phoenix     \u2506 1680000    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#table-functions","title":"Table Functions","text":"<p>In the examples earlier we first generated a DataFrame which we registered in the <code>SQLContext</code>. Polars also support directly reading from CSV, Parquet, JSON and IPC in your SQL query using table functions <code>read_xxx</code>.</p>  Python <p> <code>execute</code> <pre><code>result = ctx.execute(\n    \"\"\"\n        SELECT *\n        FROM read_csv('docs/data/iris.csv')\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (150, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sepal_length \u2506 sepal_width \u2506 petal_length \u2506 petal_width \u2506 species   \u2502\n\u2502 ---          \u2506 ---         \u2506 ---          \u2506 ---         \u2506 ---       \u2502\n\u2502 f64          \u2506 f64         \u2506 f64          \u2506 f64         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 5.1          \u2506 3.5         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.9          \u2506 3.0         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.7          \u2506 3.2         \u2506 1.3          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.6          \u2506 3.1         \u2506 1.5          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 5.0          \u2506 3.6         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 \u2026            \u2506 \u2026           \u2506 \u2026            \u2506 \u2026           \u2506 \u2026         \u2502\n\u2502 6.7          \u2506 3.0         \u2506 5.2          \u2506 2.3         \u2506 Virginica \u2502\n\u2502 6.3          \u2506 2.5         \u2506 5.0          \u2506 1.9         \u2506 Virginica \u2502\n\u2502 6.5          \u2506 3.0         \u2506 5.2          \u2506 2.0         \u2506 Virginica \u2502\n\u2502 6.2          \u2506 3.4         \u2506 5.4          \u2506 2.3         \u2506 Virginica \u2502\n\u2502 5.9          \u2506 3.0         \u2506 5.1          \u2506 1.8         \u2506 Virginica \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/show/","title":"SHOW TABLES","text":"<p>In Polars, the <code>SHOW TABLES</code> statement is used to list all the tables that have been registered in the current <code>SQLContext</code>. When you register a DataFrame with the <code>SQLContext</code>, you give it a name that can be used to refer to the DataFrame in subsequent SQL statements. The <code>SHOW TABLES</code> statement allows you to see a list of all the registered tables, along with their names.</p> <p>The syntax for the <code>SHOW TABLES</code> statement in Polars is as follows:</p> <pre><code>SHOW TABLES\n</code></pre> <p>Here's an example of how to use the <code>SHOW TABLES</code> statement in Polars:</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code># Create some DataFrames and register them with the SQLContext\ndf1 = pl.LazyFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n        \"age\": [25, 30, 35, 40],\n    }\n)\ndf2 = pl.LazyFrame(\n    {\n        \"name\": [\"Ellen\", \"Frank\", \"Gina\", \"Henry\"],\n        \"age\": [45, 50, 55, 60],\n    }\n)\nctx = pl.SQLContext(mytable1=df1, mytable2=df2)\n\ntables = ctx.execute(\"SHOW TABLES\", eager=True)\n\nprint(tables)\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name     \u2502\n\u2502 ---      \u2502\n\u2502 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 mytable1 \u2502\n\u2502 mytable2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example, we create two DataFrames and register them with the <code>SQLContext</code> using different names. We then execute a <code>SHOW TABLES</code> statement using the <code>execute()</code> method of the <code>SQLContext</code> object, which returns a DataFrame containing a list of all the registered tables and their names. The resulting DataFrame is then printed using the <code>print()</code> function.</p> <p>Note that the <code>SHOW TABLES</code> statement only lists tables that have been registered with the current <code>SQLContext</code>. If you register a DataFrame with a different <code>SQLContext</code> or in a different Python session, it will not appear in the list of tables returned by <code>SHOW TABLES</code>.</p>"},{"location":"user-guide/transformations/","title":"Transformations","text":"<p>The focus of this section is to describe different types of data transformations and provide some examples on how to use them.</p> <ul> <li>Joins</li> <li>Concatenation</li> <li>Pivot</li> <li>Melt</li> </ul>"},{"location":"user-guide/transformations/concatenation/","title":"Concatenation","text":"<p>There are a number of ways to concatenate data from separate DataFrames:</p> <ul> <li>two dataframes with the same columns can be vertically concatenated to make a longer dataframe</li> <li>two dataframes with non-overlapping columns can be horizontally concatenated to make a wider dataframe</li> <li>two dataframes with different numbers of rows and columns can be diagonally concatenated to make a dataframe which might be longer and/ or wider. Where column names overlap values will be vertically concatenated. Where column names do not overlap new rows and columns will be added. Missing values will be set as <code>null</code></li> </ul>"},{"location":"user-guide/transformations/concatenation/#vertical-concatenation-getting-longer","title":"Vertical concatenation - getting longer","text":"<p>In a vertical concatenation you combine all of the rows from a list of <code>DataFrames</code> into a single longer <code>DataFrame</code>.</p>  Python Rust <p> <code>concat</code> <pre><code>df_v1 = pl.DataFrame(\n    {\n        \"a\": [1],\n        \"b\": [3],\n    }\n)\ndf_v2 = pl.DataFrame(\n    {\n        \"a\": [2],\n        \"b\": [4],\n    }\n)\ndf_vertical_concat = pl.concat(\n    [\n        df_v1,\n        df_v2,\n    ],\n    how=\"vertical\",\n)\nprint(df_vertical_concat)\n</code></pre></p> <p> <code>concat</code> <pre><code>let df_v1 = df!(\n        \"a\"=&gt; &amp;[1],\n        \"b\"=&gt; &amp;[3],\n)?;\nlet df_v2 = df!(\n        \"a\"=&gt; &amp;[2],\n        \"b\"=&gt; &amp;[4],\n)?;\nlet df_vertical_concat = concat(\n    [df_v1.clone().lazy(), df_v2.clone().lazy()],\n    UnionArgs::default(),\n)?\n.collect()?;\nprintln!(\"{}\", &amp;df_vertical_concat);\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3   \u2502\n\u2502 2   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Vertical concatenation fails when the dataframes do not have the same column names.</p>"},{"location":"user-guide/transformations/concatenation/#horizontal-concatenation-getting-wider","title":"Horizontal concatenation - getting wider","text":"<p>In a horizontal concatenation you combine all of the columns from a list of <code>DataFrames</code> into a single wider <code>DataFrame</code>.</p>  Python Rust <p> <code>concat</code> <pre><code>df_h1 = pl.DataFrame(\n    {\n        \"l1\": [1, 2],\n        \"l2\": [3, 4],\n    }\n)\ndf_h2 = pl.DataFrame(\n    {\n        \"r1\": [5, 6],\n        \"r2\": [7, 8],\n        \"r3\": [9, 10],\n    }\n)\ndf_horizontal_concat = pl.concat(\n    [\n        df_h1,\n        df_h2,\n    ],\n    how=\"horizontal\",\n)\nprint(df_horizontal_concat)\n</code></pre></p> <p> <code>concat</code> <pre><code>let df_h1 = df!(\n        \"l1\"=&gt; &amp;[1, 2],\n        \"l2\"=&gt; &amp;[3, 4],\n)?;\nlet df_h2 = df!(\n        \"r1\"=&gt; &amp;[5, 6],\n        \"r2\"=&gt; &amp;[7, 8],\n        \"r3\"=&gt; &amp;[9, 10],\n)?;\nlet df_horizontal_concat = polars::functions::concat_df_horizontal(&amp;[df_h1, df_h2])?;\nprintln!(\"{}\", &amp;df_horizontal_concat);\n</code></pre></p> <pre><code>shape: (2, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 l1  \u2506 l2  \u2506 r1  \u2506 r2  \u2506 r3  \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3   \u2506 5   \u2506 7   \u2506 9   \u2502\n\u2502 2   \u2506 4   \u2506 6   \u2506 8   \u2506 10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Horizontal concatenation fails when dataframes have overlapping columns.</p> <p>When dataframes have different numbers of rows, columns will be padded with <code>null</code> values at the end up to the maximum length.</p>  Python Rust <p> <code>concat</code> <pre><code>df_h1 = pl.DataFrame(\n    {\n        \"l1\": [1, 2],\n        \"l2\": [3, 4],\n    }\n)\ndf_h2 = pl.DataFrame(\n    {\n        \"r1\": [5, 6, 7],\n        \"r2\": [8, 9, 10],\n    }\n)\ndf_horizontal_concat = pl.concat(\n    [\n        df_h1,\n        df_h2,\n    ],\n    how=\"horizontal\",\n)\nprint(df_horizontal_concat)\n</code></pre></p> <p> <code>concat</code> <pre><code>let df_h1 = df!(\n        \"l1\"=&gt; &amp;[1, 2],\n        \"l2\"=&gt; &amp;[3, 4],\n)?;\nlet df_h2 = df!(\n        \"r1\"=&gt; &amp;[5, 6, 7],\n        \"r2\"=&gt; &amp;[8, 9, 10],\n)?;\nlet df_horizontal_concat = polars::functions::concat_df_horizontal(&amp;[df_h1, df_h2])?;\nprintln!(\"{}\", &amp;df_horizontal_concat);\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 l1   \u2506 l2   \u2506 r1  \u2506 r2  \u2502\n\u2502 ---  \u2506 ---  \u2506 --- \u2506 --- \u2502\n\u2502 i64  \u2506 i64  \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 3    \u2506 5   \u2506 8   \u2502\n\u2502 2    \u2506 4    \u2506 6   \u2506 9   \u2502\n\u2502 null \u2506 null \u2506 7   \u2506 10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/concatenation/#diagonal-concatenation-getting-longer-wider-and-nullier","title":"Diagonal concatenation - getting longer, wider and <code>null</code>ier","text":"<p>In a diagonal concatenation you combine all of the row and columns from a list of <code>DataFrames</code> into a single longer and/or wider <code>DataFrame</code>.</p>  Python Rust <p> <code>concat</code> <pre><code>df_d1 = pl.DataFrame(\n    {\n        \"a\": [1],\n        \"b\": [3],\n    }\n)\ndf_d2 = pl.DataFrame(\n    {\n        \"a\": [2],\n        \"d\": [4],\n    }\n)\n\ndf_diagonal_concat = pl.concat(\n    [\n        df_d1,\n        df_d2,\n    ],\n    how=\"diagonal\",\n)\nprint(df_diagonal_concat)\n</code></pre></p> <p> <code>concat</code> <pre><code>let df_d1 = df!(\n    \"a\"=&gt; &amp;[1],\n    \"b\"=&gt; &amp;[3],\n)?;\nlet df_d2 = df!(\n        \"a\"=&gt; &amp;[2],\n        \"d\"=&gt; &amp;[4],)?;\nlet df_diagonal_concat = polars::functions::concat_df_diagonal(&amp;[df_d1, df_d2])?;\nprintln!(\"{}\", &amp;df_diagonal_concat);\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b    \u2506 d    \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 i64 \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3    \u2506 null \u2502\n\u2502 2   \u2506 null \u2506 4    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Diagonal concatenation generates nulls when the column names do not overlap.</p> <p>When the dataframe shapes do not match and we have an overlapping semantic key then we can join the dataframes instead of concatenating them.</p>"},{"location":"user-guide/transformations/concatenation/#rechunking","title":"Rechunking","text":"<p>Before a concatenation we have two dataframes <code>df1</code> and <code>df2</code>. Each column in <code>df1</code> and <code>df2</code> is in one or more chunks in memory. By default, during concatenation the chunks in each column are copied to a single new chunk - this is known as rechunking. Rechunking is an expensive operation, but is often worth it because future operations will be faster. If you do not want Polars to rechunk the concatenated <code>DataFrame</code> you specify <code>rechunk = False</code> when doing the concatenation.</p>"},{"location":"user-guide/transformations/joins/","title":"Joins","text":""},{"location":"user-guide/transformations/joins/#join-strategies","title":"Join strategies","text":"<p>Polars supports the following join strategies by specifying the <code>how</code> argument:</p> Strategy Description <code>inner</code> Returns row with matching keys in both frames. Non-matching rows in either the left or right frame are discarded. <code>left</code> Returns all rows in the left dataframe, whether or not a match in the right-frame is found. Non-matching rows have their right columns null-filled. <code>outer</code> Returns all rows from both the left and right dataframe. If no match is found in one frame, columns from the other frame are null-filled. <code>outer_coalesce</code> Returns all rows from both the left and right dataframe. This is similar to <code>outer</code>, but with the key columns being merged. <code>cross</code> Returns the Cartesian product of all rows from the left frame with all rows from the right frame. Duplicates rows are retained; the table length of <code>A</code> cross-joined with <code>B</code> is always <code>len(A) \u00d7 len(B)</code>. <code>semi</code> Returns all rows from the left frame in which the join key is also present in the right frame. <code>anti</code> Returns all rows from the left frame in which the join key is not present in the right frame."},{"location":"user-guide/transformations/joins/#inner-join","title":"Inner join","text":"<p>An <code>inner</code> join produces a <code>DataFrame</code> that contains only the rows where the join key exists in both <code>DataFrames</code>. Let's take for example the following two <code>DataFrames</code>:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df_customers = pl.DataFrame(\n    {\n        \"customer_id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    }\n)\nprint(df_customers)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df_customers = df! (\n\n    \"customer_id\" =&gt; &amp;[1, 2, 3],\n    \"name\" =&gt; &amp;[\"Alice\", \"Bob\", \"Charlie\"],\n)?;\n\nprintln!(\"{}\", &amp;df_customers);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 name    \u2502\n\u2502 ---         \u2506 ---     \u2502\n\u2502 i64         \u2506 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 Alice   \u2502\n\u2502 2           \u2506 Bob     \u2502\n\u2502 3           \u2506 Charlie \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p>  Python Rust <p> <code>DataFrame</code> <pre><code>df_orders = pl.DataFrame(\n    {\n        \"order_id\": [\"a\", \"b\", \"c\"],\n        \"customer_id\": [1, 2, 2],\n        \"amount\": [100, 200, 300],\n    }\n)\nprint(df_orders)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df_orders = df!(\n        \"order_id\"=&gt; &amp;[\"a\", \"b\", \"c\"],\n        \"customer_id\"=&gt; &amp;[1, 2, 2],\n        \"amount\"=&gt; &amp;[100, 200, 300],\n)?;\nprintln!(\"{}\", &amp;df_orders);\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 order_id \u2506 customer_id \u2506 amount \u2502\n\u2502 ---      \u2506 ---         \u2506 ---    \u2502\n\u2502 str      \u2506 i64         \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a        \u2506 1           \u2506 100    \u2502\n\u2502 b        \u2506 2           \u2506 200    \u2502\n\u2502 c        \u2506 2           \u2506 300    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To get a <code>DataFrame</code> with the orders and their associated customer we can do an <code>inner</code> join on the <code>customer_id</code> column:</p>  Python Rust <p> <code>join</code> <pre><code>df_inner_customer_join = df_customers.join(df_orders, on=\"customer_id\", how=\"inner\")\nprint(df_inner_customer_join)\n</code></pre></p> <p> <code>join</code> <pre><code>let df_inner_customer_join = df_customers\n    .clone()\n    .lazy()\n    .join(\n        df_orders.clone().lazy(),\n        [col(\"customer_id\")],\n        [col(\"customer_id\")],\n        JoinArgs::new(JoinType::Inner),\n    )\n    .collect()?;\nprintln!(\"{}\", &amp;df_inner_customer_join);\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 name  \u2506 order_id \u2506 amount \u2502\n\u2502 ---         \u2506 ---   \u2506 ---      \u2506 ---    \u2502\n\u2502 i64         \u2506 str   \u2506 str      \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 Alice \u2506 a        \u2506 100    \u2502\n\u2502 2           \u2506 Bob   \u2506 b        \u2506 200    \u2502\n\u2502 2           \u2506 Bob   \u2506 c        \u2506 300    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/joins/#left-join","title":"Left join","text":"<p>The <code>left</code> join produces a <code>DataFrame</code> that contains all the rows from the left <code>DataFrame</code> and only the rows from the right <code>DataFrame</code> where the join key exists in the left <code>DataFrame</code>. If we now take the example from above and want to have a <code>DataFrame</code> with all the customers and their associated orders (regardless of whether they have placed an order or not) we can do a <code>left</code> join:</p>  Python Rust <p> <code>join</code> <pre><code>df_left_join = df_customers.join(df_orders, on=\"customer_id\", how=\"left\")\nprint(df_left_join)\n</code></pre></p> <p> <code>join</code> <pre><code>let df_left_join = df_customers\n    .clone()\n    .lazy()\n    .join(\n        df_orders.clone().lazy(),\n        [col(\"customer_id\")],\n        [col(\"customer_id\")],\n        JoinArgs::new(JoinType::Left),\n    )\n    .collect()?;\nprintln!(\"{}\", &amp;df_left_join);\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 name    \u2506 order_id \u2506 amount \u2502\n\u2502 ---         \u2506 ---     \u2506 ---      \u2506 ---    \u2502\n\u2502 i64         \u2506 str     \u2506 str      \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 Alice   \u2506 a        \u2506 100    \u2502\n\u2502 2           \u2506 Bob     \u2506 b        \u2506 200    \u2502\n\u2502 2           \u2506 Bob     \u2506 c        \u2506 300    \u2502\n\u2502 3           \u2506 Charlie \u2506 null     \u2506 null   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Notice, that the fields for the customer with the <code>customer_id</code> of <code>3</code> are null, as there are no orders for this customer.</p>"},{"location":"user-guide/transformations/joins/#outer-join","title":"Outer join","text":"<p>The <code>outer</code> join produces a <code>DataFrame</code> that contains all the rows from both <code>DataFrames</code>. Columns are null, if the join key does not exist in the source <code>DataFrame</code>. Doing an <code>outer</code> join on the two <code>DataFrames</code> from above produces a similar <code>DataFrame</code> to the <code>left</code> join:</p>  Python Rust <p> <code>join</code> <pre><code>df_outer_join = df_customers.join(df_orders, on=\"customer_id\", how=\"outer\")\nprint(df_outer_join)\n</code></pre></p> <p> <code>join</code> <pre><code>let df_outer_join = df_customers\n    .clone()\n    .lazy()\n    .join(\n        df_orders.clone().lazy(),\n        [col(\"customer_id\")],\n        [col(\"customer_id\")],\n        JoinArgs::new(JoinType::Outer),\n    )\n    .collect()?;\nprintln!(\"{}\", &amp;df_outer_join);\n</code></pre></p> <pre><code>shape: (4, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 name    \u2506 order_id \u2506 customer_id_right \u2506 amount \u2502\n\u2502 ---         \u2506 ---     \u2506 ---      \u2506 ---               \u2506 ---    \u2502\n\u2502 i64         \u2506 str     \u2506 str      \u2506 i64               \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 Alice   \u2506 a        \u2506 1                 \u2506 100    \u2502\n\u2502 2           \u2506 Bob     \u2506 b        \u2506 2                 \u2506 200    \u2502\n\u2502 2           \u2506 Bob     \u2506 c        \u2506 2                 \u2506 300    \u2502\n\u2502 3           \u2506 Charlie \u2506 null     \u2506 null              \u2506 null   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/joins/#outer-coalesce-join","title":"Outer coalesce join","text":"<p>The <code>outer_coalesce</code> join combines all rows from both <code>DataFrames</code> like an <code>outer</code> join, but it merges the join keys into a single column by coalescing the values. This ensures a unified view of the join key, avoiding nulls in key columns whenever possible. Let's compare it with the outer join using the two <code>DataFrames</code> we used above:</p>  Python Rust <p> <code>join</code> <pre><code>df_outer_coalesce_join = df_customers.join(\n    df_orders, on=\"customer_id\", how=\"outer_coalesce\"\n)\nprint(df_outer_coalesce_join)\n</code></pre></p> <p> <code>join</code> <pre><code>let df_outer_join = df_customers\n    .clone()\n    .lazy()\n    .join(\n        df_orders.clone().lazy(),\n        [col(\"customer_id\")],\n        [col(\"customer_id\")],\n        JoinArgs::new(JoinType::Outer),\n    )\n    .collect()?;\nprintln!(\"{}\", &amp;df_outer_join);\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 name    \u2506 order_id \u2506 amount \u2502\n\u2502 ---         \u2506 ---     \u2506 ---      \u2506 ---    \u2502\n\u2502 i64         \u2506 str     \u2506 str      \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 Alice   \u2506 a        \u2506 100    \u2502\n\u2502 2           \u2506 Bob     \u2506 b        \u2506 200    \u2502\n\u2502 2           \u2506 Bob     \u2506 c        \u2506 300    \u2502\n\u2502 3           \u2506 Charlie \u2506 null     \u2506 null   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In contrast to an <code>outer</code> join, where <code>customer_id</code> and <code>customer_id_right</code> columns would remain separate, the <code>outer_coalesce</code> join merges these columns into a single <code>customer_id</code> column.</p>"},{"location":"user-guide/transformations/joins/#cross-join","title":"Cross join","text":"<p>A <code>cross</code> join is a Cartesian product of the two <code>DataFrames</code>. This means that every row in the left <code>DataFrame</code> is joined with every row in the right <code>DataFrame</code>. The <code>cross</code> join is useful for creating a <code>DataFrame</code> with all possible combinations of the columns in two <code>DataFrames</code>. Let's take for example the following two <code>DataFrames</code>.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df_colors = pl.DataFrame(\n    {\n        \"color\": [\"red\", \"blue\", \"green\"],\n    }\n)\nprint(df_colors)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df_colors = df!(\n        \"color\"=&gt; &amp;[\"red\", \"blue\", \"green\"],\n)?;\nprintln!(\"{}\", &amp;df_colors);\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 color \u2502\n\u2502 ---   \u2502\n\u2502 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 red   \u2502\n\u2502 blue  \u2502\n\u2502 green \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p>  Python Rust <p> <code>DataFrame</code> <pre><code>df_sizes = pl.DataFrame(\n    {\n        \"size\": [\"S\", \"M\", \"L\"],\n    }\n)\nprint(df_sizes)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df_sizes = df!(\n        \"size\"=&gt; &amp;[\"S\", \"M\", \"L\"],\n)?;\nprintln!(\"{}\", &amp;df_sizes);\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 size \u2502\n\u2502 ---  \u2502\n\u2502 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 S    \u2502\n\u2502 M    \u2502\n\u2502 L    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can now create a <code>DataFrame</code> containing all possible combinations of the colors and sizes with a <code>cross</code> join:</p>  Python Rust <p> <code>join</code> <pre><code>df_cross_join = df_colors.join(df_sizes, how=\"cross\")\nprint(df_cross_join)\n</code></pre></p> <p> <code>join</code> <pre><code>let df_cross_join = df_colors\n    .clone()\n    .lazy()\n    .cross_join(df_sizes.clone().lazy())\n    .collect()?;\nprintln!(\"{}\", &amp;df_cross_join);\n</code></pre></p> <pre><code>shape: (9, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 color \u2506 size \u2502\n\u2502 ---   \u2506 ---  \u2502\n\u2502 str   \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 red   \u2506 S    \u2502\n\u2502 red   \u2506 M    \u2502\n\u2502 red   \u2506 L    \u2502\n\u2502 blue  \u2506 S    \u2502\n\u2502 blue  \u2506 M    \u2502\n\u2502 blue  \u2506 L    \u2502\n\u2502 green \u2506 S    \u2502\n\u2502 green \u2506 M    \u2502\n\u2502 green \u2506 L    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p> <p>The <code>inner</code>, <code>left</code>, <code>outer</code> and <code>cross</code> join strategies are standard amongst dataframe libraries. We provide more details on the less familiar <code>semi</code>, <code>anti</code> and <code>asof</code> join strategies below.</p>"},{"location":"user-guide/transformations/joins/#semi-join","title":"Semi join","text":"<p>The <code>semi</code> join returns all rows from the left frame in which the join key is also present in the right frame. Consider the following scenario: a car rental company has a <code>DataFrame</code> showing the cars that it owns with each car having a unique <code>id</code>.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df_cars = pl.DataFrame(\n    {\n        \"id\": [\"a\", \"b\", \"c\"],\n        \"make\": [\"ford\", \"toyota\", \"bmw\"],\n    }\n)\nprint(df_cars)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df_cars = df!(\n        \"id\"=&gt; &amp;[\"a\", \"b\", \"c\"],\n        \"make\"=&gt; &amp;[\"ford\", \"toyota\", \"bmw\"],\n)?;\nprintln!(\"{}\", &amp;df_cars);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 make   \u2502\n\u2502 --- \u2506 ---    \u2502\n\u2502 str \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 ford   \u2502\n\u2502 b   \u2506 toyota \u2502\n\u2502 c   \u2506 bmw    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The company has another <code>DataFrame</code> showing each repair job carried out on a vehicle.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df_repairs = pl.DataFrame(\n    {\n        \"id\": [\"c\", \"c\"],\n        \"cost\": [100, 200],\n    }\n)\nprint(df_repairs)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df_repairs = df!(\n        \"id\"=&gt; &amp;[\"c\", \"c\"],\n        \"cost\"=&gt; &amp;[100, 200],\n)?;\nprintln!(\"{}\", &amp;df_repairs);\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 cost \u2502\n\u2502 --- \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 c   \u2506 100  \u2502\n\u2502 c   \u2506 200  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You want to answer this question: which of the cars have had repairs carried out?</p> <p>An inner join does not answer this question directly as it produces a <code>DataFrame</code> with multiple rows for each car that has had multiple repair jobs:</p>  Python Rust <p> <code>join</code> <pre><code>df_inner_join = df_cars.join(df_repairs, on=\"id\", how=\"inner\")\nprint(df_inner_join)\n</code></pre></p> <p> <code>join</code> <pre><code>let df_inner_join = df_cars\n    .clone()\n    .lazy()\n    .inner_join(df_repairs.clone().lazy(), col(\"id\"), col(\"id\"))\n    .collect()?;\nprintln!(\"{}\", &amp;df_inner_join);\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 make \u2506 cost \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 str  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 c   \u2506 bmw  \u2506 100  \u2502\n\u2502 c   \u2506 bmw  \u2506 200  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>However, a semi join produces a single row for each car that has had a repair job carried out.</p>  Python Rust <p> <code>join</code> <pre><code>df_semi_join = df_cars.join(df_repairs, on=\"id\", how=\"semi\")\nprint(df_semi_join)\n</code></pre></p> <p> <code>join</code> <pre><code>let df_semi_join = df_cars\n    .clone()\n    .lazy()\n    .join(\n        df_repairs.clone().lazy(),\n        [col(\"id\")],\n        [col(\"id\")],\n        JoinArgs::new(JoinType::Semi),\n    )\n    .collect()?;\nprintln!(\"{}\", &amp;df_semi_join);\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 make \u2502\n\u2502 --- \u2506 ---  \u2502\n\u2502 str \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 c   \u2506 bmw  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/joins/#anti-join","title":"Anti join","text":"<p>Continuing this example, an alternative question might be: which of the cars have not had a repair job carried out? An anti join produces a <code>DataFrame</code> showing all the cars from <code>df_cars</code> where the <code>id</code> is not present in the <code>df_repairs</code> <code>DataFrame</code>.</p>  Python Rust <p> <code>join</code> <pre><code>df_anti_join = df_cars.join(df_repairs, on=\"id\", how=\"anti\")\nprint(df_anti_join)\n</code></pre></p> <p> <code>join</code> <pre><code>let df_anti_join = df_cars\n    .clone()\n    .lazy()\n    .join(\n        df_repairs.clone().lazy(),\n        [col(\"id\")],\n        [col(\"id\")],\n        JoinArgs::new(JoinType::Anti),\n    )\n    .collect()?;\nprintln!(\"{}\", &amp;df_anti_join);\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 make   \u2502\n\u2502 --- \u2506 ---    \u2502\n\u2502 str \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 ford   \u2502\n\u2502 b   \u2506 toyota \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/joins/#asof-join","title":"Asof join","text":"<p>An <code>asof</code> join is like a left join except that we match on nearest key rather than equal keys. In Polars we can do an asof join with the <code>join_asof</code> method.</p> <p>Consider the following scenario: a stock market broker has a <code>DataFrame</code> called <code>df_trades</code> showing transactions it has made for different stocks.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df_trades = pl.DataFrame(\n    {\n        \"time\": [\n            datetime(2020, 1, 1, 9, 1, 0),\n            datetime(2020, 1, 1, 9, 1, 0),\n            datetime(2020, 1, 1, 9, 3, 0),\n            datetime(2020, 1, 1, 9, 6, 0),\n        ],\n        \"stock\": [\"A\", \"B\", \"B\", \"C\"],\n        \"trade\": [101, 299, 301, 500],\n    }\n)\nprint(df_trades)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use chrono::prelude::*;\nlet df_trades = df!(\n    \"time\"=&gt; &amp;[\n    NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 1, 0).unwrap(),\n    NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 1, 0).unwrap(),\n    NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 3, 0).unwrap(),\n    NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 6, 0).unwrap(),\n        ],\n        \"stock\"=&gt; &amp;[\"A\", \"B\", \"B\", \"C\"],\n        \"trade\"=&gt; &amp;[101, 299, 301, 500],\n)?;\nprintln!(\"{}\", &amp;df_trades);\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 trade \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:01:00 \u2506 A     \u2506 101   \u2502\n\u2502 2020-01-01 09:01:00 \u2506 B     \u2506 299   \u2502\n\u2502 2020-01-01 09:03:00 \u2506 B     \u2506 301   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 C     \u2506 500   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The broker has another <code>DataFrame</code> called <code>df_quotes</code> showing prices it has quoted for these stocks.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df_quotes = pl.DataFrame(\n    {\n        \"time\": [\n            datetime(2020, 1, 1, 9, 0, 0),\n            datetime(2020, 1, 1, 9, 2, 0),\n            datetime(2020, 1, 1, 9, 4, 0),\n            datetime(2020, 1, 1, 9, 6, 0),\n        ],\n        \"stock\": [\"A\", \"B\", \"C\", \"A\"],\n        \"quote\": [100, 300, 501, 102],\n    }\n)\n\nprint(df_quotes)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df_quotes = df!(\n        \"time\"=&gt; &amp;[\n    NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 0, 0).unwrap(),\n    NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 2, 0).unwrap(),\n    NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 4, 0).unwrap(),\n    NaiveDate::from_ymd_opt(2020, 1, 1).unwrap().and_hms_opt(9, 6, 0).unwrap(),\n        ],\n        \"stock\"=&gt; &amp;[\"A\", \"B\", \"C\", \"A\"],\n        \"quote\"=&gt; &amp;[100, 300, 501, 102],\n)?;\n\nprintln!(\"{}\", &amp;df_quotes);\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 quote \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:00:00 \u2506 A     \u2506 100   \u2502\n\u2502 2020-01-01 09:02:00 \u2506 B     \u2506 300   \u2502\n\u2502 2020-01-01 09:04:00 \u2506 C     \u2506 501   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 A     \u2506 102   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You want to produce a <code>DataFrame</code> showing for each trade the most recent quote provided before the trade. You do this with <code>join_asof</code> (using the default <code>strategy = \"backward\"</code>). To avoid joining between trades on one stock with a quote on another you must specify an exact preliminary join on the stock column with <code>by=\"stock\"</code>.</p>  Python Rust <p> <code>join_asof</code> <pre><code>df_asof_join = df_trades.join_asof(df_quotes, on=\"time\", by=\"stock\")\nprint(df_asof_join)\n</code></pre></p> <p> <code>join_asof</code> <pre><code>let df_asof_join = df_trades.join_asof_by(\n    &amp;df_quotes,\n    \"time\",\n    \"time\",\n    [\"stock\"],\n    [\"stock\"],\n    AsofStrategy::Backward,\n    None,\n)?;\nprintln!(\"{}\", &amp;df_asof_join);\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 trade \u2506 quote \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:01:00 \u2506 A     \u2506 101   \u2506 100   \u2502\n\u2502 2020-01-01 09:01:00 \u2506 B     \u2506 299   \u2506 null  \u2502\n\u2502 2020-01-01 09:03:00 \u2506 B     \u2506 301   \u2506 300   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 C     \u2506 500   \u2506 501   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If you want to make sure that only quotes within a certain time range are joined to the trades you can specify the <code>tolerance</code> argument. In this case we want to make sure that the last preceding quote is within 1 minute of the trade so we set <code>tolerance = \"1m\"</code>.</p>  Python <pre><code>df_asof_tolerance_join = df_trades.join_asof(\n    df_quotes, on=\"time\", by=\"stock\", tolerance=\"1m\"\n)\nprint(df_asof_tolerance_join)\n</code></pre> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 trade \u2506 quote \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:01:00 \u2506 A     \u2506 101   \u2506 100   \u2502\n\u2502 2020-01-01 09:01:00 \u2506 B     \u2506 299   \u2506 null  \u2502\n\u2502 2020-01-01 09:03:00 \u2506 B     \u2506 301   \u2506 300   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 C     \u2506 500   \u2506 null  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/melt/","title":"Melts","text":"<p>Melt operations unpivot a DataFrame from wide format to long format</p>"},{"location":"user-guide/transformations/melt/#dataset","title":"Dataset","text":"Python Rust <p> <code>DataFrame</code> <pre><code>import polars as pl\n\ndf = pl.DataFrame(\n    {\n        \"A\": [\"a\", \"b\", \"a\"],\n        \"B\": [1, 3, 5],\n        \"C\": [10, 11, 12],\n        \"D\": [2, 4, 6],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = df!(\n        \"A\"=&gt; &amp;[\"a\", \"b\", \"a\"],\n        \"B\"=&gt; &amp;[1, 3, 5],\n        \"C\"=&gt; &amp;[10, 11, 12],\n        \"D\"=&gt; &amp;[2, 4, 6],\n)?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 A   \u2506 B   \u2506 C   \u2506 D   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 1   \u2506 10  \u2506 2   \u2502\n\u2502 b   \u2506 3   \u2506 11  \u2506 4   \u2502\n\u2502 a   \u2506 5   \u2506 12  \u2506 6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/melt/#eager-lazy","title":"Eager + lazy","text":"<p><code>Eager</code> and <code>lazy</code> have the same API.</p>  Python Rust <p> <code>melt</code> <pre><code>out = df.melt(id_vars=[\"A\", \"B\"], value_vars=[\"C\", \"D\"])\nprint(out)\n</code></pre></p> <p> <code>melt</code> <pre><code>let out = df.melt([\"A\", \"B\"], [\"C\", \"D\"])?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (6, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 A   \u2506 B   \u2506 variable \u2506 value \u2502\n\u2502 --- \u2506 --- \u2506 ---      \u2506 ---   \u2502\n\u2502 str \u2506 i64 \u2506 str      \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 1   \u2506 C        \u2506 10    \u2502\n\u2502 b   \u2506 3   \u2506 C        \u2506 11    \u2502\n\u2502 a   \u2506 5   \u2506 C        \u2506 12    \u2502\n\u2502 a   \u2506 1   \u2506 D        \u2506 2     \u2502\n\u2502 b   \u2506 3   \u2506 D        \u2506 4     \u2502\n\u2502 a   \u2506 5   \u2506 D        \u2506 6     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/pivot/","title":"Pivots","text":"<p>Pivot a column in a <code>DataFrame</code> and perform one of the following aggregations:</p> <ul> <li>first</li> <li>sum</li> <li>min</li> <li>max</li> <li>mean</li> <li>median</li> </ul> <p>The pivot operation consists of a group by one, or multiple columns (these will be the new y-axis), the column that will be pivoted (this will be the new x-axis) and an aggregation.</p>"},{"location":"user-guide/transformations/pivot/#dataset","title":"Dataset","text":"Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"foo\": [\"A\", \"A\", \"B\", \"B\", \"C\"],\n        \"N\": [1, 2, 2, 4, 2],\n        \"bar\": [\"k\", \"l\", \"m\", \"n\", \"o\"],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = df!(\n        \"foo\"=&gt; [\"A\", \"A\", \"B\", \"B\", \"C\"],\n        \"bar\"=&gt; [\"k\", \"l\", \"m\", \"n\", \"o\"],\n        \"N\"=&gt; [1, 2, 2, 4, 2],\n)?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 N   \u2506 bar \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 1   \u2506 k   \u2502\n\u2502 A   \u2506 2   \u2506 l   \u2502\n\u2502 B   \u2506 2   \u2506 m   \u2502\n\u2502 B   \u2506 4   \u2506 n   \u2502\n\u2502 C   \u2506 2   \u2506 o   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/pivot/#eager","title":"Eager","text":"Python Rust <p> <code>pivot</code> <pre><code>out = df.pivot(index=\"foo\", columns=\"bar\", values=\"N\", aggregate_function=\"first\")\nprint(out)\n</code></pre></p> <p> <code>pivot</code> <pre><code>let out = pivot(&amp;df, [\"foo\"], [\"bar\"], Some([\"N\"]), false, None, None)?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (3, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 k    \u2506 l    \u2506 m    \u2506 n    \u2506 o    \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 1    \u2506 2    \u2506 null \u2506 null \u2506 null \u2502\n\u2502 B   \u2506 null \u2506 null \u2506 2    \u2506 4    \u2506 null \u2502\n\u2502 C   \u2506 null \u2506 null \u2506 null \u2506 null \u2506 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/pivot/#lazy","title":"Lazy","text":"<p>A Polars <code>LazyFrame</code> always need to know the schema of a computation statically (before collecting the query). As a pivot's output schema depends on the data, and it is therefore impossible to determine the schema without running the query.</p> <p>Polars could have abstracted this fact for you just like Spark does, but we don't want you to shoot yourself in the foot with a shotgun. The cost should be clear upfront.</p>  Python Rust <p> <code>pivot</code> <pre><code>q = (\n    df.lazy()\n    .collect()\n    .pivot(index=\"foo\", columns=\"bar\", values=\"N\", aggregate_function=\"first\")\n    .lazy()\n)\nout = q.collect()\nprint(out)\n</code></pre></p> <p> <code>pivot</code> <pre><code>let q = df.lazy();\nlet q2 = pivot(\n    &amp;q.collect()?,\n    [\"foo\"],\n    [\"bar\"],\n    Some([\"N\"]),\n    false,\n    None,\n    None,\n)?\n.lazy();\nlet out = q2.collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (3, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 k    \u2506 l    \u2506 m    \u2506 n    \u2506 o    \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 1    \u2506 2    \u2506 null \u2506 null \u2506 null \u2502\n\u2502 B   \u2506 null \u2506 null \u2506 2    \u2506 4    \u2506 null \u2502\n\u2502 C   \u2506 null \u2506 null \u2506 null \u2506 null \u2506 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/filter/","title":"Filtering","text":"<p>Filtering date columns works in the same way as with other types of columns using the <code>.filter</code> method.</p> <p>Polars uses Python's native <code>datetime</code>, <code>date</code> and <code>timedelta</code> for equality comparisons between the datatypes <code>pl.Datetime</code>, <code>pl.Date</code> and <code>pl.Duration</code>.</p> <p>In the following example we use a time series of Apple stock prices.</p>  Python Rust <p> <code>read_csv</code> <pre><code>import polars as pl\nfrom datetime import datetime\n\ndf = pl.read_csv(\"docs/data/apple_stock.csv\", try_parse_dates=True)\nprint(df)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>let df = CsvReader::from_path(\"docs/data/apple_stock.csv\")\n    .unwrap()\n    .with_try_parse_dates(true)\n    .finish()\n    .unwrap();\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 1982-07-08 \u2506 11.0   \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-05-16 \u2506 546.08 \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/filter/#filtering-by-single-dates","title":"Filtering by single dates","text":"<p>We can filter by a single date by casting the desired date string to a <code>Date</code> object in a filter expression:</p>  Python Rust <p> <code>filter</code> <pre><code>filtered_df = df.filter(\n    pl.col(\"Date\") == datetime(1995, 10, 16),\n)\nprint(filtered_df)\n</code></pre></p> <p> <code>filter</code> <pre><code>let filtered_df = df\n    .clone()\n    .lazy()\n    .filter(col(\"Date\").eq(lit(NaiveDate::from_ymd_opt(1995, 10, 16).unwrap())))\n    .collect()?;\nprintln!(\"{}\", &amp;filtered_df);\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close \u2502\n\u2502 ---        \u2506 ---   \u2502\n\u2502 date       \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1995-10-16 \u2506 36.13 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note we are using the lowercase <code>datetime</code> method rather than the uppercase <code>Datetime</code> data type.</p>"},{"location":"user-guide/transformations/time-series/filter/#filtering-by-a-date-range","title":"Filtering by a date range","text":"<p>We can filter by a range of dates using the <code>is_between</code> method in a filter expression with the start and end dates:</p>  Python Rust <p> <code>filter</code> \u00b7 <code>is_between</code> <pre><code>filtered_range_df = df.filter(\n    pl.col(\"Date\").is_between(datetime(1995, 7, 1), datetime(1995, 11, 1)),\n)\nprint(filtered_range_df)\n</code></pre></p> <p> <code>filter</code> \u00b7 <code>is_between</code> <pre><code>let filtered_range_df = df\n    .clone()\n    .lazy()\n    .filter(\n        col(\"Date\")\n            .gt(lit(NaiveDate::from_ymd_opt(1995, 7, 1).unwrap()))\n            .and(col(\"Date\").lt(lit(NaiveDate::from_ymd_opt(1995, 11, 1).unwrap()))),\n    )\n    .collect()?;\nprintln!(\"{}\", &amp;filtered_range_df);\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close \u2502\n\u2502 ---        \u2506 ---   \u2502\n\u2502 date       \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1995-07-06 \u2506 47.0  \u2502\n\u2502 1995-10-16 \u2506 36.13 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/filter/#filtering-with-negative-dates","title":"Filtering with negative dates","text":"<p>Say you are working with an archeologist and are dealing in negative dates. Polars can parse and store them just fine, but the Python <code>datetime</code> library does not. So for filtering, you should use attributes in the <code>.dt</code> namespace:</p>  Python Rust <p> <code>str.to_date</code> <pre><code>ts = pl.Series([\"-1300-05-23\", \"-1400-03-02\"]).str.to_date()\n\nnegative_dates_df = pl.DataFrame({\"ts\": ts, \"values\": [3, 4]})\n\nnegative_dates_filtered_df = negative_dates_df.filter(pl.col(\"ts\").dt.year() &lt; -1300)\nprint(negative_dates_filtered_df)\n</code></pre></p> <p> <code>str.replace_all</code> \u00b7  Available on feature dtype-date <pre><code>    let negative_dates_df = df!(\n    \"ts\"=&gt; &amp;[\"-1300-05-23\", \"-1400-03-02\"],\n    \"values\"=&gt; &amp;[3, 4])?\n    .lazy()\n    .with_column(col(\"ts\").str().to_date(StrptimeOptions::default()))\n    .collect()?;\n\n    let negative_dates_filtered_df = negative_dates_df\n        .clone()\n        .lazy()\n        .filter(col(\"ts\").dt().year().lt(-1300))\n        .collect()?;\n    println!(\"{}\", &amp;negative_dates_filtered_df);\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ts          \u2506 values \u2502\n\u2502 ---         \u2506 ---    \u2502\n\u2502 date        \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 -1400-03-02 \u2506 4      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/parsing/","title":"Parsing","text":"<p>Polars has native support for parsing time series data and doing more sophisticated operations such as temporal grouping and resampling.</p>"},{"location":"user-guide/transformations/time-series/parsing/#datatypes","title":"Datatypes","text":"<p>Polars has the following datetime datatypes:</p> <ul> <li><code>Date</code>: Date representation e.g. 2014-07-08. It is internally represented as days since UNIX epoch encoded by a 32-bit signed integer.</li> <li><code>Datetime</code>: Datetime representation e.g. 2014-07-08 07:00:00. It is internally represented as a 64 bit integer since the Unix epoch and can have different units such as ns, us, ms.</li> <li><code>Duration</code>: A time delta type that is created when subtracting <code>Date/Datetime</code>. Similar to <code>timedelta</code> in Python.</li> <li><code>Time</code>: Time representation, internally represented as nanoseconds since midnight.</li> </ul>"},{"location":"user-guide/transformations/time-series/parsing/#parsing-dates-from-a-file","title":"Parsing dates from a file","text":"<p>When loading from a CSV file Polars attempts to parse dates and times if the <code>try_parse_dates</code> flag is set to <code>True</code>:</p>  Python Rust <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"docs/data/apple_stock.csv\", try_parse_dates=True)\nprint(df)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>let df = CsvReader::from_path(\"docs/data/apple_stock.csv\")\n    .unwrap()\n    .with_try_parse_dates(true)\n    .finish()\n    .unwrap();\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 1982-07-08 \u2506 11.0   \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-05-16 \u2506 546.08 \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>On the other hand binary formats such as parquet have a schema that is respected by Polars.</p>"},{"location":"user-guide/transformations/time-series/parsing/#casting-strings-to-dates","title":"Casting strings to dates","text":"<p>You can also cast a column of datetimes encoded as strings to a datetime type. You do this by calling the string <code>str.to_date</code> method and passing the format of the date string:</p>  Python Rust <p> <code>read_csv</code> \u00b7 <code>str.to_date</code> <pre><code>df = pl.read_csv(\"docs/data/apple_stock.csv\", try_parse_dates=False)\n\ndf = df.with_columns(pl.col(\"Date\").str.to_date(\"%Y-%m-%d\"))\nprint(df)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7 <code>str.replace_all</code> \u00b7  Available on feature dtype-date \u00b7  Available on feature csv <pre><code>let df = CsvReader::from_path(\"docs/data/apple_stock.csv\")\n    .unwrap()\n    .with_try_parse_dates(false)\n    .finish()\n    .unwrap();\nlet df = df\n    .clone()\n    .lazy()\n    .with_columns([col(\"Date\").str().to_date(StrptimeOptions::default())])\n    .collect()?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 1982-07-08 \u2506 11.0   \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-05-16 \u2506 546.08 \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The format string specification can be found here..</p>"},{"location":"user-guide/transformations/time-series/parsing/#extracting-date-features-from-a-date-column","title":"Extracting date features from a date column","text":"<p>You can extract data features such as the year or day from a date column using the <code>.dt</code> namespace on a date column:</p>  Python Rust <p> <code>dt.year</code> <pre><code>df_with_year = df.with_columns(pl.col(\"Date\").dt.year().alias(\"year\"))\nprint(df_with_year)\n</code></pre></p> <p> <code>dt.year</code> <pre><code>let df_with_year = df\n    .clone()\n    .lazy()\n    .with_columns([col(\"Date\").dt().year().alias(\"year\")])\n    .collect()?;\nprintln!(\"{}\", &amp;df_with_year);\n</code></pre></p> <pre><code>shape: (100, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2506 year \u2502\n\u2502 ---        \u2506 ---    \u2506 ---  \u2502\n\u2502 date       \u2506 f64    \u2506 i32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2506 1981 \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2506 1981 \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2506 1981 \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2506 1981 \u2502\n\u2502 1982-07-08 \u2506 11.0   \u2506 1982 \u2502\n\u2502 \u2026          \u2506 \u2026      \u2506 \u2026    \u2502\n\u2502 2012-05-16 \u2506 546.08 \u2506 2012 \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2506 2012 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2506 2013 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2506 2013 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2506 2014 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/parsing/#mixed-offsets","title":"Mixed offsets","text":"<p>If you have mixed offsets (say, due to crossing daylight saving time), then you can use <code>utc=True</code> and then convert to your time zone:</p>  Python Rust <p> <code>str.to_datetime</code> \u00b7 <code>dt.convert_time_zone</code> \u00b7  Available on feature timezone <pre><code>data = [\n    \"2021-03-27T00:00:00+0100\",\n    \"2021-03-28T00:00:00+0100\",\n    \"2021-03-29T00:00:00+0200\",\n    \"2021-03-30T00:00:00+0200\",\n]\nmixed_parsed = (\n    pl.Series(data)\n    .str.to_datetime(\"%Y-%m-%dT%H:%M:%S%z\")\n    .dt.convert_time_zone(\"Europe/Brussels\")\n)\nprint(mixed_parsed)\n</code></pre></p> <p> <code>str.replace_all</code> \u00b7 <code>dt.convert_time_zone</code> \u00b7  Available on feature timezones \u00b7  Available on feature dtype-datetime <pre><code>let data = [\n    \"2021-03-27T00:00:00+0100\",\n    \"2021-03-28T00:00:00+0100\",\n    \"2021-03-29T00:00:00+0200\",\n    \"2021-03-30T00:00:00+0200\",\n];\nlet q = col(\"date\")\n    .str()\n    .to_datetime(\n        Some(TimeUnit::Microseconds),\n        None,\n        StrptimeOptions {\n            format: Some(\"%Y-%m-%dT%H:%M:%S%z\".to_string()),\n            ..Default::default()\n        },\n        lit(\"raise\"),\n    )\n    .dt()\n    .convert_time_zone(\"Europe/Brussels\".to_string());\nlet mixed_parsed = df!(\"date\" =&gt; &amp;data)?.lazy().select([q]).collect()?;\n\nprintln!(\"{}\", &amp;mixed_parsed);\n</code></pre></p> <pre><code>shape: (4,)\nSeries: '' [datetime[\u03bcs, Europe/Brussels]]\n[\n    2021-03-27 00:00:00 CET\n    2021-03-28 00:00:00 CET\n    2021-03-29 00:00:00 CEST\n    2021-03-30 00:00:00 CEST\n]\n</code></pre>"},{"location":"user-guide/transformations/time-series/resampling/","title":"Resampling","text":"<p>We can resample by either:</p> <ul> <li>upsampling (moving data to a higher frequency)</li> <li>downsampling (moving data to a lower frequency)</li> <li>combinations of these e.g. first upsample and then downsample</li> </ul>"},{"location":"user-guide/transformations/time-series/resampling/#downsampling-to-a-lower-frequency","title":"Downsampling to a lower frequency","text":"<p>Polars views downsampling as a special case of the group_by operation and you can do this with <code>group_by_dynamic</code> and <code>group_by_rolling</code> - see the temporal group by page for examples.</p>"},{"location":"user-guide/transformations/time-series/resampling/#upsampling-to-a-higher-frequency","title":"Upsampling to a higher frequency","text":"<p>Let's go through an example where we generate data at 30 minute intervals:</p>  Python Rust <p> <code>DataFrame</code> \u00b7 <code>date_range</code> <pre><code>df = pl.DataFrame(\n    {\n        \"time\": pl.datetime_range(\n            start=datetime(2021, 12, 16),\n            end=datetime(2021, 12, 16, 3),\n            interval=\"30m\",\n            eager=True,\n        ),\n        \"groups\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\"],\n        \"values\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> \u00b7 <code>date_range</code> \u00b7  Available on feature range \u00b7  Available on feature dtype-date <pre><code>let time = polars::time::date_range(\n    \"time\",\n    NaiveDate::from_ymd_opt(2021, 12, 16)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    NaiveDate::from_ymd_opt(2021, 12, 16)\n        .unwrap()\n        .and_hms_opt(3, 0, 0)\n        .unwrap(),\n    Duration::parse(\"30m\"),\n    ClosedWindow::Both,\n    TimeUnit::Milliseconds,\n    None,\n)?;\nlet df = df!(\n    \"time\" =&gt; time,\n    \"groups\" =&gt; &amp;[\"a\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\"],\n    \"values\" =&gt; &amp;[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],\n)?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2506 values \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 01:00:00 \u2506 a      \u2506 3.0    \u2502\n\u2502 2021-12-16 01:30:00 \u2506 b      \u2506 4.0    \u2502\n\u2502 2021-12-16 02:00:00 \u2506 b      \u2506 5.0    \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2506 7.0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Upsampling can be done by defining the new sampling interval. By upsampling we are adding in extra rows where we do not have data. As such upsampling by itself gives a DataFrame with nulls. These nulls can then be filled with a fill strategy or interpolation.</p>"},{"location":"user-guide/transformations/time-series/resampling/#upsampling-strategies","title":"Upsampling strategies","text":"<p>In this example we upsample from the original 30 minutes to 15 minutes and then use a <code>forward</code> strategy to replace the nulls with the previous non-null value:</p>  Python Rust <p> <code>upsample</code> <pre><code>out1 = df.upsample(time_column=\"time\", every=\"15m\").fill_null(strategy=\"forward\")\nprint(out1)\n</code></pre></p> <p> <code>upsample</code> <pre><code>let out1 = df\n    .clone()\n    .upsample::&lt;[String; 0]&gt;([], \"time\", Duration::parse(\"15m\"), Duration::parse(\"0\"))?\n    .fill_null(FillNullStrategy::Forward(None))?;\nprintln!(\"{}\", &amp;out1);\n</code></pre></p> <pre><code>shape: (13, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2506 values \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:15:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 00:45:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 01:00:00 \u2506 a      \u2506 3.0    \u2502\n\u2502 \u2026                   \u2506 \u2026      \u2506 \u2026      \u2502\n\u2502 2021-12-16 02:00:00 \u2506 b      \u2506 5.0    \u2502\n\u2502 2021-12-16 02:15:00 \u2506 b      \u2506 5.0    \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 02:45:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2506 7.0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example we instead fill the nulls by linear interpolation:</p>  Python Rust <p> <code>upsample</code> \u00b7 <code>interpolate</code> \u00b7 <code>fill_null</code> <pre><code>out2 = (\n    df.upsample(time_column=\"time\", every=\"15m\")\n    .interpolate()\n    .fill_null(strategy=\"forward\")\n)\nprint(out2)\n</code></pre></p> <p> <code>upsample</code> \u00b7 <code>interpolate</code> \u00b7 <code>fill_null</code> <pre><code>let out2 = df\n    .clone()\n    .upsample::&lt;[String; 0]&gt;([], \"time\", Duration::parse(\"15m\"), Duration::parse(\"0\"))?\n    .lazy()\n    .with_columns([col(\"values\").interpolate(InterpolationMethod::Linear)])\n    .collect()?\n    .fill_null(FillNullStrategy::Forward(None))?;\nprintln!(\"{}\", &amp;out2);\n</code></pre></p> <pre><code>shape: (13, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2506 values \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:15:00 \u2506 a      \u2506 1.5    \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 00:45:00 \u2506 a      \u2506 2.5    \u2502\n\u2502 2021-12-16 01:00:00 \u2506 a      \u2506 3.0    \u2502\n\u2502 \u2026                   \u2506 \u2026      \u2506 \u2026      \u2502\n\u2502 2021-12-16 02:00:00 \u2506 b      \u2506 5.0    \u2502\n\u2502 2021-12-16 02:15:00 \u2506 b      \u2506 5.5    \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 02:45:00 \u2506 a      \u2506 6.5    \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2506 7.0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/","title":"Grouping","text":""},{"location":"user-guide/transformations/time-series/rolling/#grouping-by-fixed-windows","title":"Grouping by fixed windows","text":"<p>We can calculate temporal statistics using <code>group_by_dynamic</code> to group rows into days/months/years etc.</p>"},{"location":"user-guide/transformations/time-series/rolling/#annual-average-example","title":"Annual average example","text":"<p>In following simple example we calculate the annual average closing price of Apple stock prices. We first load the data from CSV:</p>  Python Rust <p> <code>upsample</code> <pre><code>df = pl.read_csv(\"docs/data/apple_stock.csv\", try_parse_dates=True)\ndf = df.sort(\"Date\")\nprint(df)\n</code></pre></p> <p> <code>upsample</code> <pre><code>let df = CsvReader::from_path(\"docs/data/apple_stock.csv\")\n    .unwrap()\n    .with_try_parse_dates(true)\n    .finish()\n    .unwrap()\n    .sort(\n        [\"Date\"],\n        SortMultipleOptions::default().with_maintain_order(true),\n    )?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 1982-07-08 \u2506 11.0   \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-05-16 \u2506 546.08 \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Info</p> <p>The dates are sorted in ascending order - if they are not sorted in this way the <code>group_by_dynamic</code> output will not be correct!</p> <p>To get the annual average closing price we tell <code>group_by_dynamic</code> that we want to:</p> <ul> <li>group by the <code>Date</code> column on an annual (<code>1y</code>) basis</li> <li>take the mean values of the <code>Close</code> column for each year:</li> </ul>  Python Rust <p> <code>group_by_dynamic</code> <pre><code>annual_average_df = df.group_by_dynamic(\"Date\", every=\"1y\").agg(pl.col(\"Close\").mean())\n\ndf_with_year = annual_average_df.with_columns(pl.col(\"Date\").dt.year().alias(\"year\"))\nprint(df_with_year)\n</code></pre></p> <p> <code>group_by_dynamic</code> \u00b7  Available on feature dynamic_group_by <pre><code>let annual_average_df = df\n    .clone()\n    .lazy()\n    .group_by_dynamic(\n        col(\"Date\"),\n        [],\n        DynamicGroupOptions {\n            every: Duration::parse(\"1y\"),\n            period: Duration::parse(\"1y\"),\n            offset: Duration::parse(\"0\"),\n            ..Default::default()\n        },\n    )\n    .agg([col(\"Close\").mean()])\n    .collect()?;\n\nlet df_with_year = annual_average_df\n    .lazy()\n    .with_columns([col(\"Date\").dt().year().alias(\"year\")])\n    .collect()?;\nprintln!(\"{}\", &amp;df_with_year);\n</code></pre></p> <p>The annual average closing price is then:</p> <pre><code>shape: (34, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close     \u2506 year \u2502\n\u2502 ---        \u2506 ---       \u2506 ---  \u2502\n\u2502 date       \u2506 f64       \u2506 i32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-01-01 \u2506 23.5625   \u2506 1981 \u2502\n\u2502 1982-01-01 \u2506 11.0      \u2506 1982 \u2502\n\u2502 1983-01-01 \u2506 30.543333 \u2506 1983 \u2502\n\u2502 1984-01-01 \u2506 27.583333 \u2506 1984 \u2502\n\u2502 1985-01-01 \u2506 18.166667 \u2506 1985 \u2502\n\u2502 \u2026          \u2506 \u2026         \u2506 \u2026    \u2502\n\u2502 2010-01-01 \u2506 278.265   \u2506 2010 \u2502\n\u2502 2011-01-01 \u2506 368.225   \u2506 2011 \u2502\n\u2502 2012-01-01 \u2506 560.965   \u2506 2012 \u2502\n\u2502 2013-01-01 \u2506 464.955   \u2506 2013 \u2502\n\u2502 2014-01-01 \u2506 522.06    \u2506 2014 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/#parameters-for-group_by_dynamic","title":"Parameters for <code>group_by_dynamic</code>","text":"<p>A dynamic window is defined by a:</p> <ul> <li>every: indicates the interval of the window</li> <li>period: indicates the duration of the window</li> <li>offset: can be used to offset the start of the windows</li> </ul> <p>The value for <code>every</code> sets how often the groups start. The time period values are flexible - for example we could take:</p> <ul> <li>the average over 2 year intervals by replacing <code>1y</code> with <code>2y</code></li> <li>the average over 18 month periods by replacing <code>1y</code> with <code>1y6mo</code></li> </ul> <p>We can also use the <code>period</code> parameter to set how long the time period for each group is. For example, if we set the <code>every</code> parameter to be <code>1y</code> and the <code>period</code> parameter to be <code>2y</code> then we would get groups at one year intervals where each groups spanned two years.</p> <p>If the <code>period</code> parameter is not specified then it is set equal to the <code>every</code> parameter so that if the <code>every</code> parameter is set to be <code>1y</code> then each group spans <code>1y</code> as well.</p> <p>Because every does not have to be equal to period, we can create many groups in a very flexible way. They may overlap or leave boundaries between them.</p> <p>Let's see how the windows for some parameter combinations would look. Let's start out boring. \ud83e\udd71</p> <ul> <li>every: 1 day -&gt; <code>\"1d\"</code></li> <li>period: 1 day -&gt; <code>\"1d\"</code></li> </ul> <pre><code>this creates adjacent windows of the same size\n|--|\n   |--|\n      |--|\n</code></pre> <ul> <li>every: 1 day -&gt; <code>\"1d\"</code></li> <li>period: 2 days -&gt; <code>\"2d\"</code></li> </ul> <pre><code>these windows have an overlap of 1 day\n|----|\n   |----|\n      |----|\n</code></pre> <ul> <li>every: 2 days -&gt; <code>\"2d\"</code></li> <li>period: 1 day -&gt; <code>\"1d\"</code></li> </ul> <pre><code>this would leave gaps between the windows\ndata points that in these gaps will not be a member of any group\n|--|\n       |--|\n              |--|\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/#truncate","title":"<code>truncate</code>","text":"<p>The <code>truncate</code> parameter is a Boolean variable that determines what datetime value is associated with each group in the output. In the example above the first data point is on 23rd February 1981. If <code>truncate = True</code> (the default) then the date for the first year in the annual average is 1st January 1981. However, if <code>truncate = False</code> then the date for the first year in the annual average is the date of the first data point on 23rd February 1981. Note that <code>truncate</code> only affects what's shown in the <code>Date</code> column and does not affect the window boundaries.</p>"},{"location":"user-guide/transformations/time-series/rolling/#using-expressions-in-group_by_dynamic","title":"Using expressions in <code>group_by_dynamic</code>","text":"<p>We aren't restricted to using simple aggregations like <code>mean</code> in a group by operation - we can use the full range of expressions available in Polars.</p> <p>In the snippet below we create a <code>date range</code> with every day (<code>\"1d\"</code>) in 2021 and turn this into a <code>DataFrame</code>.</p> <p>Then in the <code>group_by_dynamic</code> we create dynamic windows that start every month (<code>\"1mo\"</code>) and have a window length of <code>1</code> month. The values that match these dynamic windows are then assigned to that group and can be aggregated with the powerful expression API.</p> <p>Below we show an example where we use group_by_dynamic to compute:</p> <ul> <li>the number of days until the end of the month</li> <li>the number of days in a month</li> </ul>  Python Rust <p> <code>group_by_dynamic</code> \u00b7 <code>DataFrame.explode</code> \u00b7 <code>date_range</code> <pre><code>df = (\n    pl.date_range(\n        start=date(2021, 1, 1),\n        end=date(2021, 12, 31),\n        interval=\"1d\",\n        eager=True,\n    )\n    .alias(\"time\")\n    .to_frame()\n)\n\nout = df.group_by_dynamic(\"time\", every=\"1mo\", period=\"1mo\", closed=\"left\").agg(\n    pl.col(\"time\").cum_count().reverse().head(3).alias(\"day/eom\"),\n    ((pl.col(\"time\") - pl.col(\"time\").first()).last().dt.total_days() + 1).alias(\n        \"days_in_month\"\n    ),\n)\nprint(out)\n</code></pre></p> <p> <code>group_by_dynamic</code> \u00b7 <code>DataFrame.explode</code> \u00b7 <code>date_range</code> \u00b7  Available on feature dynamic_group_by \u00b7  Available on feature range \u00b7  Available on feature dtype-date <pre><code>let time = polars::time::date_range(\n    \"time\",\n    NaiveDate::from_ymd_opt(2021, 1, 1)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    NaiveDate::from_ymd_opt(2021, 12, 31)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    Duration::parse(\"1d\"),\n    ClosedWindow::Both,\n    TimeUnit::Milliseconds,\n    None,\n)?\n.cast(&amp;DataType::Date)?;\n\nlet df = df!(\n    \"time\" =&gt; time,\n)?;\n\nlet out = df\n    .clone()\n    .lazy()\n    .group_by_dynamic(\n        col(\"time\"),\n        [],\n        DynamicGroupOptions {\n            every: Duration::parse(\"1mo\"),\n            period: Duration::parse(\"1mo\"),\n            offset: Duration::parse(\"0\"),\n            closed_window: ClosedWindow::Left,\n            ..Default::default()\n        },\n    )\n    .agg([\n        col(\"time\")\n            .cum_count(true) // python example has false\n            .reverse()\n            .head(Some(3))\n            .alias(\"day/eom\"),\n        ((col(\"time\").last() - col(\"time\").first()).map(\n            // had to use map as .duration().days() is not available\n            |s| {\n                Ok(Some(\n                    s.duration()?\n                        .into_iter()\n                        .map(|d| d.map(|v| v / 1000 / 24 / 60 / 60))\n                        .collect::&lt;Int64Chunked&gt;()\n                        .into_series(),\n                ))\n            },\n            GetOutput::from_type(DataType::Int64),\n        ) + lit(1))\n        .alias(\"days_in_month\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (12, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time       \u2506 day/eom      \u2506 days_in_month \u2502\n\u2502 ---        \u2506 ---          \u2506 ---           \u2502\n\u2502 date       \u2506 list[u32]    \u2506 i64           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-01-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2502 2021-02-01 \u2506 [28, 27, 26] \u2506 28            \u2502\n\u2502 2021-03-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2502 2021-04-01 \u2506 [30, 29, 28] \u2506 30            \u2502\n\u2502 2021-05-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2502 \u2026          \u2506 \u2026            \u2506 \u2026             \u2502\n\u2502 2021-08-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2502 2021-09-01 \u2506 [30, 29, 28] \u2506 30            \u2502\n\u2502 2021-10-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2502 2021-11-01 \u2506 [30, 29, 28] \u2506 30            \u2502\n\u2502 2021-12-01 \u2506 [31, 30, 29] \u2506 31            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/#grouping-by-rolling-windows","title":"Grouping by rolling windows","text":"<p>The rolling operation, <code>rolling</code>, is another entrance to the <code>group_by</code>/<code>agg</code> context. But different from the <code>group_by_dynamic</code> where the windows are fixed by a parameter <code>every</code> and <code>period</code>. In a <code>rolling</code>, the windows are not fixed at all! They are determined by the values in the <code>index_column</code>.</p> <p>So imagine having a time column with the values <code>{2021-01-06, 2021-01-10}</code> and a <code>period=\"5d\"</code> this would create the following windows:</p> <pre><code>2021-01-01   2021-01-06\n    |----------|\n\n       2021-01-05   2021-01-10\n             |----------|\n</code></pre> <p>Because the windows of a rolling group by are always determined by the values in the <code>DataFrame</code> column, the number of groups is always equal to the original <code>DataFrame</code>.</p>"},{"location":"user-guide/transformations/time-series/rolling/#combining-group-by-operations","title":"Combining group by operations","text":"<p>Rolling and dynamic group by operations can be combined with normal group by operations.</p> <p>Below is an example with a dynamic group by.</p>  Python Rust <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"time\": pl.datetime_range(\n            start=datetime(2021, 12, 16),\n            end=datetime(2021, 12, 16, 3),\n            interval=\"30m\",\n            eager=True,\n        ),\n        \"groups\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\"],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let time = polars::time::date_range(\n    \"time\",\n    NaiveDate::from_ymd_opt(2021, 12, 16)\n        .unwrap()\n        .and_hms_opt(0, 0, 0)\n        .unwrap(),\n    NaiveDate::from_ymd_opt(2021, 12, 16)\n        .unwrap()\n        .and_hms_opt(3, 0, 0)\n        .unwrap(),\n    Duration::parse(\"30m\"),\n    ClosedWindow::Both,\n    TimeUnit::Milliseconds,\n    None,\n)?;\nlet df = df!(\n    \"time\" =&gt; time,\n    \"groups\"=&gt; [\"a\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\"],\n)?;\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <pre><code>shape: (7, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2502\n\u2502 ---                 \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2502\n\u2502 2021-12-16 01:00:00 \u2506 a      \u2502\n\u2502 2021-12-16 01:30:00 \u2506 b      \u2502\n\u2502 2021-12-16 02:00:00 \u2506 b      \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust <p> <code>group_by_dynamic</code> <pre><code>out = df.group_by_dynamic(\n    \"time\",\n    every=\"1h\",\n    closed=\"both\",\n    group_by=\"groups\",\n    include_boundaries=True,\n).agg(pl.len())\nprint(out)\n</code></pre></p> <p> <code>group_by_dynamic</code> \u00b7  Available on feature dynamic_group_by <pre><code>let out = df\n    .clone()\n    .lazy()\n    .group_by_dynamic(\n        col(\"time\"),\n        [col(\"groups\")],\n        DynamicGroupOptions {\n            every: Duration::parse(\"1h\"),\n            period: Duration::parse(\"1h\"),\n            offset: Duration::parse(\"0\"),\n            include_boundaries: true,\n            closed_window: ClosedWindow::Both,\n            ..Default::default()\n        },\n    )\n    .agg([len()])\n    .collect()?;\nprintln!(\"{}\", &amp;out);\n</code></pre></p> <pre><code>shape: (7, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 groups \u2506 _lower_boundary     \u2506 _upper_boundary     \u2506 time                \u2506 len \u2502\n\u2502 ---    \u2506 ---                 \u2506 ---                 \u2506 ---                 \u2506 --- \u2502\n\u2502 str    \u2506 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a      \u2506 2021-12-15 23:00:00 \u2506 2021-12-16 00:00:00 \u2506 2021-12-15 23:00:00 \u2506 1   \u2502\n\u2502 a      \u2506 2021-12-16 00:00:00 \u2506 2021-12-16 01:00:00 \u2506 2021-12-16 00:00:00 \u2506 3   \u2502\n\u2502 a      \u2506 2021-12-16 01:00:00 \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 01:00:00 \u2506 1   \u2502\n\u2502 a      \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 03:00:00 \u2506 2021-12-16 02:00:00 \u2506 2   \u2502\n\u2502 a      \u2506 2021-12-16 03:00:00 \u2506 2021-12-16 04:00:00 \u2506 2021-12-16 03:00:00 \u2506 1   \u2502\n\u2502 b      \u2506 2021-12-16 01:00:00 \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 01:00:00 \u2506 2   \u2502\n\u2502 b      \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 03:00:00 \u2506 2021-12-16 02:00:00 \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/timezones/","title":"Time zones","text":"<p>Tom Scott</p> <p>You really should never, ever deal with time zones if you can help it.</p> <p>The <code>Datetime</code> datatype can have a time zone associated with it. Examples of valid time zones are:</p> <ul> <li><code>None</code>: no time zone, also known as \"time zone naive\".</li> <li><code>UTC</code>: Coordinated Universal Time.</li> <li><code>Asia/Kathmandu</code>: time zone in \"area/location\" format.   See the list of tz database time zones   to see what's available.</li> </ul> <p>Caution: Fixed offsets such as +02:00, should not be used for handling time zones. It's advised to use the \"Area/Location\" format mentioned above, as it can manage timezones more effectively.</p> <p>Note that, because a <code>Datetime</code> can only have a single time zone, it is impossible to have a column with multiple time zones. If you are parsing data with multiple offsets, you may want to pass <code>utc=True</code> to convert them all to a common time zone (<code>UTC</code>), see parsing dates and times.</p> <p>The main methods for setting and converting between time zones are:</p> <ul> <li><code>dt.convert_time_zone</code>: convert from one time zone to another.</li> <li><code>dt.replace_time_zone</code>: set/unset/change time zone.</li> </ul> <p>Let's look at some examples of common operations:</p>  Python Rust <p> <code>str.to_datetime</code> \u00b7 <code>dt.replace_time_zone</code> \u00b7  Available on feature timezone <pre><code>ts = [\"2021-03-27 03:00\", \"2021-03-28 03:00\"]\ntz_naive = pl.Series(\"tz_naive\", ts).str.to_datetime()\ntz_aware = tz_naive.dt.replace_time_zone(\"UTC\").rename(\"tz_aware\")\ntime_zones_df = pl.DataFrame([tz_naive, tz_aware])\nprint(time_zones_df)\n</code></pre></p> <p> <code>str.replace_all</code> \u00b7 <code>dt.replace_time_zone</code> \u00b7  Available on feature timezones \u00b7  Available on feature dtype-datetime <pre><code>let ts = [\"2021-03-27 03:00\", \"2021-03-28 03:00\"];\nlet tz_naive = Series::new(\"tz_naive\", &amp;ts);\nlet time_zones_df = DataFrame::new(vec![tz_naive])?\n    .lazy()\n    .select([col(\"tz_naive\").str().to_datetime(\n        Some(TimeUnit::Milliseconds),\n        None,\n        StrptimeOptions::default(),\n        lit(\"raise\"),\n    )])\n    .with_columns([col(\"tz_naive\")\n        .dt()\n        .replace_time_zone(Some(\"UTC\".to_string()), lit(\"raise\"), NonExistent::Raise)\n        .alias(\"tz_aware\")])\n    .collect()?;\n\nprintln!(\"{}\", &amp;time_zones_df);\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 tz_naive            \u2506 tz_aware                \u2502\n\u2502 ---                 \u2506 ---                     \u2502\n\u2502 datetime[\u03bcs]        \u2506 datetime[\u03bcs, UTC]       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-03-27 03:00:00 \u2506 2021-03-27 03:00:00 UTC \u2502\n\u2502 2021-03-28 03:00:00 \u2506 2021-03-28 03:00:00 UTC \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust <p> <code>dt.convert_time_zone</code> \u00b7 <code>dt.replace_time_zone</code> \u00b7  Available on feature timezone <pre><code>time_zones_operations = time_zones_df.select(\n    [\n        pl.col(\"tz_aware\")\n        .dt.replace_time_zone(\"Europe/Brussels\")\n        .alias(\"replace time zone\"),\n        pl.col(\"tz_aware\")\n        .dt.convert_time_zone(\"Asia/Kathmandu\")\n        .alias(\"convert time zone\"),\n        pl.col(\"tz_aware\").dt.replace_time_zone(None).alias(\"unset time zone\"),\n    ]\n)\nprint(time_zones_operations)\n</code></pre></p> <p> <code>dt.convert_time_zone</code> \u00b7 <code>dt.replace_time_zone</code> \u00b7  Available on feature timezones <pre><code>let time_zones_operations = time_zones_df\n    .lazy()\n    .select([\n        col(\"tz_aware\")\n            .dt()\n            .replace_time_zone(\n                Some(\"Europe/Brussels\".to_string()),\n                lit(\"raise\"),\n                NonExistent::Raise,\n            )\n            .alias(\"replace time zone\"),\n        col(\"tz_aware\")\n            .dt()\n            .convert_time_zone(\"Asia/Kathmandu\".to_string())\n            .alias(\"convert time zone\"),\n        col(\"tz_aware\")\n            .dt()\n            .replace_time_zone(None, lit(\"raise\"), NonExistent::Raise)\n            .alias(\"unset time zone\"),\n    ])\n    .collect()?;\nprintln!(\"{}\", &amp;time_zones_operations);\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 replace time zone             \u2506 convert time zone            \u2506 unset time zone     \u2502\n\u2502 ---                           \u2506 ---                          \u2506 ---                 \u2502\n\u2502 datetime[\u03bcs, Europe/Brussels] \u2506 datetime[\u03bcs, Asia/Kathmandu] \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-03-27 03:00:00 CET       \u2506 2021-03-27 08:45:00 +0545    \u2506 2021-03-27 03:00:00 \u2502\n\u2502 2021-03-28 03:00:00 CEST      \u2506 2021-03-28 08:45:00 +0545    \u2506 2021-03-28 03:00:00 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"}]}